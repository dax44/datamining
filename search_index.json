[
["index.html", "Eksploracja danych Wstęp O książce Zakres przedmiotu Zakres technik stosowanych w data mining Etapy eksploracji danych", " Eksploracja danych Dariusz Majerek Katedra Matematyki Stosowanej Wydział Podstaw Techniki Politechnika Lubelskad.majerek@pollub.pl 2019-05-06 Wstęp O książce Niniejsza książka powstała na bazie doświadczeń autora, a głównym jej celem jest przybliżenie czytelnikowi podstaw z dziedziny Data mining studentom kierunku Matematyka Politechniki Lubelskiej. Będzie łączyć w sobie zarówno treści teoretyczne związane z przedstawianymi etapami eksploracji danych i budową modeli, jak i praktyczne wskazówki dotczące budowy modeli w środowisku R (R Core Team 2018). Podane zostaną również wskazówki, jak raportować wyniki analiz i jak dokonać właściwych ilustracji wyników. Bardzo użyteczny w napisaniu książki były pakiety programu R: bookdown (Xie 2018a), knitr (Xie 2018b) oraz pakiet rmarkdown (Allaire et al. 2018). Zakres przedmiotu Przedmiot Eksploracja danych będzie obejmował swoim zakresem eksplorację i wizualizację danych oraz uczenie maszynowe. Eksploracja danych ma na celu pozyskiwanie i systematyzację wiedzy pochodzącej z danych. Odbywa się ona głównie przy użyciu technik statystycznych, rachunku prawdopodobieństwa i metod z zakresu baz danych. Natomiast uczenie maszynowe, to gałąź nauki (obejmuje nie tylko statystykę, choć to na niej się głównie opiera) dotyczącej budowy modeli zdolnych do rozpoznawania wzorców, przewidywania wartości i klasyfikacji obiektów. Data mining to szybko rosnaca grupa metod analizy danych rozwijana nie tylko przez statystyków ale również przez biologów, genetyków, cybernetyków, informatyków, ekonomistów, osoby pracujace nad rozpoznawaniem obrazów i wiele innych grup zawodowych. W dzisiejszych czasch trudno sobie wyobrazić życie bez sztucznej inteligencji. Towarzyszy ona nam w codziennym, życiu kiedy korzystamy z telefonów komórkowych, wyszukiwarek internetowych, robotów sprzątających, automatycznych samochodów, nawigacji czy gier komputerowych. Lista ta jest niepełna i stale się wydłuża. href=“https://twitter.com/i/status/1091069356367200256”&gt;January 31, 2019 Zakres technik stosowanych w data mining statystyka opisowa wielowymiarowa analiza danych analiza szeregów czasowych analiza danych przestrzennych reguły asocjacji uczenie maszynowe1, w tym: klasyfikacja predykcja analiza skupień text mining i wiele innych Rysunek .: Przykład nienadzorowanego uczenia maszynowego. Źródło:https://analyticstraining.com/cluster-analysis-for-business/ href=“https://twitter.com/i/status/1097199751072690176”&gt;Ferbruary 17, 2019 Etapy eksploracji danych Rysunek .: Etapy eksploracji danych (Kavakiotis et al. 2017) Czyszczenie danych - polega na usuwaniu braków danych, usuwaniu stałych zmiennych, imputacji braków danych oraz przygotowaniu danych do dalszych analiz. Integracja danych - łączenie danych pochodzących z różnych źródeł. Selekcja danych - wybór z bazy tych danych, które są potrzebne do dalszych analiz. Transformacja danych - przekształcenie i konsolidacja danych do postaci przydatnej do eksploracji. Eksploracja danych - zastosowanie technik wymienionych wcześniej w celu odnalezienia wzorców2 i zależności. Ewaluacja modeli - ocena poprawności modeli oraz wzorców z nich uzyskanych. Wizualizacja wyników - graficzne przedstawienie odkrytych wzorców. Wdrażanie modeli - zastosowanie wyznaczonych wzorców. Bibliografia "],
["roz1.html", "1 Import danych", " 1 Import danych Środowisko R pozwala na import i export plików o różnych rozszerzeniach (txt, csv, xls, xlsx, sav, xpt, dta, itd.)3. W tym celu czasami trzeba zainstalować pakiety rozszerzające podstawowe możliwości R-a. Najnowsza4 wersja programu RStudio (v. 1.1.463)5 pozwala na wczytanie danych z popularnych źródeł za pomocą GUI. Rysunek 1.1: Narzędzie do importu plików programu RStudio Jeśli dane są zapisane w trybie tekstowym (np. txt, csv), to wczytujemy je w następujący sposób dane1 &lt;- read.table(&quot;data/dane1.txt&quot;, header = T) head(dane1) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane2 &lt;- read.csv2(&quot;data/dane1.csv&quot;, header = T) head(dane2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # funkcja pakietu readr wczytuje plik jako ramkę danych w formacie tibble # pakiet readr jest częsią większego pakietu tidyverse, # który został wczytany wczsniej dane3 &lt;- read_csv2(&quot;data/dane1.csv&quot;) dane3 ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows Jeśli dane są przechowywane w pliku Excel (np. xlsx), to importujemy je za pomocą funkcji read_excel pakietu readxl. Domyślnie jest wczytywany arkusz pierwszy ale jeśli zachodzi taka potrzeba, to można ustalić, który arkusz pliku Excel ma być wczytany za pomocą paramteru sheet, np. sheet=3, co oznacza, że zostanie wczytany trzeci arkusz pliku. Rysunek 1.2: Fragment pliku Excel Ponieważ w pliku dane1.xlsx braki danych zostały zakodowane znakami BD oraz -, to należy ten fakt przekazać funkcji, aby poprawnie wczytać braki danych. W przeciwnym przypadku zmienne zawierające braki tak kodowane, będą wczytane jako zmienne znakowe. library(readxl) dane4 &lt;- read_excel(&quot;data/dane1.xlsx&quot;, na = c(&quot;BD&quot;, &quot;-&quot;)) dane4 ## # A tibble: 150 x 5 ## `Długość kielic~ `Szerokość kiel~ `Długość płatka` `Szerokość płat~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; Istniej oczywiście jeszcze wiele innych fomatów danych, charakterystycznych dla programów, w których są traktowane jako domyślne.6 W szczególny sposób należy zwrócić uwagę na pliki o rozszerzeniu RData lub rda7 oraz pliki rds. Pliki rda służą do przechowywania obiektów programu R. Mogą to być pliki danych ale również obiekty graficzne (typu wyniki funkcji ggplot), modele (np. wynik funkcji lm()), zdefiniowane funkcje i wszystkie inne obiekty, które da się zapisać w środowisku R. Ponadto pliki rda pozawalają na zapisanie wielu obiektów w jednym pliku. Pliki o rozszerzeniu rds mają podobną funkcję z tym, że pozwalają na przechowywanie tylko jednego obiektu. # wszystkie wczytane wcześniej pliki zapisuje w jednym pliku save(dane1, dane2, dane3, dane4, file = &quot;data/dane.rda&quot;) # plik rda został zapisany list.files(path = &quot;data/&quot;) ## [1] &quot;algae.csv&quot; &quot;Analysis.txt&quot; &quot;dane.rda&quot; &quot;dane1.csv&quot; ## [5] &quot;dane1.txt&quot; &quot;dane1.xlsx&quot; &quot;dane4.rds&quot; &quot;dane4.sav&quot; # usuwam dane ze środowiska R rm(dane1, dane2, dane3, dane4) # sprawdzam co jest wczytane do R ls() ## character(0) # wczytuję plik rda load(&quot;data/dane.rda&quot;) # jeszcze raz sprawdzam co jest wczytane do R ls() ## [1] &quot;dane1&quot; &quot;dane2&quot; &quot;dane3&quot; &quot;dane4&quot; Zapisując obiekty jako oddzielne pliki, można przy wczytywaniu nadawać im nazwy. rm(dane1, dane2, dane3) ls() ## [1] &quot;dane4&quot; saveRDS(dane4, file = &quot;data/dane4.rds&quot;) nowe_dane &lt;- readRDS(&quot;data/dane4.rds&quot;) nowe_dane ## # A tibble: 150 x 5 ## `Długość kielic~ `Szerokość kiel~ `Długość płatka` `Szerokość płat~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; Oprócz wielu zalet takiego sposobu importu i eksportu danych jest jedna poważna wada, pliki te można odczytać jedynie za pomocą R. Osobiście polecam stosować do importu i eksportu danych plików w takich formatach, które mogą przeczytać wszyscy. Jak dotąd widać do importu różnych formatów danych potrzebujemy różnych funkcji, czasami nawet z różnych pakietów. Istnieje rozwiązanie tej niedogodności 🙋 library(rio) dane1 &lt;- import(&quot;data/dane1.txt&quot;) head(dane1) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane2 &lt;- import(&quot;data/dane1.csv&quot;, dec = &quot;,&quot;) # dane1.csv miały , jako znak rozdzielający cechę i mantysę liczb # dlatego włączamy parametr dec head(dane2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane3 &lt;- import(&quot;data/dane1.xlsx&quot;, na=c(&quot;BD&quot;,&quot;-&quot;)) head(dane3) ## Długość kielicha Szerokość kielicha Długość płatka Szerokość płatka ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## Gatunki ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa dane4 &lt;- import(&quot;data/dane4.rds&quot;) dane4 ## # A tibble: 150 x 5 ## `Długość kielic~ `Szerokość kiel~ `Długość płatka` `Szerokość płat~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; Lista możliwości jaką daje nam pakiet rio (Chan and Leeper 2018) jest niemal nieograniczona:8 Comma-separated data (.csv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE Pipe-separated data (.psv), using fread or, if fread = FALSE, read.table with sep = ‘|’, row.names = FALSE and stringsAsFactors = FALSE Tab-separated data (.tsv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE SAS (.sas7bdat), using read_sas. SAS XPORT (.xpt), using read_xpt or, if haven = FALSE, read.xport. SPSS (.sav), using read_sav. If haven = FALSE, read.spss can be used. Stata (.dta), using read_dta. If haven = FALSE, read.dta can be used. SAS XPORT (.xpt), using read.xport. SPSS Portable Files (.por), using read_por. Excel (.xls and .xlsx), using read_excel. Use which to specify a sheet number. For .xlsx files, it is possible to set readxl = FALSE, so that read.xlsx can be used instead of readxl (the default). R syntax object (.R), using dget Saved R objects (.RData,.rda), using load for single-object .Rdata files. Use which to specify an object name for multi-object .Rdata files. This can be any R object (not just a data frame). Serialized R objects (.rds), using readRDS. This can be any R object (not just a data frame). Epiinfo (.rec), using read.epiinfo Minitab (.mtp), using read.mtp Systat (.syd), using read.systat “XBASE” database files (.dbf), using read.dbf Weka Attribute-Relation File Format (.arff), using read.arff Data Interchange Format (.dif), using read.DIF Fortran data (no recognized extension), using read.fortran Fixed-width format data (.fwf), using a faster version of read.fwf that requires a widths argument and by default in rio has stringsAsFactors = FALSE. If readr = TRUE, import will be performed using read_fwf, where widths should be: NULL, a vector of column widths, or the output of fwf_empty, fwf_widths, or fwf_positions. gzip comma-separated data (.csv.gz), using read.table with row.names = FALSE and stringsAsFactors = FALSE CSVY (CSV with a YAML metadata header) using read_csvy. Feather R/Python interchange format (.feather), using read_feather Fast storage (.fst), using read.fst JSON (.json), using fromJSON Matlab (.mat), using read.mat EViews (.wf1), using readEViews OpenDocument Spreadsheet (.ods), using read_ods. Use which to specify a sheet number. Single-table HTML documents (.html), using read_html. The data structure will only be read correctly if the HTML file can be converted to a list via as_list. Shallow XML documents (.xml), using read_xml. The data structure will only be read correctly if the XML file can be converted to a list via as_list. YAML (.yml), using yaml.load Clipboard import (on Windows and Mac OS), using read.table with row.names = FALSE Google Sheets, as Comma-separated data (.csv) Przykład 1.1 Poniższa ilustracja przedstawia fragment pliku danych Analysis.txt zawierającego pewne błędy, które należy naprawić na etapie importu danych. Po pierwsze brakuje w nim nazw zmiennych (choć nie widać tego na rysunku). Poszczególne kolumny nazywają się następująco: season, size, speed, mxPH, mnO2, Cl, NO3, NH4, oPO4, PO4, Chla, a1, a2, a3, a4, a5, a6, a7. Naszym zadaniem jest import tego pliku z jednoczesną obsługą braków (braki danych są zakodowane przez XXXXXXX) oraz nadaniem nagłówków kolumn. Plik Analisis.txt jest umieszczony w kagalogu data/. Z racji, że plik dotyczy glonów, to dane zapiszemy pod nazwą algae. Rysunek 1.3: Fragment pliku danych Analisis.txt algae &lt;- import(&#39;data/Analysis.txt&#39;, header=F, dec=&#39;.&#39;, col.names=c(&#39;season&#39;,&#39;size&#39;,&#39;speed&#39;,&#39;mxPH&#39;,&#39;mnO2&#39;,&#39;Cl&#39;, &#39;NO3&#39;,&#39;NH4&#39;,&#39;oPO4&#39;,&#39;PO4&#39;,&#39;Chla&#39;,&#39;a1&#39;,&#39;a2&#39;, &#39;a3&#39;,&#39;a4&#39;,&#39;a5&#39;,&#39;a6&#39;,&#39;a7&#39;), na.strings=c(&#39;XXXXXXX&#39;)) head(algae) ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla ## 1 winter small medium 8.00 9.8 60.800 6.238 578.000 105.000 170.000 50.0 ## 2 spring small medium 8.35 8.0 57.750 1.288 370.000 428.750 558.750 1.3 ## 3 autumn small medium 8.10 11.4 40.020 5.330 346.667 125.667 187.057 15.6 ## 4 spring small medium 8.07 4.8 77.364 2.302 98.182 61.182 138.700 1.4 ## 5 autumn small medium 8.06 9.0 55.350 10.416 233.700 58.222 97.580 10.5 ## 6 winter small high 8.25 13.1 65.750 9.248 430.000 18.250 56.667 28.4 ## a1 a2 a3 a4 a5 a6 a7 ## 1 0.0 0.0 0.0 0.0 34.2 8.3 0.0 ## 2 1.4 7.6 4.8 1.9 6.7 0.0 2.1 ## 3 3.3 53.6 1.9 0.0 0.0 0.0 9.7 ## 4 3.1 41.0 18.9 0.0 1.4 0.0 1.4 ## 5 9.2 2.9 7.5 0.0 7.5 4.1 1.0 ## 6 15.1 14.6 1.4 0.0 22.5 12.6 2.9 summary(algae) ## season size speed mxPH ## Length:200 Length:200 Length:200 Min. :5.600 ## Class :character Class :character Class :character 1st Qu.:7.700 ## Mode :character Mode :character Mode :character Median :8.060 ## Mean :8.012 ## 3rd Qu.:8.400 ## Max. :9.700 ## NA&#39;s :1 ## mnO2 Cl NO3 NH4 ## Min. : 1.500 Min. : 0.222 Min. : 0.050 Min. : 5.00 ## 1st Qu.: 7.725 1st Qu.: 10.981 1st Qu.: 1.296 1st Qu.: 38.33 ## Median : 9.800 Median : 32.730 Median : 2.675 Median : 103.17 ## Mean : 9.118 Mean : 43.636 Mean : 3.282 Mean : 501.30 ## 3rd Qu.:10.800 3rd Qu.: 57.824 3rd Qu.: 4.446 3rd Qu.: 226.95 ## Max. :13.400 Max. :391.500 Max. :45.650 Max. :24064.00 ## NA&#39;s :2 NA&#39;s :10 NA&#39;s :2 NA&#39;s :2 ## oPO4 PO4 Chla a1 ## Min. : 1.00 Min. : 1.00 Min. : 0.200 Min. : 0.00 ## 1st Qu.: 15.70 1st Qu.: 41.38 1st Qu.: 2.000 1st Qu.: 1.50 ## Median : 40.15 Median :103.29 Median : 5.475 Median : 6.95 ## Mean : 73.59 Mean :137.88 Mean : 13.971 Mean :16.92 ## 3rd Qu.: 99.33 3rd Qu.:213.75 3rd Qu.: 18.308 3rd Qu.:24.80 ## Max. :564.60 Max. :771.60 Max. :110.456 Max. :89.80 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :12 ## a2 a3 a4 a5 ## Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 3.000 Median : 1.550 Median : 0.000 Median : 1.900 ## Mean : 7.458 Mean : 4.309 Mean : 1.992 Mean : 5.064 ## 3rd Qu.:11.375 3rd Qu.: 4.925 3rd Qu.: 2.400 3rd Qu.: 7.500 ## Max. :72.600 Max. :42.800 Max. :44.600 Max. :44.400 ## ## a6 a7 ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 0.000 Median : 1.000 ## Mean : 5.964 Mean : 2.495 ## 3rd Qu.: 6.925 3rd Qu.: 2.400 ## Max. :77.600 Max. :31.600 ## export(algae, file = &quot;data/algae.csv&quot;) Bibliografia "],
["przygotowanie-danych.html", "2 Przygotowanie danych 2.1 Identyfikacja braków danych 2.2 Zastępowanie braków danych", " 2 Przygotowanie danych Dane, które importujemy z zewnętrznego źródła najczęściej nie spełniają formatów obowiązujących w R. Często zmienne zawierają niedopuszczalne znaki szczególne, odstępy w nazwach, powtórzone nazwy kolumn, nazwy zmiennych zaczynające się od liczby, czy puste wiersze lub kolumny. Przed przystąpieniem do analizy zbioru należy rozważyć ewentualne poprawki nazw zmiennych, czy usunięcie pustych kolumn i wierszy. Niektórych czynności można dokonać już na etapie importu danych, stosując pewne pakiety oraz nowe funkcjonalności środowiska RStudio. W większości przypadków uchroni nas to od żmudnego przekształcania typów zmiennych. Oczywiście wszystkie te czynności czyszczenia danych można również dokonać już po imporcie danych, za pomocą odpowiednich komend R. ## przykładowe niepożądane nazwy zmiennych test_df &lt;- as.data.frame(matrix(rnorm(18),ncol = 6)) names(test_df) &lt;- c(&quot;hIgHlo&quot;, &quot;REPEAT VALUE&quot;, &quot;REPEAT VALUE&quot;, &quot;% successful (2009)&quot;, &quot;abc@!*&quot;, &quot;&quot;) test_df ## hIgHlo REPEAT VALUE REPEAT VALUE % successful (2009) abc@!* ## 1 1.0263219 -0.4260190 -1.2152640 0.1009963 0.6798836 ## 2 1.3175625 0.7178012 -0.4132225 1.6192768 0.4576477 ## 3 -0.2323977 -0.7498732 1.6017626 0.6285578 -0.4910620 ## ## 1 -0.4880386 ## 2 -0.4096234 ## 3 0.8594420 ## do poprawy nazw zmiennych użyjemy funkcji make.names names(test_df) &lt;- make.names(names(test_df)) test_df ## hIgHlo REPEAT.VALUE REPEAT.VALUE X..successful..2009. abc... ## 1 1.0263219 -0.4260190 -1.2152640 0.1009963 0.6798836 ## 2 1.3175625 0.7178012 -0.4132225 1.6192768 0.4576477 ## 3 -0.2323977 -0.7498732 1.6017626 0.6285578 -0.4910620 ## X ## 1 -0.4880386 ## 2 -0.4096234 ## 3 0.8594420 Efekt końcowy choć skuteczny, to nie jest zadowalający. Czyszczenia nazw zmiennych można też dokonać stosując funkcję clean_names pakietu janitor (Firke 2018). Pozwala on również na usuwanie pustych wierszy i kolumn, znajdowanie zduplikowanych rekordów, itp. library(janitor) test_df %&gt;% # aby na stałe zmienić nazwy zmiennych trzeba podstawienia clean_names() ## h_ig_hlo repeat_value repeat_value_2 x_successful_2009 abc ## 1 1.0263219 -0.4260190 -1.2152640 0.1009963 0.6798836 ## 2 1.3175625 0.7178012 -0.4132225 1.6192768 0.4576477 ## 3 -0.2323977 -0.7498732 1.6017626 0.6285578 -0.4910620 ## x ## 1 -0.4880386 ## 2 -0.4096234 ## 3 0.8594420 # przykładowe dane x &lt;- data.frame(w1=c(1,4,2,NA),w2=c(NA,2,3,NA), w3=c(1,NA,1,NA)) x ## w1 w2 w3 ## 1 1 NA 1 ## 2 4 2 NA ## 3 2 3 1 ## 4 NA NA NA x %&gt;% remove_empty(&quot;rows&quot;) ## w1 w2 w3 ## 1 1 NA 1 ## 2 4 2 NA ## 3 2 3 1 2.1 Identyfikacja braków danych Zanim usuniemy jakiekolwiek braki w zbiorze, powinniśmy je najpierw zidentyfikować, określić ich charakter, a dopiero potem ewentualnie podjąć decyzję o uzupełnianiu braków. algae &lt;- rio::import(&quot;data/algae.csv&quot;) # najprościej jest wywołać summary summary(algae) ## season size speed mxPH ## Length:200 Length:200 Length:200 Min. :5.600 ## Class :character Class :character Class :character 1st Qu.:7.700 ## Mode :character Mode :character Mode :character Median :8.060 ## Mean :8.012 ## 3rd Qu.:8.400 ## Max. :9.700 ## NA&#39;s :1 ## mnO2 Cl NO3 NH4 ## Min. : 1.500 Min. : 0.222 Min. : 0.050 Min. : 5.00 ## 1st Qu.: 7.725 1st Qu.: 10.981 1st Qu.: 1.296 1st Qu.: 38.33 ## Median : 9.800 Median : 32.730 Median : 2.675 Median : 103.17 ## Mean : 9.118 Mean : 43.636 Mean : 3.282 Mean : 501.30 ## 3rd Qu.:10.800 3rd Qu.: 57.824 3rd Qu.: 4.446 3rd Qu.: 226.95 ## Max. :13.400 Max. :391.500 Max. :45.650 Max. :24064.00 ## NA&#39;s :2 NA&#39;s :10 NA&#39;s :2 NA&#39;s :2 ## oPO4 PO4 Chla a1 ## Min. : 1.00 Min. : 1.00 Min. : 0.200 Min. : 0.00 ## 1st Qu.: 15.70 1st Qu.: 41.38 1st Qu.: 2.000 1st Qu.: 1.50 ## Median : 40.15 Median :103.29 Median : 5.475 Median : 6.95 ## Mean : 73.59 Mean :137.88 Mean : 13.971 Mean :16.92 ## 3rd Qu.: 99.33 3rd Qu.:213.75 3rd Qu.: 18.308 3rd Qu.:24.80 ## Max. :564.60 Max. :771.60 Max. :110.456 Max. :89.80 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :12 ## a2 a3 a4 a5 ## Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 3.000 Median : 1.550 Median : 0.000 Median : 1.900 ## Mean : 7.458 Mean : 4.309 Mean : 1.992 Mean : 5.064 ## 3rd Qu.:11.375 3rd Qu.: 4.925 3rd Qu.: 2.400 3rd Qu.: 7.500 ## Max. :72.600 Max. :42.800 Max. :44.600 Max. :44.400 ## ## a6 a7 ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 0.000 Median : 1.000 ## Mean : 5.964 Mean : 2.495 ## 3rd Qu.: 6.925 3rd Qu.: 2.400 ## Max. :77.600 Max. :31.600 ## ## wyświetl niekompletne wiersze algae[!complete.cases(algae),] %&gt;% head() ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 28 autumn small high 6.8 11.1 9.00 0.630 20 4.0 NA 2.7 30.3 1.9 0.0 ## 38 spring small high 8.0 NA 1.45 0.810 10 2.5 3.0 0.3 75.8 0.0 0.0 ## 48 winter small low NA 12.6 9.00 0.230 10 5.0 6.0 1.1 35.5 0.0 0.0 ## 55 winter small high 6.6 10.8 NA 3.245 10 1.0 6.5 NA 24.3 0.0 0.0 ## 56 spring small medium 5.6 11.8 NA 2.220 5 1.0 1.0 NA 82.7 0.0 0.0 ## 57 autumn small medium 5.7 10.8 NA 2.550 10 1.0 4.0 NA 16.8 4.6 3.9 ## a4 a5 a6 a7 ## 28 0.0 2.1 1.4 2.1 ## 38 0.0 0.0 0.0 0.0 ## 48 0.0 0.0 0.0 0.0 ## 55 0.0 0.0 0.0 0.0 ## 56 0.0 0.0 0.0 0.0 ## 57 11.5 0.0 0.0 0.0 ## policz niekompletne wiersze nrow(algae[!complete.cases(algae),]) ## [1] 16 ## sprawdzenie liczby braków w wierszach apply(algae, 1, function(x) sum(is.na(x))) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ## 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## 2 2 2 2 2 2 2 6 1 0 0 0 0 0 0 0 0 0 ## 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 ## 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ## 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ## 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 199 200 ## 6 0 Wiele ciekawych funkcji do eksploracji danych znajduje się w pakiecie DMwR (Torgo 2013), który został przygotowany przy okazji publikacji książki Data Mining with R. ## poszukiwanie wierszy zawierających wiele braków ## w tym przypadku próg wyświetlania ustawiony jest na 0.2 ## czyli 20% wszystkich kolumn library(DMwR) manyNAs(algae) ## 62 199 ## 62 199 ## tworzenie zbioru pozbawionego wierszy zawierających wiele braków algae2 &lt;- algae[-manyNAs(algae), ] ## sprawdzamy liczbę wybrakowanych wierszy które pozostały nrow(algae2[!complete.cases(algae2),]) ## [1] 14 ## usuwamy wszystkie wiersze z brakami algae3 &lt;- na.omit(algae) ## wyświetl wiersze z brakami algae3[!complete.cases(algae3),] %&gt;% head() ## [1] season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 ## [11] Chla a1 a2 a3 a4 a5 a6 a7 ## &lt;0 rows&gt; (or 0-length row.names) ## liczba pozostałych wybrakowanych wierszy nrow(algae3[!complete.cases(algae3),]) ## [1] 0 ## można oczywiście też ręcznie usuwać wiersze (nie polecam) algae4 &lt;- algae[-c(62,199),] Można też zbudować funkcję, która będzie usuwała braki danych wg naszego upodobania. ## najpierw budujemy funkcję i ją kompilujemy aby R mógł ja stosować ## parametr prog ustala próg odcięcia wierszy czysc.dane &lt;- function(dt, prog = 0){ licz.braki &lt;- apply(dt, 1, function(x) sum(is.na(x))) czyste.dt &lt;- dt[!(licz.braki/ncol(dt)&gt;prog), ] return(czyste.dt) } ## potem ją możemy stosować algae4 &lt;- czysc.dane(algae) nrow(algae4[!complete.cases(algae4),]) ## [1] 0 ## czyścimy wiersze, których liczba braków przekracza 20% wszystkich kolumn algae5 &lt;- czysc.dane(algae, prog = 0.2) nrow(algae5[!complete.cases(algae5),]) ## [1] 14 Bardzo ciekawym narzędziem do znajdowania braków danych jest funkcja md.pattern pakietu mice (van Buuren and Groothuis-Oudshoorn 2018). Wskazuje on ile braków występuje w ramach każdej zmiennej. library(mice) md.pattern(algae) Rysunek 2.1: Na czerwono zaznaczone są zmienne, które zwierają braki danych. Liczba w wierszu po lewej stronie wykresu wskazuje ile wierszy w bazie ma daną charakterystykę, a liczba po prawej oznacza ile zmiennych było wybrakowanych ## season size speed a1 a2 a3 a4 a5 a6 a7 mxPH mnO2 NO3 NH4 oPO4 PO4 Cl ## 184 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 ## 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 ## 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 ## 0 0 0 0 0 0 0 0 0 0 1 2 2 2 2 2 10 ## Chla ## 184 1 0 ## 3 0 1 ## 1 1 1 ## 7 0 2 ## 1 1 1 ## 1 0 6 ## 1 1 1 ## 1 0 6 ## 1 1 1 ## 12 33 2.2 Zastępowanie braków danych Zastępowanie braków danych (zwane także imputacją danych) jest kolejnym etapem procesu przygotowania danych do analiz. Nie można jednak wyróżnić uniwersalnego sposobu zastępowania braków dla wszystkich możliwych sytuacji. Wśród statystyków panuje przekonanie, że w przypadku wystąpienia braków danych można zastosować trzy strategie: nic nie robić z brakami - co wydaje się niedorzeczne ale wcale takie nie jest, ponieważ istnieje wiele modeli statystycznych (np. drzewa decyzyjne), które świetnie radzą sobie w sytuacji braków danych. Niestety nie jest to sposób, który można stosować zawsze, ponieważ są również modele wymagające kompletności danych jak na przykład sieci neuronowe. usuwać braki wierszami9 - to metoda, która jest stosowana domyślnie w przypadku kiedy twórca modelu nie zadecyduje o innym sposobie obsługi luk. Metoda ta ma swoją niewątpliwą zaletę w postaci jasnej i prostej procedury, ale szczególnie w przypadku niewielkich zbiorów może skutkować obciążeniem estymatorów. Nie wiemy bowiem jaka wartość faktycznie jest przypisana danej cesze. Jeśli jest to wartość bliska np. średniej, to nie wpłynie znacząco na obciążenie estymatora wartości oczekiwanej. W przypadku, gdy różni się ona znacznie od średniej tej cechy, to estymator może już wykazywać obciążenie. Jego wielkość zależy również od liczby usuniętych elementów. Nie jest zalecane usuwanie wielu wierszy ze zbioru danych i na podstawie okrojonego zbioru wyciąganie wniosków o populacji, ponieważ próba jest wówczas znacząco inna niż populacja. Dodatkowo jeśli estymatory są wyznaczane na podstawie zbioru wyraźnie mniej licznego, to precyzja estymatorów wyrażona wariancją spada. Reasumując, jeśli liczba wierszy z brakującymi danymi jest niewielka w stosunku do całego zbioru, to usuwanie wierszy jest sensownym rozwiązaniem. uzupełnianie braków - to procedura polegająca na zastępowaniu braków różnymi technikami. Jej niewątpliwą zaletą jest fakt posiadania kompletnych danych bez konieczności usuwania wierszy. Niestety wiąże się to również z pewnymi wadami. Zbiór posiadający wiele braków uzupełnianych nawet bardzo wyrafinowanymi metodami może cechować się zaniżoną wariancją poszczególnych cech oraz tzw. przeuczeniem10. Uzupełnianie średnią - braki w zakresie danej zmiennej uzupełniamy średnią tej zmiennej przypadków uzupełnionych. algae[is.na(algae$mxPH), ] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 a4 a5 ## 48 winter small low NA 12.6 9 0.23 10 5 6 1.1 35.5 0 0 0 0 ## a6 a7 ## 48 0 0 m &lt;- mean(algae$mxPH, na.rm = T) algae[is.na(algae$mxPH), &quot;mxPH&quot;] &lt;- m algae[is.na(algae$mxPH), ] ## [1] season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 ## [11] Chla a1 a2 a3 a4 a5 a6 a7 ## &lt;0 rows&gt; (or 0-length row.names) Uzupełnianie medianą - braki w zakresie danej zmiennej uzupełniamy medianą tej zmiennej przypadków uzupełnionych. algae %&gt;% filter(is.na(Chla)) %&gt;% head ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 1 winter small high 6.6 10.8 NA 3.245 10 1 6.5 NA 24.3 0.0 0.0 ## 2 spring small medium 5.6 11.8 NA 2.220 5 1 1.0 NA 82.7 0.0 0.0 ## 3 autumn small medium 5.7 10.8 NA 2.550 10 1 4.0 NA 16.8 4.6 3.9 ## 4 spring small high 6.6 9.5 NA 1.320 20 1 6.0 NA 46.8 0.0 0.0 ## 5 summer small high 6.6 10.8 NA 2.640 10 2 11.0 NA 46.9 0.0 0.0 ## 6 autumn small medium 6.6 11.3 NA 4.170 10 1 6.0 NA 47.1 0.0 0.0 ## a4 a5 a6 a7 ## 1 0.0 0 0.0 0 ## 2 0.0 0 0.0 0 ## 3 11.5 0 0.0 0 ## 4 28.8 0 0.0 0 ## 5 13.4 0 0.0 0 ## 6 0.0 0 1.2 0 algae[is.na(algae$Chla), &quot;Chla&quot;] &lt;- median(algae$Chla, na.rm = T) Wypełnianie zmiennych typu wyliczeniowego, logicznego lub znakowego odbywa się najczęściej przez dobranie w miejsce brakującej wartości, elementu powtarzającego się najczęściej wśród obiektów obserwowanych. W pakiecie DMwR istnieje funkcja centralImputation, która wypełnia braki wartością centralną (w przypadku zmiennych typu liczbowego - medianą, a dla wartości logicznych, wyliczeniowych lub tekstowych - modą). algae[48, &quot;season&quot;] ## [1] &quot;winter&quot; algae[48, &quot;season&quot;] &lt;- NA algae.uzup &lt;- centralImputation(algae) algae.uzup[48,] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 winter small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 Jeszcze innym sposobem imputacji danych są algorytmy oparte o metodę \\(k\\)-najbliższych sąsiadów. Algorytm opiera się na prostej zasadzie, uzupełniania brakujących wartości medianą (w przypadku zmiennych ilościowych) lub modą (w przypadku zmiennych jakościowych) elementów, które są \\(k\\)-tymi najbliższymi sąsiadami w metryce \\[\\begin{equation}\\label{knn} d(x,y)=\\sqrt{\\sum_{i=1}^{p}\\delta_i(x_i,y_i)}, \\end{equation}\\] gdzie \\(\\delta_i\\) jest odległością pomiędzy dwoma elementami ze względu na \\(i\\)-tą cech, określoną następująco \\[\\begin{equation}\\label{metryka} \\delta_i(v_1, v_2)=\\begin{cases} 1,&amp; \\text{jeśli zmienna jest jakościowa i }v_1\\neq v_2\\\\ 0,&amp; \\text{jeśli zmienna jest jakościowa i }v_1=v_2\\\\ (v_1-v_2)^2,&amp; \\text{jeśli zmienna jest ilościowa.} \\end{cases} \\end{equation}\\] Odległości są mierzone dla zmiennych standaryzowanych. Istnieje też odmiana z wagami, które maleją wraz ze wzrostem odległości pomiędzy sąsiadem a uzupełnianym elementem (np. \\(w(d)=\\exp(d)\\)). algae[48, ] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 &lt;NA&gt; small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 algae &lt;- algae %&gt;% mutate_if(is.character, as.factor) algae.uzup &lt;- knnImputation(algae, k = 5, scale = F, meth = &quot;median&quot;) algae.uzup[48,] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 summer small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 Istnieją również dużo bardziej złożone algorytmy imputacji danych oparte na bardziej wyrafinowanych technikach, takich jak: predykcja modelami liniowymi, nieliniowymi, analiza dyskryminacyjna, drzewa klasyfikacyjne. Dwa najbardziej znane pakiety zawierające funkcje do imputacji w sposób złożony, to Amelia i mice. Imputacja danych z zastosowaniem pakietu mice wymaga podjęcia kilku decyzji przed przystąpieniem do uzupełniania danych: Czy dane są MAR (ang. Missing At Random) czy MNAR (ang. Missing Not At Random), co oznacza, że musimy się zastanowić jakie mogły być źródła braków danych, przypadkowe czy systematyczne? Należy się zdecydować na formę imputacji, określając strukturę zależności pomiędzy cechami oraz rozkład błędu danej cechy? Wybrać zbiór danych, który posłuży nam za predyktory w imputacji (nie mogą zawierać braków). Określenie, które niepełne zmienne są funkcjami innych wybrakowanych zmiennych. Określić w jakiej kolejności dane będą imputowane. Określić parametry startowe imputacji (liczbę iteracji, warunek zbieżności). Określić liczę imputowanych zbiorów. Ad 1. Wyróżniamy następujące rodzaje braków danych: MCAR (ang. Missing Completely At Random) - z definicji to braki, których pojawienie się jest kompletnie losowe. Przykładowo gdy osoba poproszona o wypełnienie wieku w ankiecie będzie rzucać monetą czy wypełnić tą zmienną. MAR - oznacza, że obserwowane wartości i wybrakowane mają inne rozkłady ale da się je oszacować na podstawie danych obserwowanych. Przykładowo ciśnienie tętnicze u osób, które nie wypełniły tej wartości jest wyższe niż u osób, które wpisały swoje ciśnienie. Okazuje się, że osoby starsze z nadciśnieniem nie wypełniały ankiety w tym punkcie. MNAR - jeśli nie jest spełniony warunek MCAR i MAR, wówczas brak ma charakter nielosowy. Przykładowo respondenci osiągający wyższe zarobki sukcesywnie nie wypełniają pola “zarobki” i dodatkowo nie ma w ankiecie zmiennych, które pozwoliłyby nam ustalić, jakie to osoby. Ad 2. Decyzja o algorytmie imputacji wynika bezpośrednio ze skali w jakiej jest mierzona dana zmienna. Ze względu na rodzaj cechy używać będziemy następujących metod: Tabela 2.1: Zestaw metod imputacji danych stosowanych w pakiecie mice method type description pmm any Predictive.mean.matching midastouch any Weighted predictive mean matching sample any Random sample from observed values cart any Classification and regression trees rf any Random forest imputations mean numeric Unconditional mean imputation norm numeric Bayesian linear regression norm.nob numeric Linear regression ignoring model error norm.boot numeric Linear regression using bootstrap norm.predict numeric Linear regression, predicted values quadratic numeric Imputation of quadratic terms ri numeric Random indicator for nonignorable data logreg binary Logistic regression logreg.boot binary Logistic regression with bootstrap polr ordered Proportional odds model polyreg unordered Polytomous logistic regression lda unordered Linear discriminant analysis 2l.norm numeric Level-1 normal heteroscedastic 2l.lmer numeric Level-1 normal homoscedastic, lmer 2l.pan numeric Level-1 normal homoscedastic, pan 2l.bin binary Level-1 logistic, glmer 2lonly.mean numeric Level-2 class mean 2lonly.norm numeric Level-2 class normal 2lonly.pmm any Level-2 class predictive mean matching Każdy z czterech typów danych ma swój domyślny algorytm przeznaczony do imputacji: zmienna ilościowa - pmm zmienna dychotomiczna (stany 0 lub 1) - logreg zmienna typu wyliczeniowego (nieuporządkowana) - polyreg zmienna typu wyliczeniowego (uporządkowana) - polr Niewątpliwą zaletą metody pmm jest to, że wartości imputowane są ograniczone jedynie do obserwowanych wartości. Metody norm i norm.nob uzupełniają brakujące wartości w oparciu o model liniowy. Są one szybkie i efektywne w przypadku gdy reszty modelu są zbliżone rozkładem do normalności. Druga z tych technik nie bierze pod uwagę niepewności związanej z modelem imputującym. Metoda 2L.norm opiera się na dwupoziomowym heterogenicznym modelu liniowym (skupienia są włączone jako efekt do modelu). Technika polyreg korzysta z funkcji multinom pakietu nnet tworzącej model wielomianowy. polr opiera się o proporcjonalny model logitowy z pakietu MASS. lda to model dyskryminacyjny klasyfikujący obiekty na podstawie prawdopodobieństw a posteriori. Metoda sample zastępuje braki losowa wybranymi wartościami spośród wartości obserwowanych. Ad 3. Do ustalenia predyktorów w modelu mice służy funkcja predictorMatrix. Po pierwsze wyświetla ona domyślny układ predyktorów włączanych do modelu. Można go dowolnie zmienić i podstawić do modelu imputującego dane parametrem predictorMatrix. Zera występujące w kolejnych wierszach macierzy predyktorów oznaczają pominięcie tej zmiennej przy imputacji innej zmiennej. Jeśli dodatkowo chcemy by jakaś zmienna nie była imputowana, to oprócz usunięcia jej z listy predyktorów, należy wymazać ją z listy metod predykcji (method). Ogólne zalecenia co do tego jakie zmienne stosować jako predyktory jest takie, żeby brać ich jak najwięcej. Spowoduje to, że bardziej prawdopodobny staje się brak typu MAR a nie MNAR. Z drugiej jednak strony, nierzadko zbiory zawierają olbrzymią liczbę zmiennych i włączanie ich wszystkich do modelu imputującego nie będzie miało sensu. Zalecenia doboru zmiennych są następujące: weź wszystkie te zmienne, które są włączane do modelu właściwego, czyli tego za pomocą którego chcesz poznać strukturę zależności; czasem do modelu imputującego należy też włączyć interakcje zmiennych z modelu właściwego; dodaj zmienne, które mogą mieć wpływ na wybrakowane cechy; włącz zmienne istotnie podnoszące poziom wyjaśnionej wariancji modelu; na koniec usuń te zmienne spośród predyktorów, które same zawierają zbyt wiele braków. Ad 4-7. Decyzje podejmowane w tych punktach zależą istotnie od analizowanego zbioru i będą przedmiotem oddzielnych analiz w kontekście rozważanych zbiorów i zadań. Przykład 2.1 Dokonamy imputacji zbioru airquality z wykorzystaniem pakietów mice i VIM (Templ et al. 2019) data &lt;- airquality summary(data) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## # tworzymy dodatkowe braki danych data[4:10,3] &lt;- rep(NA,7) data[1:5,4] &lt;- NA summary(data) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :57.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:73.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.806 Mean :78.28 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 NA&#39;s :7 NA&#39;s :5 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## md.pattern(data) ## Month Day Temp Solar.R Wind Ozone ## 104 1 1 1 1 1 1 0 ## 34 1 1 1 1 1 0 1 ## 3 1 1 1 1 0 1 1 ## 1 1 1 1 1 0 0 2 ## 4 1 1 1 0 1 1 1 ## 1 1 1 1 0 1 0 2 ## 1 1 1 1 0 0 1 2 ## 3 1 1 0 1 1 1 1 ## 1 1 1 0 1 0 1 2 ## 1 1 1 0 0 0 0 4 ## 0 0 5 7 7 37 56 Do ilustracji braków danych można zastosować funkcje pakietu VIM. library(VIM) aggr(data, numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7) ## ## Variables sorted by number of missings: ## Variable Count ## Ozone 0.24183007 ## Solar.R 0.04575163 ## Wind 0.04575163 ## Temp 0.03267974 ## Month 0.00000000 ## Day 0.00000000 Tak przedstawia się wykres rozrzutu zmiennych Ozone i Solar.R z uwzględnieniem położenia braków danych. marginplot(data[c(1,2)]) Dokonamy imputacji metodą pmm. tempData &lt;- mice(data, maxit=50, meth=&#39;pmm&#39;, seed=44, printFlag = F) summary(tempData) ## Class: mids ## Number of multiple imputations: 5 ## Imputation methods: ## Ozone Solar.R Wind Temp Month Day ## &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## Ozone Solar.R Wind Temp Month Day ## Ozone 0 1 1 1 1 1 ## Solar.R 1 0 1 1 1 1 ## Wind 1 1 0 1 1 1 ## Temp 1 1 1 0 1 1 ## Month 1 1 1 1 0 1 ## Day 1 1 1 1 1 0 Ponieważ, funkcja mice domyślnie dokonuje 5 kompletnych imputacji, możemy się przekonać jak bardzo różnią się poszczególne imputacje i zdecydować się na jedną z nich. head(tempData$imp$Ozone) ## 1 2 3 4 5 ## 5 28 29 20 18 45 ## 10 23 13 9 13 12 ## 25 18 14 18 14 6 ## 26 12 37 1 20 28 ## 27 23 9 13 12 13 ## 32 45 41 20 23 46 Ostatecznie imputacji dokonujemy wybierając jeden z zestawów danych uzupełniających (np. pierwszy). completedData &lt;- mice::complete(tempData, 1) summary(completedData) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :57.00 ## 1st Qu.: 18.00 1st Qu.:115.0 1st Qu.: 7.400 1st Qu.:73.00 ## Median : 30.00 Median :207.0 Median : 9.700 Median :79.00 ## Mean : 41.46 Mean :185.8 Mean : 9.814 Mean :78.24 ## 3rd Qu.: 61.00 3rd Qu.:259.0 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 Za pomocą funkcji pakietu mice możemy również przedstawić graficznie gdzie i jak zostały uzupełnione dane. densityplot(tempData, ~Ozone+Solar.R+Wind+Temp) stripplot(tempData, Ozone+Solar.R+Wind+Temp~.imp, pch = 20, cex = 1.2) Bibliografia "],
["podzia-metod-data-mining.html", "3 Podział metod data mining 3.1 Rodzaje wnioskowania 3.2 Modele regresyjne 3.3 Modele klasyfikacyjne 3.4 Modele grupujące", " 3 Podział metod data mining 3.1 Rodzaje wnioskowania Data mining to zestaw metod pozyskiwania wiedzy na podstawie danych. Ową wiedzę zdobywamy w procesie wnioskowania na podstawie modeli. Wnioskowanie możemy podzielić na dedukcyjne i indukcyjne. I tak z wnioskowaniem dedukcyjnym mamy do czynienia wówczas, gdy na podstawie obecnego stanu wiedzy potrafimy odpowiedzieć na postawione pytanie dotyczące nowej wiedzy, stosując reguły wnioskowania. O wnioskowaniem indukcyjnym powiemy, że jest to metoda pozyskiwania wiedzy na podstawie informacji ze zbioru uczącego. Znajduje ono szerokie zastosowanie w data mining i charakteryzuje się omylnością, ponieważ nawet najlepiej nauczony model na zbiorze uczącym nie zapewnia nam prawdziwości odpowiedzi w przypadku nowych danych, a jedynie je uprawdopodabnia. Esencją wnioskowania indukcyjnego w zakresie data mining, jest poszukiwanie na podstawie danych uczących modelu charakteryzującego się najlepszymi właściwościami predykcyjnymi i dającego się zastosować do zupełnie nowego zbioru danych. Każdy proces uczenia z wykorzystaniem wnioskowania indukcyjnego składa się z następujących elementów. 3.1.1 Dziedzina Dziedzina to zbiór wszystkich obiektów pozostających w zainteresowaniu badacza, będących przedmiotem wnioskowania, oznaczana najczęściej przez \\(X\\). Przykładowo mogą to być zbiory osób, transakcji, urządzeń, instytucji, itp. 3.1.2 Obserwacja Każdy element dziedziny \\(x\\in X\\) nazywamy obserwacją. Obserwacją nazywać będziemy zarówno rekordy danych ze zbioru uczącego, jak i ze zbioru testowego. 3.1.3 Atrybuty obserwacji Każdy obiekt z dziedziny \\(x\\in X\\) można opisać zestawem cech (atrybutów), które w notacji matematycznej oznaczymy przez \\(a:X\\to A\\), gdzie \\(A\\) jest przestrzenią wartości atrybutów. Każda obserwacja \\(x\\) posiadająca \\(k\\) cech da się wyrazić wektorowo jako \\((a_1(x), a_2(x), \\ldots, a_k(x))\\). Dla większości algorytmów uczenia maszynowego wyróżnia się trzy typy atrybutów: nominalne - posiadające skończoną liczbę stanów, które posiadają porządku; porządkowe - posiadające skończoną liczbę stanów z zachowaniem porządku; ciągłe - przyjmujące wartości numeryczne. Często jeden z atrybutów spełnia specjalną rolę, ponieważ stanowi realizację cechy, którą traktujemy jako wyjściową (ang. target value attribute). W tym przypadku powiemy o nadzorowanym uczeniu maszynowym. Jeśli zmiennej wyjściowej nie ma dziedzinie, to mówimy o nienadzorowanym uczeniu maszynowym. 3.1.4 Zbiór uczący Zbiorem uczącym \\(T\\) (ang. training set) nazywamy podzbiór \\(D\\) dziedziny \\(X\\) (czyli \\(T\\subseteq D\\subseteq X\\)), gdzie zbiór \\(D\\) stanowi ogół dostępnych obserwacji z dziedziny \\(X\\). Zbiór uczący zawiera informacje dotyczące badanego zjawiska, na podstawie których, dokonuje się doboru modelu, selekcji cech istotnych z punktu widzenia własności predykcyjnych lub jakości klasyfikacji, budowy modelu oraz optymalizacji jego parametrów. W przypadku uczenia z nauczycielem (nadzorowanego) zbiór \\(T\\) zawiera informację o wartościach atrybutów zmiennej wynikowej. 3.1.5 Zbiór testowy Zbiór testowy \\(T&#39;\\) (ang. test set) będący dopełnieniem zbioru uczącego do zbioru \\(D\\), czyli \\(T&#39;=D\\setminus T\\), stanowi zestaw danych służący do oceny poprawności modelu nadzorowanego. W przypadku metod nienadzorowanych raczej nie stosuje się zbiorów testowych. 3.1.6 Model Model to narzędzie pozyskiwania wiedzy na podstawie zbioru uczącego. Nauczony model jest zbiorem reguł \\(f\\), którego zadaniem jest oszacowanie wielkości wartości wynikowej lub odpowiednia klasyfikacja obiektów. W zadaniu grupowania obiektów (ang. clustering task), celem modelu jest podanie grup możliwie najbardziej jednorodnych przy zadanym zestawie zmiennych oraz ustalonej liczbie skupień (czasami wyznaczenie liczby skupień jest również częścią zadania stawianego przed modelem). 3.1.7 Jakość dopasowania modelu Do oceny jakości dopasowania modelu wykorzystuje się, w zależności od zadania, wiele współczynników (np. dla zadań regresyjnych są to błąd średnio-kwadratowy - ang. Mean Square Error, a dla zadań klasyfikacyjnych - trafność - ang. Accuracy). Możemy mówić dwóch rodzajach dopasowania modeli: poziom dopasowania na zbiorze uczącym poziom dopasowania na zbiorze testowym (oczywiście z punktu widzenia utylitarności modelu ten współczynnik jest ważniejszy). W sytuacji, w której model wykazuje dobre charakterystyki jakości dopasowania na zbiorze uczącym ale słabe na testowym, mówimy o zjawisku przeuczenia modelu (ang. overfitting). Oznacza to, że model wskazuje predykcję poprawnie jedynie dla zbioru treningowego ale ma słaba własności generalizacyjne nowe przypadki danych. Takie model nie przedstawiają znaczącej wartości w odkrywaniu wiedzy w sposób indukcyjny. Z drugiej strony parametry dopasowania modelu mogą pokazywać słabe dopasowanie, zarówno na zbiorze uczącym, jak i testowym. Wówczas również model nie jest użyteczny w pozyskiwaniu wiedzy na temat badanego zjawiska, a sytuację taką nazywamy niedouczeniem (ang. underfitting). Rysunek 3.1: Przykłady niedoucznia (wykresy 1 i 4), poprawego modelu (2 i 5) i przeuczenia (3 i 6). Pierwszy wiersz wykresów pokazuje klasyfikację na podstawie modelu na zbiorze uczącym, a drugi na zbiorze testowym. Wykres na dole pokazuje związek pomiędzy złożonością modelu a wielkością błędu predykcji. Źródło: https://cambridgecoding.wordpress.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/ 3.2 Modele regresyjne Jednym z rodzajów zadań bazującym na wnioskowaniu indukcyjnym jest model regresyjny. Należy on do grupy metod nadzorowanych, których celem jest oszacowanie wartości cechy wyjściowej (która jest ilościowa) na podstawie zestawu predyktorów, które mogą być ilościowe i jakościowe. Uczenie takich modeli odbywa się poprzez optymalizację funkcji celu (np. \\(MSE\\)) na podstawie zbioru uczącego. 3.3 Modele klasyfikacyjne Podobnie jak modele regresyjne, modele klasyfikacyjne należą do grupy metod nadzorowanego uczenia maszynowego. Ich zadaniem jest właściwa klasyfikacja obiektów na podstawie wielkości predyktorów. Odpowiedzią modelu jest zawsze cecha typu jakościowego, natomiast predyktory mogą mieć dowolny typ. Wyróżnia się klasyfikację dwu i wielostanową. Lista modeli realizujących klasyfikację binarną jest nieco dłuższa niż w przypadku modeli z wielostanową cechą wynikową. Proces uczenia modelu klasyfikacyjnego również opiera się na optymalizacji funkcji celu. Tym razem są to zupełnie inne miary jakości dopasowania (np. trafność, czyli odsetek poprawnych klasyfikacji). 3.4 Modele grupujące Bardzo szeroką gamę modeli nienadzorowanych stanowią metody analizy skupień. Ich zadaniem jest grupowanie obiektów w możliwie najbardziej jednorodne grupy, na podstawie wartości atrybutów poddanych analizie. Ponieważ są to metody “bez nauczyciela”, to ocena ich przydatności ma nieco inny charakter i choć istnieją różne wskaźniki jakości grupowania, to trudno tu o obiektywne wskazanie najlepszego rozwiązania. "],
["drzewa-decyzyjne.html", "4 Drzewa decyzyjne 4.1 Węzły i gałęzie 4.2 Rodzaje reguł podziału 4.3 Algorytm budowy drzewa 4.4 Kryteria zatrzymania 4.5 Reguły podziału 4.6 Przycinanie drzewa decyzyjnego 4.7 Obsługa braków danych 4.8 Zalety i wady 4.9 Inne algorytmy budowy drzew decyzyjnych implementowane w R", " 4 Drzewa decyzyjne Drzewo decyzyjne11 jest strukturą hierarchiczną przedstawiającą model klasyfikacyjny lub regresyjny. Stosowane są szczególnie często wówczas, gdy funkcyjna postać związku pomiędzy predyktorami a zmienną wynikową jest nieznana lub ciężka do ustalenia. Każde drzewo decyzyjne składa się z korzenia (ang. root), węzłów (ang. nodes) i liści (ang. leaves). Korzeniem nazywamy początkowy węzeł drzewa, z którego poprzez podziały (ang. splits) powstają kolejne węzły potomne. Końcowe węzły, które nie podlegają podziałom nazywamy liśćmi, a linie łączące węzły nazywamy gałęziami (ang. branches). Jeśli drzewo służy do zadań klasyfikacyjnych, to liście zawierają informację o tym, która klasa w danym ciągu podziałów jest najbardziej prawdopodobna. Natomiast, jeśli drzewo jest regresyjne, to liście zawierają warunkowe miary tendencji centralnej (najczęściej średnią) wartości zmiennej wynikowej. Warunek stanowi szereg podziałów doprowadzający do danego węzła terminalnego (liścia). W obu przypadkach (klasyfikacji i regresji) drzewo “dąży” do takiego podziału by kolejne węzły, a co za tym idzie również liście, były ja najbardziej jednorodne ze względu na zmienną wynikową. Rysunek 4.1: Przykład działania drzewa regresyjnego. Wykes w lewym górnym rogu pokazuje prawdziwą zależność, wyres po prawej stronie jest ilustracją drzewa decyzyjnego, a wykres w lewym dolnym rogu pokazuje dyskretyzację przestrzeni dokonaną przez drzewo, czyli sposób jego działania. 4.1 Węzły i gałęzie Każdy podział rozdziela dziedzinę \\(X\\) na dwa lub więcej podobszarów dziedziny i wówczas każda obserwacja węzła nadrzędnego jest przyporządkowana węzłom potomnym. Każdy odchodzący węzeł potomny jest połączony gałęzią, która to wiąże się ściśle z możliwymi wynikami podziału. Każdy \\(\\mathbf{n}\\)-ty węzeł można opisać jako podzbiór dziedziny w następujący sposób \\[\\begin{equation} X_{\\mathbf{n}}=\\{x\\in X|t_1(x)=r_1,t_2(x)=r_2,\\ldots,t_k(x)=r_k\\}, \\end{equation}\\] gdzie \\(t_1,t_2,\\ldots,t_k\\) są podziałami, które przeprowadzają \\(x\\) w obszary \\(r_1, r_2,\\ldots, r_k\\). Przez \\[\\begin{equation} S_{\\mathbf{n}, t=r}=\\{x\\in S|t(x)=r\\} \\end{equation}\\] rozumiemy, że dokonano takiego ciągu podziałów zbioru \\(S\\), że jego wartości znalazły się w \\(\\mathbf{n}\\)-tym węźle. 4.2 Rodzaje reguł podziału Najczęściej występujące reguły podziału w drzewach decyzyjnych są jednowymiarowe, czyli warunek podziału jest generowany na podstawie jednego atrybutu. Istnieją podziały wielowymiarowe ale ze względu na złożoność obliczeniową są rzadziej stosowane. 4.2.1 Podziały dla atrybutów ze skali nominalnej Istnieją dwa typy reguł podziału dla skali nominalnej: oparte na wartości atrybutu (ang. value based) - wówczas funkcja testowa przyjmuje postać \\(t(x)=a(x)\\), czyli podział generują wartości atrybutu; oparte na równości (ang. equality based) - gdzie funkcja testowa jest zdefiniowana jako \\[\\begin{equation} t(x)= \\begin{cases} 1, &amp;\\text{ gdy } a(x)=\\nu\\\\ 0, &amp; \\text{ w przeciwnym przypadku}, \\end{cases} \\end{equation}\\] gdzie \\(\\nu\\in A\\) i \\(A\\) jest zbiorem możliwych wartości \\(a\\). W tym przypadku podział jest dychotomiczny, albo obiekt ma wartość atrybutu równą \\(\\nu\\), albo go nie ma. 4.2.2 Podziały dla atrybutów ze skali ciągłej Reguły podziału stosowane do skali ciągłej, to: oparta na nierównościach (ang. inequality based) - zdefiniowana jako \\[\\begin{equation} t(x) = \\begin{cases} 1, &amp;\\text{ gdy }a(x)\\leq \\nu\\\\ 0, &amp; \\text{w przeciwnym przypadku}, \\end{cases} \\end{equation}\\] gdzie \\(\\nu\\in A\\); przedziałowa (ang. interval based) - zdefiniowana jako \\[\\begin{equation} t(x) = \\begin{cases} 1, &amp;\\text{ gdy }a(x) \\in I_1\\\\ 2, &amp;\\text{ gdy }a(x) \\in I_2\\\\ \\vdots &amp; \\\\ k, &amp;\\text{ gdy }a(x) \\in I_k\\\\ \\end{cases} \\end{equation}\\] gdzie \\(I_1,I_2,\\ldots,I_k\\subset A\\) stanowią rozłączny podział (przedziałami) przeciwdziedziny \\(A\\). 4.2.3 Podziały dla atrybutów ze skali porządkowej Podziały te mogą wykorzystywać oba wcześniej wspomniane typy, w zależności od potrzeb. 4.3 Algorytm budowy drzewa stwórz początkowy węzeł (korzeń) i oznacz go jako otwarty; przypisz wszystkie możliwe rekordy do węzła początkowego; dopóki istnieją otwarte węzły wykonuj: wybierz węzeł \\(\\mathbf{n}\\), wyznacz potrzebne statystyki opisowe zmiennej zależnej dla tego węzła i przypisz wartość docelową; jeśli kryterium zatrzymania podziału jest spełnione dla węzła \\(n\\), to oznacz go za zamknięty; w przeciwnym przypadku wybierz podział \\(r\\) elementów węzła \\(\\mathbf{n}\\), i dla każdego podzbioru podziału stwórz węzeł niższego rzędu (potomka) \\(\\mathbf{n}_r\\) oraz oznacz go jako otwarty; następnie przypisz wszystkie przypadki generowane podziałem \\(r\\) do odpowiednich węzłów potomków \\(\\mathbf{n}_r\\); oznacza węzeł \\(\\mathbf{n}\\) jako zamknięty. Sposób przypisywania wartości docelowej wiąże się ściśle z rodzajem drzewa. W drzewach regresyjnych chodzi o wyliczenie średniej lub mediany dla obserwacji ujętych w danym węźle. Natomiast w przypadku drzewa klasyfikacyjnego, wyznacza się wartości prawdopodobieństw przynależności obserwacji znajdującej się w danym węźle do poszczególnych klas \\[\\begin{equation} \\P(d|\\mathbf{n})=\\P_{T_\\mathbf{n}}(d)=\\frac{|T_\\mathbf{n}^d|}{|T_\\mathbf{n}|}, \\end{equation}\\] gdzie \\(T_\\mathbf{n}\\) oznaczają obserwacje zbioru uczącego znajdujące się w węźle \\(\\mathbf{n}\\), a \\(T_\\mathbf{n}^d\\) oznacza dodatkowo podzbiór zbioru uczącego w \\(\\mathbf{n}\\) węźle, które należą do klasy \\(d\\). Oczywiście klasyfikacja na podstawie otrzymanych prawdopodobieństw w danym węźle jest dokonana przez wybór klasy charakteryzującej się najwyższym prawdopodobieństwem. 4.4 Kryteria zatrzymania Kryterium zatrzymania jest warunkiem, który decyduje o tym, że dany węzeł uznajemy za zamknięty i nie dokonujemy dalszego jego podziału. Wyróżniamy następujące kryteria zatrzymania: jednorodność węzła - w przypadku drzewa klasyfikacyjnego może zdarzyć się sytuacja, że wszystkie obserwacje węzła będą pochodziły z jednej klasy. Wówczas nie ma sensu dokonywać dalszego podziału węzła; węzeł jest pusty - zbiór przypisanych obserwacji zbioru uczącego do \\(\\mathbf{n}\\)-tego węzła jest pusty; brak reguł podziału - wszystkie reguły podziału zostały wykorzystane, zatem nie da się stworzyć potomnych węzłów, które charakteryzowałyby się większą homogenicznością; Warunki ujęte w pierwszych dwóch kryteriach mogą być nieco złagodzone, poprzez zatrzymanie podziałów wówczas, gdy prawdopodobieństwo przynależenia do pewnej klasy przekroczy ustalony próg lub gdy liczebność węzła spadnie poniżej ustalonej wartości. W literaturze tematu istnieje jeszcze jedno często stosowane kryterium zatrzymania oparte na wielkości drzewa. Węzeł potomny ustala się jako zamknięty, gdy długość ścieżki dojścia do nie go przekroczy ustaloną wartość. 4.5 Reguły podziału Ważnym elementem algorytmu tworzenia drzewa regresyjnego jest reguła podziału. Dobierana jest w taki sposób aby zmaksymalizować zdolności generalizacyjne drzewa. Złożoność drzewa mierzona jest najczęściej przeciętną liczbą podziałów potrzebnych do dotarcia do liścia zaczynając od korzenia. Liście są najczęściej tworzone wówczas gdy dyspersja wartości wynikowej jest stosunkowo mała lub węzeł zawiera w miarę homogeniczne obserwacje ze względu na przynależność do klasy zmiennej wynikowej. W przypadku drzew regresyjnych zmienność na poziomie węzłów jest dobrą miarą służącą do definiowania podziału w węźle. I tak, jeśli pewien podział generuje nam stosunkowo małe dyspersje wartości docelowych w węzłach potomnych, to można ten podział uznać za właściwy. Jeśli \\(T_n\\) oznacza zbiór rekordów należących do węzła \\(n\\), a \\(T_{n,t=r}\\) są podzbiorami generowanymi przez podział \\(r\\) w węzłach potomnych dla \\(n\\), to dyspersję wartości docelowej \\(f\\) będziemy oznaczali następująco \\[\\begin{equation}\\label{dyspersja} \\operatorname{disp}_{T_{n,t=r}}(f). \\end{equation}\\] Regułę podziału możemy określać poprzez minimalizację średniej ważonej dyspersji wartości docelowej następującej postaci \\[\\begin{equation}\\label{reg_podz} \\operatorname{disp}_n(f|t)=\\sum_{r\\in R_t}\\frac{|T_{n,t=r}|}{|T_n|}\\operatorname{disp}_{T_{n,t=r}}(f), \\end{equation}\\] gdzie \\(|\\ |\\) oznacza moc zbioru, a \\(R_t\\) zbiór wszystkich możliwych wartości reguły podziału. Czasami wygodniej będzie maksymalizować przyrost dyspersji (lub spadek) \\[\\begin{equation}\\label{przyrost} \\bigtriangleup \\operatorname{disp}_n(f|t)=\\operatorname{disp}_n(f)-\\sum_{r\\in R_t}\\frac{|T_{n,t=r}|}{|T_n|}\\operatorname{disp}_{T_{n,t=r}}(f). \\end{equation}\\] Miarą heterogeniczności węzłów ze względu na zmienną wynikową (ang. impurity) w drzewach klasyfikacyjnych, która pozwala na tworzenie kolejnych podziałów węzła, są najczęściej wskaźnik Gini’ego i entropia (Breiman 1998). Entropią podzbioru uczącego w węźle \\(\\mathbf{n}\\), wyznaczamy wg wzoru \\[\\begin{equation} E_{T_{\\mathbf{n}}}(c|t) = \\sum_{x\\in R_t} \\frac{|T_{\\mathbf{n}, t=r}|}{|T_{\\mathbf{n}}|}E_{T_{\\mathbf{n}, t=r}}(c), \\end{equation}\\] gdzie \\(t\\) jest podziałem (kandydatem), \\(r\\) potencjalnym wynikiem podziału \\(t\\), \\(c\\) jest oznaczeniem klasy zmiennej wynikowej, a \\[\\begin{equation} E_{T_{\\mathbf{n}, t=r}}(c) = \\sum_{d\\in C}-\\P_{T_{\\mathbf{n}, t=r}}(c=d)\\log\\P_{T_{\\mathbf{n}, t=r}}(c=d), \\end{equation}\\] przy czym \\[\\begin{equation} \\P_{T_{\\mathbf{n}, t=r}}(c=d)= \\P_{T_{\\mathbf{n}}}(c=d|t=r). \\end{equation}\\] Podobnie definiuje się indeks Gini’ego \\[\\begin{equation} Gi_{T_{\\mathbf{n}}}(c|t) = \\sum_{x\\in R_t} \\frac{|T_{\\mathbf{n}, t=r}|}{|T_{\\mathbf{n}}|}Gi_{T_{\\mathbf{n}, t=r}}(c), \\end{equation}\\] gdzie \\[\\begin{equation} Gi_{T_{\\mathbf{n}, t=r}}(c) = \\sum_{d\\in C}\\P_{T_{\\mathbf{n}, t=r}}(c=d)\\cdot(1-\\P_{T_{\\mathbf{n}, t=r}}(c=d))= 1-\\sum_{d\\in C}\\P^2_{T_{\\mathbf{n}, t=r}}(c=d). \\end{equation}\\] Dla tak zdefiniowanych miar “nieczystości” węzłów, podziału dokonujemy w taki sposób, aby zminimalizować współczynnik Gini’ego lub entropię. Im niższe miary nieczystości, tym bardziej obserwacje znajdujące się w węźle są monokulturą12. Nierzadko korzysta się również z współczynnika przyrostu informacji (ang. information gain) \\[\\begin{equation} \\Delta E_{T_{\\mathbf{n}}}(c|t)=E_{T_{\\mathbf{n}}}(c)-E_{T_{\\mathbf{n}}}(c|t). \\end{equation}\\] Istnieje również jego odpowiednik dla indeksu Gini’ego. W obu przypadkach optymalnego podziału szukamy poprzez maksymalizację przyrostu informacji. 4.6 Przycinanie drzewa decyzyjnego Uczenie drzewa decyzyjnego wiąże się z ryzykiem przeuczenia modelu (podobnie jak to się ma w przypadku innych modeli predykcyjnych). Wcześniej przytoczone reguły zatrzymania (np. głębokość drzewa czy zatrzymanie przy osiągnięciu jednorodności na zadanym poziomie) pomagają kontrolować poziom generalizacji drzewa ale czasami będzie dodatkowo potrzebne przycięcie drzewa, czyli usunięcie pewnych podziałów, a co za tym idzie, również liści (węzłów). 4.6.1 Przycinanie redukujące błąd Jedną ze strategii przycinania drzewa jest przycinanie redukujące błąd (ang. reduced error pruning). Polega ono na porównaniu błędów (najczęściej używana jest miara odsetka błędnych klasyfikacji lub MSE) liścia \\(\\mathbf{l}\\) i węzła do którego drzewo przycinamy \\(\\mathbf{n}\\) na całkiem nowym zbiorze uczącym \\(R\\). Niech \\(e_R(\\mathbf{l})\\) i \\(e_R(\\mathbf{n})\\) oznaczają odpowiednio błędy na zbiorze \\(R\\) liścia i węzła. Przez błąd węzła rozumiemy błąd pod-drzewa o korzeniu \\(\\mathbf{n}\\). Wówczas jeśli zachodzi warunek \\[\\begin{equation} e_R(\\mathbf{l})\\leq e_R(\\mathbf{n}), \\end{equation}\\] to zaleca się zastąpić węzeł \\(\\mathbf{n}\\) liściem \\(\\mathbf{l}\\). 4.6.2 Przycinanie minimalizujące błąd Przycinanie minimalizujące błąd opiera się na spostrzeżeniu, że błąd drzewa przyciętego charakteryzuje się zbyt pesymistyczną oceną i dlatego wymaga korekty. Węzeł drzewa klasyfikacyjnego \\(\\mathbf{n}\\) zastępujemy liściem \\(\\mathbf{l}\\), jeśli \\[\\begin{equation} \\hat{e}_T(\\mathbf{l})\\leq \\hat{e}_T(\\mathbf{n}), \\end{equation}\\] gdzie \\[\\begin{equation} \\hat{e}_T(\\mathbf{n})=\\sum_{\\mathbf{n}&#39;\\in N(\\mathbf{n})}\\frac{|T_{\\mathbf{n}&#39;}|}{|T_\\mathbf{n}|}\\hat{e}_T(\\mathbf{n}&#39;), \\end{equation}\\] a \\(N(\\mathbf{n})\\) jest zbiorem wszystkich możliwych węzłów potomnych węzła \\(\\mathbf{n}\\) i \\[\\begin{equation} \\hat{e}_T(\\mathbf{l})=1-\\frac{|\\{x\\in T_\\mathbf{l}|c(x)=d_{\\mathbf{l}}\\}|+mp}{|T_\\mathbf{l}|+m}, \\end{equation}\\] gdzie \\(p\\) jest prawdopodobieństwem przynależności do klasy \\(d_{\\mathbf{l}}\\) ustalona na podstawie zewnętrznej wiedzy (gdy jej nie posiadamy przyjmujemy \\(p=1/|C|\\)). W przypadku drzewa regresyjnego znajdujemy wiele analogii, ponieważ jeśli dla pewnego zbioru rekordów \\(T\\) spełniony jest warunek \\[\\begin{equation}\\label{kryterium1} \\operatorname{mse}_T(\\mathbf{l})\\leq\\operatorname{mse}_T(\\mathbf{n}), \\end{equation}\\] gdzie \\(\\mathbf{l}\\) i \\(\\mathbf{n}\\) oznaczają odpowiednio liść i węzeł, to wówczas zastępujemy węzeł \\(\\mathbf{n}\\) przez liść \\(\\mathbf{l}\\). Estymatory wyznaczone na podstawie niewielkiej próby, mogą być obarczone znaczącym błędem. Wyliczanie błędu średnio-kwadratowego dla podzbioru nowych wartości może się charakteryzować takim obciążeniem. Dlatego stosuje się statystyki opisowe z poprawką, której pochodzenie może mieć trzy źródła: wiedza merytoryczna na temat szukanej wartości, założeń modelu lub na podstawie wyliczeń opartych o cały zbiór wartości. Skorygowany estymator błędu średnio-kwadratowego ma następującą postać \\[\\begin{equation}\\label{mse} \\widehat{\\operatorname{mse}}_T(\\mathbf{l})=\\frac{\\sum_{x\\in T}(f(x)-m_{\\mathbf{l},m,m_0}(f))^2+mS_0^2}{|T_\\mathbf{l}|+m}, \\end{equation}\\] gdzie \\[\\begin{equation}\\label{poprawka} m_{\\mathbf{l},m,m_0}(f)=\\frac{\\sum_{x\\in T_\\mathbf{l}}f(x)+mm_0}{|T_\\mathbf{l}|+m}, \\end{equation}\\] a \\(m_0\\) i \\(S_0^2\\) są średnią i wariancją wyznaczonymi na całej próbie uczącej. Błąd średnio-kwadratowy węzła \\(\\mathbf{n}\\) ma postać \\[\\begin{equation}\\label{propagacja} \\widehat{\\operatorname{mse}}_T(\\mathbf{n})=\\sum_{\\mathbf{n}&#39;\\in N(\\mathbf{n})}\\frac{|T_{\\mathbf{n}&#39;}|}{|T_\\mathbf{n}|}\\widehat{\\operatorname{mse}}_T(\\mathbf{n}&#39;). \\end{equation}\\] Wówczas kryterium podcięcia można zapisać w następujący sposób \\[\\begin{equation}\\label{kryterium2} \\widehat{\\operatorname{mse}}_T(\\mathbf{l}) \\leq \\widehat{\\operatorname{mse}}_T(\\mathbf{n}) \\end{equation}\\] 4.6.3 Przycinanie ze względu na współczynnik złożoności drzewa Przycinanie ze względu na współczynnik złożoności drzewa (ang. cost-complexity pruning) polega na wprowadzeniu “kary” za zwiększoną złożoność drzewa. Drzewa klasyfikacyjne przycinamy gdy spełniony jest warunek \\[\\begin{equation} e_T(\\mathbf{l})\\leq e_T(\\mathbf{n})+\\alpha C(\\mathbf{n}), \\end{equation}\\] gdzie \\(C(\\mathbf{n})\\) oznacza złożoność drzewa mierzoną liczbą liści, a \\(\\alpha\\) parametrem wagi kary za złożoność drzewa. Wspomniane kryterium przycięcia dla drzew regresyjnych bazuje na względnym błędzie średnio-kwadratowym (ang. relative square error), czyli \\[\\begin{equation}\\label{rse} \\widehat{\\operatorname{rse}}_T(\\mathbf{n})=\\frac{|T|\\widehat{\\operatorname{mse}}_T(\\mathbf{n})}{(|T|-1)S^2_T(f)}, \\end{equation}\\] gdzie \\(T\\) oznacza podzbiór \\(X\\), \\(S^2_T\\) wariancję na zbiorze \\(T\\). Wówczas kryterium podcięcia wygląda następująco \\[\\begin{equation}\\label{kryterium3} \\widehat{\\operatorname{rse}}_T(\\mathbf{l})\\leq \\widehat{\\operatorname{rse}}_T(\\mathbf{n})+\\alpha C(\\mathbf{n}). \\end{equation}\\] 4.7 Obsługa braków danych Drzewa decyzyjne wyjątkowo dobrze radzą sobie z obsługa zbiorów z brakami. Stosowane są głównie dwie strategie: udziałów obserwacji (ang. fractional instances) - rozważane są wszystkie możliwe podziały dla brakującej obserwacji i przypisywana jest im odpowiednia waga lub prawdopodobieństwo, w oparciu o zaobserwowany rozkład znanych obserwacji. Te same wagi są stosowane do predykcji wartości na podstawie drzewa z brakami danych. podziałów zastępczych (ang. surrogate splits) - jeśli wynik podziału nie może być ustalony dla obserwacji z brakami, to używany jest podział zastępczy (pierwszy), jeśli i ten nie może zostać ustalony, to stosuje się kolejny. Kolejne podziały zastępcze są generowane tak, aby wynik podziału możliwie najbardziej przypominał podział właściwy. 4.8 Zalety i wady 4.8.1 Zalety łatwe w interpretacji; nie wymagają żmudnego przygotowania danych (brak standaryzacji, wprowadzania zmiennych binarnych, dopuszcza występowanie braków danych); działa na obu typach zmiennych - jakościowych i ilościowych; dopuszcza nieliniowość związku między zmienną wynikową a predyktorami; odporny na odstępstwa od założeń; pozwala na obsługę dużych zbiorów danych. 4.8.2 Wady brak jawnej postaci zależności; zależność struktury drzewa od użytego algorytmu; przegrywa jakością predykcji z innymi metodami nadzorowanego uczenia maszynowego. Przykład 4.1 Przykładem zastosowania drzew decyzyjnych będzie klasyfikacja irysów na podstawie długości i szerokości kielicha i płatka. Przykładem zastosowania drzew decyzyjnych będzie klasyfikacja irysów na podstawie długości i szerokości kielicha i płatka. library(tidyverse) library(rpart) # pakiet do tworzenia drzew typu CART library(rpart.plot) # pakiet do rysowania drzew Każde zadanie ucznia maszynowego zaczynamy od czyszczenia danych i odpowiedniego ich przygotowania ale w tym przypadku skupimy się jedynie na budowie, optymalizacji i ewaluacji modelu. Podział zbioru na próbę uczącą i testową set.seed(2019) dt.train &lt;- iris %&gt;% sample_frac(size = 0.7) dt.test &lt;- setdiff(iris, dt.train) str(dt.train) ## &#39;data.frame&#39;: 105 obs. of 5 variables: ## $ Sepal.Length: num 4.8 6.7 6.2 5.4 7.7 5 5.7 5 6.3 6.8 ... ## $ Sepal.Width : num 3.4 3.3 2.2 3.9 2.6 2 3.8 3 3.4 3 ... ## $ Petal.Length: num 1.9 5.7 4.5 1.3 6.9 3.5 1.7 1.6 5.6 5.5 ... ## $ Petal.Width : num 0.2 2.1 1.5 0.4 2.3 1 0.3 0.2 2.4 2.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 3 2 1 3 2 1 1 3 3 ... str(dt.test) ## &#39;data.frame&#39;: 45 obs. of 5 variables: ## $ Sepal.Length: num 4.4 5.4 4.8 4.3 5.7 5.1 5.2 5.2 5.2 4.9 ... ## $ Sepal.Width : num 2.9 3.7 3 3 4.4 3.8 3.5 3.4 4.1 3.1 ... ## $ Petal.Length: num 1.4 1.5 1.4 1.1 1.5 1.5 1.5 1.4 1.5 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.1 0.1 0.4 0.3 0.2 0.2 0.1 0.2 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Budowa drzewa Budowy drzewa dokonujemy za pomocą funkcji rpart pakietu rpart (Therneau and Atkinson 2018) stosując zapis formuły zależności. Drzewo zostanie zbudowane z uwzględnieniem kilku kryteriów zatrzymania: minimalna liczebność węzła, który może zostać podzielony to 10 - ze względu na małą liczebność zbioru uczącego; minimalna liczebność liścia to 5 - aby nie dopuścić do przeuczenia modelu; maksymalna głębokość drzewa to 4 - aby nie dopuścić do przeuczenia modelu. mod.rpart &lt;- rpart(Species~., data = dt.train, control = rpart.control(minsplit = 10, minbucket = 5, maxdepth = 4)) summary(mod.rpart) ## Call: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.51470588 0 1.00000000 1.1176471 0.06737554 ## 2 0.39705882 1 0.48529412 0.4852941 0.06995514 ## 3 0.02941176 2 0.08823529 0.1617647 0.04614841 ## 4 0.01000000 3 0.05882353 0.1617647 0.04614841 ## ## Variable importance ## Petal.Width Petal.Length Sepal.Length Sepal.Width ## 33 32 20 15 ## ## Node number 1: 105 observations, complexity param=0.5147059 ## predicted class=virginica expected loss=0.647619 P(node) =1 ## class counts: 35 33 37 ## probabilities: 0.333 0.314 0.352 ## left son=2 (35 obs) right son=3 (70 obs) ## Primary splits: ## Petal.Length &lt; 2.6 to the left, improve=35.03810, (0 missing) ## Petal.Width &lt; 0.8 to the left, improve=35.03810, (0 missing) ## Sepal.Length &lt; 5.45 to the left, improve=25.60255, (0 missing) ## Sepal.Width &lt; 3.35 to the right, improve=14.70881, (0 missing) ## Surrogate splits: ## Petal.Width &lt; 0.8 to the left, agree=1.000, adj=1.000, (0 split) ## Sepal.Length &lt; 5.45 to the left, agree=0.933, adj=0.800, (0 split) ## Sepal.Width &lt; 3.35 to the right, agree=0.848, adj=0.543, (0 split) ## ## Node number 2: 35 observations ## predicted class=setosa expected loss=0 P(node) =0.3333333 ## class counts: 35 0 0 ## probabilities: 1.000 0.000 0.000 ## ## Node number 3: 70 observations, complexity param=0.3970588 ## predicted class=virginica expected loss=0.4714286 P(node) =0.6666667 ## class counts: 0 33 37 ## probabilities: 0.000 0.471 0.529 ## left son=6 (37 obs) right son=7 (33 obs) ## Primary splits: ## Petal.Width &lt; 1.75 to the left, improve=24.297670, (0 missing) ## Petal.Length &lt; 4.75 to the left, improve=24.174190, (0 missing) ## Sepal.Length &lt; 5.75 to the left, improve= 4.483555, (0 missing) ## Sepal.Width &lt; 2.55 to the left, improve= 3.793760, (0 missing) ## Surrogate splits: ## Petal.Length &lt; 4.75 to the left, agree=0.886, adj=0.758, (0 split) ## Sepal.Length &lt; 6.15 to the left, agree=0.671, adj=0.303, (0 split) ## Sepal.Width &lt; 2.65 to the left, agree=0.671, adj=0.303, (0 split) ## ## Node number 6: 37 observations, complexity param=0.02941176 ## predicted class=versicolor expected loss=0.1351351 P(node) =0.352381 ## class counts: 0 32 5 ## probabilities: 0.000 0.865 0.135 ## left son=12 (31 obs) right son=13 (6 obs) ## Primary splits: ## Petal.Length &lt; 4.95 to the left, improve=4.0464980, (0 missing) ## Petal.Width &lt; 1.35 to the left, improve=1.0296010, (0 missing) ## Sepal.Width &lt; 3.05 to the right, improve=0.2615519, (0 missing) ## Sepal.Length &lt; 5.95 to the left, improve=0.1828101, (0 missing) ## ## Node number 7: 33 observations ## predicted class=virginica expected loss=0.03030303 P(node) =0.3142857 ## class counts: 0 1 32 ## probabilities: 0.000 0.030 0.970 ## ## Node number 12: 31 observations ## predicted class=versicolor expected loss=0.03225806 P(node) =0.2952381 ## class counts: 0 30 1 ## probabilities: 0.000 0.968 0.032 ## ## Node number 13: 6 observations ## predicted class=virginica expected loss=0.3333333 P(node) =0.05714286 ## class counts: 0 2 4 ## probabilities: 0.000 0.333 0.667 rpart.plot(mod.rpart) Rysunek 4.2: Obraz drzewa klasyfikacyjnego. Powyższy wykres przedstawia strukturę drzewa klasyfikacyjnego. Kolorami są oznaczone klasy, które w danym węźle dominują. Nasycenie barwy decyduje o sile tej dominacji. W każdym węźle podana jest klasa, do której najprawdopodobniej należą jego obserwacje. Ponadto podane są proporcje przynależności do klas zmiennej wynikowej oraz procent obserwacji zbioru uczącego należących do danego węzła. Pod każdym węzłem podana jest reguła podziału. Przycinanie drzewa Zanim przystąpimy do przycinania drzewa należy sprawdzić, jakie są zdolności generalizacyjne modelu. Oceny tej dokonujemy najczęściej sprawdzając macierz klasyfikacji. pred.prob &lt;- predict(mod.rpart, newdata = dt.test) pred.prob[10:20,] ## setosa versicolor virginica ## 10 1 0.0000000 0.00000000 ## 11 1 0.0000000 0.00000000 ## 12 1 0.0000000 0.00000000 ## 13 1 0.0000000 0.00000000 ## 14 1 0.0000000 0.00000000 ## 15 1 0.0000000 0.00000000 ## 16 0 0.9677419 0.03225806 ## 17 0 0.9677419 0.03225806 ## 18 0 0.9677419 0.03225806 ## 19 0 0.9677419 0.03225806 ## 20 0 0.9677419 0.03225806 pred.class &lt;- predict(mod.rpart, newdata = dt.test, type = &quot;class&quot;) pred.class ## 1 2 3 4 5 6 ## setosa setosa setosa setosa setosa setosa ## 7 8 9 10 11 12 ## setosa setosa setosa setosa setosa setosa ## 13 14 15 16 17 18 ## setosa setosa setosa versicolor versicolor versicolor ## 19 20 21 22 23 24 ## versicolor versicolor versicolor versicolor versicolor versicolor ## 25 26 27 28 29 30 ## versicolor versicolor versicolor versicolor versicolor versicolor ## 31 32 33 34 35 36 ## versicolor versicolor virginica virginica virginica virginica ## 37 38 39 40 41 42 ## virginica virginica virginica virginica virginica virginica ## 43 44 45 ## virginica virginica virginica ## Levels: setosa versicolor virginica tab &lt;- table(predykcja = pred.class, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Jak widać z powyższej tabeli, model całkiem dobrze radzi sobie z poprawną klasyfikacją obserwacji do odpowiednich kategorii. W dalszej kolejności sprawdzimy, czy nie jest konieczne przycięcie drzewa. Jednym z kryteriów przycinania drzewa jest przycinanie ze względu na złożoność drzewa. W tym przypadku jest wyrażony parametrem cp. Istnieje powszechnie stosowana reguła jednego odchylenia standardowego, która mówi, że drzewo należy przyciąć wówczas, gdy błąd oszacowany na podstawie sprawdzianu krzyżowego (xerror), pierwszy raz zejdzie poniżej poziomu wyznaczonego przez najniższą wartość błędu powiększonego o odchylenie standardowe tego błędu (xstd). Na podstawie poniższej tabeli można ustalić, że poziomem odcięcia jest wartość \\(0.16176+0.046148=0.207908\\). Pierwszy raz błąd przyjmuje wartość mniejszą od \\(0.16176\\) po drugim podziale (nsplit=2). Temu poziomowi odpowiada cp o wartości \\(0.029412\\) i to jest złożoność drzewa, którą powinniśmy przyjąć do przycięcia drzewa. printcp(mod.rpart) ## ## Classification tree: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## ## Variables actually used in tree construction: ## [1] Petal.Length Petal.Width ## ## Root node error: 68/105 = 0.64762 ## ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.514706 0 1.000000 1.11765 0.067376 ## 2 0.397059 1 0.485294 0.48529 0.069955 ## 3 0.029412 2 0.088235 0.16176 0.046148 ## 4 0.010000 3 0.058824 0.16176 0.046148 plotcp(mod.rpart) Rysunek 4.3: Na wykresie błędów punkt odcięcia zaznaczony jest linią przerywaną Przycięte drzewo wygląda następująco: mod.rpart2 &lt;- prune(mod.rpart, cp = 0.029412) summary(mod.rpart2) ## Call: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.5147059 0 1.00000000 1.1176471 0.06737554 ## 2 0.3970588 1 0.48529412 0.4852941 0.06995514 ## 3 0.0294120 2 0.08823529 0.1617647 0.04614841 ## ## Variable importance ## Petal.Width Petal.Length Sepal.Length Sepal.Width ## 34 31 20 15 ## ## Node number 1: 105 observations, complexity param=0.5147059 ## predicted class=virginica expected loss=0.647619 P(node) =1 ## class counts: 35 33 37 ## probabilities: 0.333 0.314 0.352 ## left son=2 (35 obs) right son=3 (70 obs) ## Primary splits: ## Petal.Length &lt; 2.6 to the left, improve=35.03810, (0 missing) ## Petal.Width &lt; 0.8 to the left, improve=35.03810, (0 missing) ## Sepal.Length &lt; 5.45 to the left, improve=25.60255, (0 missing) ## Sepal.Width &lt; 3.35 to the right, improve=14.70881, (0 missing) ## Surrogate splits: ## Petal.Width &lt; 0.8 to the left, agree=1.000, adj=1.000, (0 split) ## Sepal.Length &lt; 5.45 to the left, agree=0.933, adj=0.800, (0 split) ## Sepal.Width &lt; 3.35 to the right, agree=0.848, adj=0.543, (0 split) ## ## Node number 2: 35 observations ## predicted class=setosa expected loss=0 P(node) =0.3333333 ## class counts: 35 0 0 ## probabilities: 1.000 0.000 0.000 ## ## Node number 3: 70 observations, complexity param=0.3970588 ## predicted class=virginica expected loss=0.4714286 P(node) =0.6666667 ## class counts: 0 33 37 ## probabilities: 0.000 0.471 0.529 ## left son=6 (37 obs) right son=7 (33 obs) ## Primary splits: ## Petal.Width &lt; 1.75 to the left, improve=24.297670, (0 missing) ## Petal.Length &lt; 4.75 to the left, improve=24.174190, (0 missing) ## Sepal.Length &lt; 5.75 to the left, improve= 4.483555, (0 missing) ## Sepal.Width &lt; 2.55 to the left, improve= 3.793760, (0 missing) ## Surrogate splits: ## Petal.Length &lt; 4.75 to the left, agree=0.886, adj=0.758, (0 split) ## Sepal.Length &lt; 6.15 to the left, agree=0.671, adj=0.303, (0 split) ## Sepal.Width &lt; 2.65 to the left, agree=0.671, adj=0.303, (0 split) ## ## Node number 6: 37 observations ## predicted class=versicolor expected loss=0.1351351 P(node) =0.352381 ## class counts: 0 32 5 ## probabilities: 0.000 0.865 0.135 ## ## Node number 7: 33 observations ## predicted class=virginica expected loss=0.03030303 P(node) =0.3142857 ## class counts: 0 1 32 ## probabilities: 0.000 0.030 0.970 rpart.plot(mod.rpart2) Rysunek 4.4: Drzewo klasyfikacyjne po przycięciu Ocena dopasowania modelu Na koniec budowy modelu należy sprawdzić jego jakość na zbiorze testowym. pred.class2 &lt;- predict(mod.rpart2, newdata = dt.test, type = &quot;class&quot;) tab2 &lt;- table(predykcja = pred.class2, obserwacja = dt.test$Species) tab2 ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Mimo przycięcia drzewa, klasyfikacja pozostaje na niezmienionym poziomie. Odsetek poprawnych klasyfikacji możemy oszacować za pomocą round(sum(diag(tab2))/sum(tab2)*100,1) ## [1] 100 4.9 Inne algorytmy budowy drzew decyzyjnych implementowane w R Oprócz najbardziej znanego algorytmu CART implementowanego w postaci funkcji pakietu rpart, istnieją również inne algorytmy, które znalazły swoje implementacje w R. Są to: CHAID13 - algorytm przeznaczony do budowy drzew klasyfikacyjnych, gdzie zarówno zmienna wynikowa, jak i zmienne niezależne muszą być ze skali jakościowej. Główną różnicą w stosunku do drzew typu CART jest sposób budowy podziałów, oparty na teście niezależności \\(\\chi^2\\) Pearsona. Wyboru reguły podziału dokonuje się poprzez testowanie niezależności zmiennej niezależnej z predyktorami. Reguła o największej wartości statystyki \\(\\chi^2\\) jest stosowana w pierwszej kolejności. Implementacja tego algorytmu znajduje się w pakiecie CHAID14 (funkcja do tworzenia drzewa o tej samej nazwie chaid) (Team 2015). Ctree15 - algorytm zbliżony zasadą działania do CHAID, ponieważ również wykorzystuje testowanie do wyboru reguły podziału. Różni się jednak tym, że może być stosowany do zmiennych dowolnego typu oraz tym, że może być zarówno drzewem klasyfikacyjnym jak i regresyjnym. Implementację R-ową można znaleźć w pakietach party (Hothorn, Hornik, and Zeileis 2006) lub partykit (Hothorn and Zeileis 2015) - funkcją do tworzenia modelu jest ctree. C4.5 - algorytm stworzony przez Quinlan (1993) w oparciu, o również jego autorstwa, algorytm ID3. Służy jedynie do zadań klasyfikacyjnych. W dużym uproszczeniu, dobór reguł podziału odbywa się na podstawie przyrostu informacji (patrz Reguły podziału). W przeciwieństwie do pierwotnego algorytmu ID3, C4.5 nie raczej nie przeucza drzew. Implementacja R-owa znajduje się w pakiecie RWeka (Hornik, Buchta, and Zeileis 2009) - funkcja do budowy drzewa to J48. C5.0 - kolejny algorytm autorstwa Kuhn and Quinlan (2018) jest usprawnieniem algorytmu C4.5, generującym mniejsze drzewa automatycznie przycinane na podstawie złożoności drzewa. Służy jedynie do zadań klasyfikacyjnych. Jest szybszy od poprzednika i pozwala na zastosowanie metody boosting16. Implementacja R-owa znajduje się w pakiecie C50, a funkcja do budowy drzewa to C5.0. Przykład 4.2 W celu porównania wyników klasyfikacji na podstawie drzew decyzyjnych o różnych algorytmach, zostaną nauczone modele w oparciu o funkcje ctree, J48 i C5.0 dla tego samego zestawu danych co w przykładzie wcześniejszym 4.1. Drzewo ctree Na początek ustalamy parametry ograniczające rozrost drzewa podobne jak w poprzednim przykładzie. library(partykit) tree2 &lt;- ctree(Species~., data = dt.train, control = ctree_control(minsplit = 10, minbucket = 5, maxdepth = 4)) tree2 ## ## Model formula: ## Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width ## ## Fitted party: ## [1] root ## | [2] Petal.Length &lt;= 1.9: setosa (n = 35, err = 0.0%) ## | [3] Petal.Length &gt; 1.9 ## | | [4] Petal.Width &lt;= 1.7 ## | | | [5] Petal.Length &lt;= 4.9: versicolor (n = 31, err = 3.2%) ## | | | [6] Petal.Length &gt; 4.9: virginica (n = 6, err = 33.3%) ## | | [7] Petal.Width &gt; 1.7: virginica (n = 33, err = 3.0%) ## ## Number of inner nodes: 3 ## Number of terminal nodes: 4 plot(tree2) Rysunek 4.5: Wykres drzewa decyzyjnego zbudowanego metodą ctree Wydaje się, że drzewo nie jest optymalne, ponieważ w węźle 6 obserwacje z grup versicolor i virginica są nieco pomieszane. Ostateczne oceny dokonujemy na podstawie próby testowej. pred2 &lt;- predict(tree2, newdata = dt.test) tab &lt;- table(predykcja = pred2, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Dopiero ocena jakości klasyfikacji na podstawie próby testowej pokazuje, że model zbudowany za pomocą ctree daje podobną precyzję jak rpart przycięty. Drzewo J48 W tym przypadku model sam poszukuje optymalnego rozwiązania przycinając się automatycznie. library(RWeka) tree3 &lt;- J48(Species~., data = dt.train) tree3 ## J48 pruned tree ## ------------------ ## ## Petal.Width &lt;= 0.6: setosa (35.0) ## Petal.Width &gt; 0.6 ## | Petal.Width &lt;= 1.7 ## | | Petal.Length &lt;= 4.9: versicolor (31.0/1.0) ## | | Petal.Length &gt; 4.9 ## | | | Petal.Width &lt;= 1.5: virginica (3.0) ## | | | Petal.Width &gt; 1.5: versicolor (3.0/1.0) ## | Petal.Width &gt; 1.7: virginica (33.0/1.0) ## ## Number of Leaves : 5 ## ## Size of the tree : 9 plot(tree3) Rysunek 4.6: Wykres drzewa decyzyjnego zbudowanego metodą J48 Drzewo jest nieco bardziej rozbudowane niż tree2 i mod.rpart2. summary(tree3) ## ## === Summary === ## ## Correctly Classified Instances 102 97.1429 % ## Incorrectly Classified Instances 3 2.8571 % ## Kappa statistic 0.9571 ## Mean absolute error 0.0331 ## Root mean squared error 0.1286 ## Relative absolute error 7.4482 % ## Root relative squared error 27.2918 % ## Total Number of Instances 105 ## ## === Confusion Matrix === ## ## a b c &lt;-- classified as ## 35 0 0 | a = setosa ## 0 32 1 | b = versicolor ## 0 2 35 | c = virginica Podsumowanie dopasowania drzewa na próbie uczącej jest bardzo dobre, bo poprawnych klasyfikacji jest ponad 97%. Oceny dopasowania i tak dokonujemy na zbiorze testowym. pred3 &lt;- predict(tree3, newdata = dt.test) tab &lt;- table(predykcja = pred3, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Otrzymujemy identyczną macierz klasyfikacji jak w poprzednich przypadkach. Drzewo C50 Tym razem również nie trzeba ustawiać parametrów drzewa, ponieważ algorytm działa tak aby zapobiec rozrostowi drzewa przy jednoczesnej wysokiej poprawności klasyfikacji. library(C50) tree4 &lt;- C5.0(Species~., data = dt.train) summary(tree4) ## ## Call: ## C5.0.formula(formula = Species ~ ., data = dt.train) ## ## ## C5.0 [Release 2.07 GPL Edition] Mon May 06 21:35:06 2019 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 105 cases (5 attributes) from undefined.data ## ## Decision tree: ## ## Petal.Length &lt;= 1.9: setosa (35) ## Petal.Length &gt; 1.9: ## :...Petal.Width &gt; 1.7: virginica (33/1) ## Petal.Width &lt;= 1.7: ## :...Petal.Length &lt;= 4.9: versicolor (31/1) ## Petal.Length &gt; 4.9: virginica (6/2) ## ## ## Evaluation on training data (105 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 4 4( 3.8%) &lt;&lt; ## ## ## (a) (b) (c) &lt;-classified as ## ---- ---- ---- ## 35 (a): class setosa ## 30 3 (b): class versicolor ## 1 36 (c): class virginica ## ## ## Attribute usage: ## ## 100.00% Petal.Length ## 66.67% Petal.Width ## ## ## Time: 0.0 secs Otrzymujemy identyczne drzewo jak w przypadku zastosowania algorytmu ctree. plot(tree4) Rysunek 4.7: Wykres drzewa decyzyjnego zbudowanego metodą C5.0 Dla pewności przeprowadzimy sprawdzenie na zbiorze testowym. pred4 &lt;- predict(tree4, newdata = dt.test) tab &lt;- table(predykcja = pred4, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Bibliografia "],
["pochodne-drzew-decyzyjnych.html", "5 Pochodne drzew decyzyjnych 5.1 Bagging 5.2 Lasy losowe 5.3 Boosting", " 5 Pochodne drzew decyzyjnych Przykład zastosowania drzew decyzyjnych na zbiorze iris w poprzednich przykładach może skłaniać do przypuszczenia, że drzewa decyzyjne zawsze dobrze radzą sobie z predykcją wartości wynikowej. Niestety w przykładach nieco bardziej skomplikowanych, gdzie chociażby klasy zmiennej wynikowej nie są tak wyraźnie separowalne, drzewa decyzyjne wypadają gorzej w porównaniu z innymi modelami nadzorowanego uczenia maszynowego. I tak u podstaw metod bazujących na prostych drzewach decyzyjnych stał pomysł, że skoro jedno drzewo nie ma wystarczających własności predykcyjnych, to może zastosowanie wielu drzew połączonych w pewien sposób poprawi je. Tak powstały metody bagging, random forest i boosting17. Należy zaznaczyć, że metody znajdują swoje zastosowanie również w innych modelach nadzorowanego uczenia maszynowego. 5.1 Bagging Technika ta została wprowadzona przez Breiman (1996) i ma na celu zmniejszenie wariancji modelu pojedynczego drzewa. Podobnie jak technika bootstrap, w której statystyki są wyliczane na wielu próbach pobranych z tego samego rozkładu (próby), w metodzie bagging losuje się wiele prób ze zbioru uczącego (najczęściej poprzez wielokrotne losowanie próby o rozmiarze zbioru uczącego ze zwracaniem), a następnie dla każdej próby bootstrapowej buduje się drzewo. W ten sposób otrzymujemy \\(B\\) drzew decyzyjnych \\(\\hat{f}^1(x), \\hat{f}^2(x),\\ldots, \\hat{f}^B(x)\\). Na koniec poprzez uśrednienie otrzymujemy model charakteryzujący się większą precyzją \\[\\begin{equation} \\hat{f}_{bag}(x)=\\frac1B\\sum_{b=1}^B\\hat{f}^b(x). \\end{equation}\\] Ponieważ podczas budowy drzew na podstawie prób bootstrapowych nie kontrolujemy złożoności, to w rezultacie każde z drzew może charakteryzować się dużą wariancją. Poprzez uśrednianie wyników pojedynczych drzew otrzymujemy mniejsze obciążenie ale również przy dostatecznie dużej liczbie prób (\\(B\\) często liczy się w setkach, czy tysiącach) zmniejszamy wariancję “średniej” predykcji z drzew. Oczywiście metodę tą trzeba dostosować do zadań klasyfikacyjnych, ponieważ nie istnieje średnia klasyfikacji z wielu drzew. W miejsce średniej stosuje się modę, czyli wartość dominującą. Przyjrzyjmy się jak maszyna losuje obserwacje ze zwracaniem n &lt;- NULL m &lt;- NULL for(i in 1:1000){ x &lt;- sample(1:500, size = 500, replace = T) y &lt;- setdiff(1:500, x) z &lt;- unique(x) n[i] &lt;- length(z) m[i] &lt;- length(y) } mean(n)/500*100 ## [1] 63.2802 mean(m)/500*100 ## [1] 36.7198 Faktycznie uczenie modelu metodą bagging odbywa się średnio na 2/3 obserwacji zbioru uczącego wylosowanych do prób bootstrapowych, a pozostała 1/3 (ang. out-of-bag) jest wykorzystana do oceny jakości predykcji. Niewątpliwą zaletą drzew decyzyjnych była ich łatwa interpretacja. W przypadku metody bagging jest ona znacznie utrudniona, ponieważ jej wynik składa się z agregacji wielu drzew. Można natomiast ocenić ważność predyktorów (ang. variable importance). I tak, przez obserwację spadku \\(RSS\\) dla baggingu regresyjnego przy zastosowaniu danego predyktora w podziałach drzewa i uśrednieniu wyniku otrzymamy wskaźnik ważności predyktora dużo lepszy niż dla pojedynczego drzewa. W przypadku baggingu klasyfikacyjnego w miejsce \\(RSS\\) stosujemy indeks Gini’ego. Implementacja R-owa metody bagging znajduje się w pakiecie ipred, a funkcja do budowy modelu nazywa się bagging (Peters and Hothorn 2018). Można również stosować funkcję randomForest pakietu randomForest (Liaw and Wiener 2002) - powody takiego działania wyjaśnią się w podrozdziale Lasy losowe. Przykład 5.1 Tym razem cel zadania jest regresyjny i polega na ustaleniu miary tendencji centralnej ceny mieszkań w Bostonie na podstawie zmiennych umieszczonych w zbiorze Boston pakietu MASS (Venables and Ripley 2002). Zmienną zależną będzie mediana cen mieszkań na przedmieściach Bostonu (medv). library(MASS) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 set.seed(2019) boston.train &lt;- Boston %&gt;% sample_frac(size = 2/3) boston.test &lt;- setdiff(Boston, boston.train) Aby móc porównać wyniki predykcji z metody bagging, najpierw zostanie zbudowane jedno drzewo decyzyjne w oparciu o algorytm CART. library(rpart) library(rpart.plot) boston.rpart &lt;- rpart(medv~., data = boston.train) x &lt;- summary(boston.rpart) ## Call: ## rpart(formula = medv ~ ., data = boston.train) ## n= 337 ## ## CP nsplit rel error xerror xstd ## 1 0.49839799 0 1.0000000 1.0086928 0.10259521 ## 2 0.15725128 1 0.5016020 0.5442932 0.06125724 ## 3 0.07485605 2 0.3443507 0.4031978 0.05139310 ## 4 0.03672387 3 0.2694947 0.3127794 0.04599170 ## 5 0.03552748 4 0.2327708 0.2974517 0.04560807 ## 6 0.01695185 5 0.1972433 0.2553208 0.04022970 ## 7 0.01422576 6 0.1802915 0.2713816 0.04099092 ## 8 0.01103490 7 0.1660657 0.2744789 0.04107777 ## 9 0.01000000 8 0.1550308 0.2720415 0.04119266 ## ## Variable importance ## rm lstat indus ptratio crim age nox dis zn ## 33 19 9 8 7 6 6 5 3 ## tax rad chas ## 2 1 1 ## ## Node number 1: 337 observations, complexity param=0.498398 ## mean=22.69792, MSE=79.32964 ## left son=2 (286 obs) right son=3 (51 obs) ## Primary splits: ## rm &lt; 6.92 to the left, improve=0.4983980, (0 missing) ## lstat &lt; 9.725 to the right, improve=0.4424796, (0 missing) ## indus &lt; 6.66 to the right, improve=0.2796065, (0 missing) ## ptratio &lt; 19.65 to the right, improve=0.2600149, (0 missing) ## nox &lt; 0.6695 to the right, improve=0.2346383, (0 missing) ## Surrogate splits: ## ptratio &lt; 14.55 to the right, agree=0.884, adj=0.235, (0 split) ## lstat &lt; 4.915 to the right, agree=0.878, adj=0.196, (0 split) ## zn &lt; 87.5 to the left, agree=0.864, adj=0.098, (0 split) ## indus &lt; 1.605 to the right, agree=0.864, adj=0.098, (0 split) ## crim &lt; 0.013355 to the right, agree=0.852, adj=0.020, (0 split) ## ## Node number 2: 286 observations, complexity param=0.1572513 ## mean=20.04266, MSE=37.17489 ## left son=4 (114 obs) right son=5 (172 obs) ## Primary splits: ## lstat &lt; 14.405 to the right, improve=0.3954065, (0 missing) ## nox &lt; 0.6695 to the right, improve=0.3012249, (0 missing) ## crim &lt; 8.37969 to the right, improve=0.2817286, (0 missing) ## ptratio &lt; 20.15 to the right, improve=0.2392532, (0 missing) ## dis &lt; 2.4737 to the left, improve=0.2295258, (0 missing) ## Surrogate splits: ## age &lt; 84.3 to the right, agree=0.808, adj=0.518, (0 split) ## dis &lt; 2.23935 to the left, agree=0.773, adj=0.430, (0 split) ## crim &lt; 4.067905 to the right, agree=0.762, adj=0.404, (0 split) ## nox &lt; 0.5765 to the right, agree=0.762, adj=0.404, (0 split) ## indus &lt; 16.57 to the right, agree=0.759, adj=0.395, (0 split) ## ## Node number 3: 51 observations, complexity param=0.07485605 ## mean=37.58824, MSE=54.4677 ## left son=6 (34 obs) right son=7 (17 obs) ## Primary splits: ## rm &lt; 7.47 to the left, improve=0.72041550, (0 missing) ## lstat &lt; 3.99 to the right, improve=0.34223650, (0 missing) ## ptratio &lt; 15.05 to the right, improve=0.21227430, (0 missing) ## rad &lt; 2.5 to the left, improve=0.10053340, (0 missing) ## tax &lt; 267 to the right, improve=0.07935891, (0 missing) ## Surrogate splits: ## lstat &lt; 3.99 to the right, agree=0.824, adj=0.471, (0 split) ## indus &lt; 1.215 to the right, agree=0.706, adj=0.118, (0 split) ## chas &lt; 0.5 to the left, agree=0.706, adj=0.118, (0 split) ## tax &lt; 225 to the right, agree=0.706, adj=0.118, (0 split) ## crim &lt; 1.3713 to the left, agree=0.686, adj=0.059, (0 split) ## ## Node number 4: 114 observations, complexity param=0.03552748 ## mean=15.33333, MSE=21.50994 ## left son=8 (77 obs) right son=9 (37 obs) ## Primary splits: ## crim &lt; 0.69916 to the right, improve=0.3873341, (0 missing) ## nox &lt; 0.6615 to the right, improve=0.3541892, (0 missing) ## dis &lt; 2.3497 to the left, improve=0.3182514, (0 missing) ## ptratio &lt; 19.45 to the right, improve=0.3102781, (0 missing) ## tax &lt; 567.5 to the right, improve=0.2823826, (0 missing) ## Surrogate splits: ## ptratio &lt; 19.95 to the right, agree=0.895, adj=0.676, (0 split) ## indus &lt; 14.345 to the right, agree=0.868, adj=0.595, (0 split) ## nox &lt; 0.5825 to the right, agree=0.868, adj=0.595, (0 split) ## tax &lt; 397 to the right, agree=0.868, adj=0.595, (0 split) ## rad &lt; 16 to the right, agree=0.860, adj=0.568, (0 split) ## ## Node number 5: 172 observations, complexity param=0.03672387 ## mean=23.16395, MSE=23.11579 ## left son=10 (82 obs) right son=11 (90 obs) ## Primary splits: ## lstat &lt; 9.645 to the right, improve=0.24693150, (0 missing) ## rm &lt; 6.543 to the left, improve=0.17749260, (0 missing) ## ptratio &lt; 17.85 to the right, improve=0.07815189, (0 missing) ## nox &lt; 0.5125 to the right, improve=0.07760816, (0 missing) ## tax &lt; 267.5 to the right, improve=0.07238020, (0 missing) ## Surrogate splits: ## nox &lt; 0.5125 to the right, agree=0.756, adj=0.488, (0 split) ## indus &lt; 7.625 to the right, agree=0.750, adj=0.476, (0 split) ## rm &lt; 6.26 to the left, agree=0.738, adj=0.451, (0 split) ## age &lt; 65.25 to the right, agree=0.727, adj=0.427, (0 split) ## dis &lt; 3.8824 to the left, agree=0.709, adj=0.390, (0 split) ## ## Node number 6: 34 observations ## mean=33.15882, MSE=13.41419 ## ## Node number 7: 17 observations ## mean=46.44706, MSE=18.85661 ## ## Node number 8: 77 observations, complexity param=0.0110349 ## mean=13.33247, MSE=15.64998 ## left son=16 (37 obs) right son=17 (40 obs) ## Primary splits: ## lstat &lt; 20.1 to the right, improve=0.24481010, (0 missing) ## crim &lt; 15.718 to the right, improve=0.23250740, (0 missing) ## dis &lt; 2.0037 to the left, improve=0.17113480, (0 missing) ## nox &lt; 0.6615 to the right, improve=0.11757680, (0 missing) ## rm &lt; 5.5675 to the left, improve=0.09054612, (0 missing) ## Surrogate splits: ## dis &lt; 1.9733 to the left, agree=0.792, adj=0.568, (0 split) ## rm &lt; 5.632 to the left, agree=0.727, adj=0.432, (0 split) ## age &lt; 95.35 to the right, agree=0.675, adj=0.324, (0 split) ## crim &lt; 9.08499 to the right, agree=0.662, adj=0.297, (0 split) ## black &lt; 396.295 to the right, agree=0.623, adj=0.216, (0 split) ## ## Node number 9: 37 observations ## mean=19.4973, MSE=8.034858 ## ## Node number 10: 82 observations ## mean=20.66098, MSE=6.55677 ## ## Node number 11: 90 observations, complexity param=0.01695185 ## mean=25.44444, MSE=27.29425 ## left son=22 (83 obs) right son=23 (7 obs) ## Primary splits: ## age &lt; 86.7 to the left, improve=0.1844883, (0 missing) ## lstat &lt; 4.46 to the right, improve=0.1773076, (0 missing) ## dis &lt; 3.0037 to the right, improve=0.1652768, (0 missing) ## crim &lt; 0.628575 to the left, improve=0.1203635, (0 missing) ## nox &lt; 0.5585 to the left, improve=0.1122403, (0 missing) ## Surrogate splits: ## nox &lt; 0.5585 to the left, agree=0.978, adj=0.714, (0 split) ## dis &lt; 2.1491 to the right, agree=0.978, adj=0.714, (0 split) ## crim &lt; 0.643205 to the left, agree=0.967, adj=0.571, (0 split) ## indus &lt; 16.57 to the left, agree=0.956, adj=0.429, (0 split) ## ptratio &lt; 14.75 to the right, agree=0.956, adj=0.429, (0 split) ## ## Node number 16: 37 observations ## mean=11.2973, MSE=10.14026 ## ## Node number 17: 40 observations ## mean=15.215, MSE=13.37128 ## ## Node number 22: 83 observations, complexity param=0.01422576 ## mean=24.79277, MSE=13.56694 ## left son=44 (55 obs) right son=45 (28 obs) ## Primary splits: ## rm &lt; 6.543 to the left, improve=0.3377388, (0 missing) ## lstat &lt; 5.41 to the right, improve=0.2548210, (0 missing) ## tax &lt; 267.5 to the right, improve=0.2210129, (0 missing) ## ptratio &lt; 18.05 to the right, improve=0.1394682, (0 missing) ## dis &lt; 6.4889 to the right, improve=0.1125739, (0 missing) ## Surrogate splits: ## lstat &lt; 5.055 to the right, agree=0.783, adj=0.357, (0 split) ## ptratio &lt; 15.75 to the right, agree=0.723, adj=0.179, (0 split) ## crim &lt; 0.39646 to the left, agree=0.699, adj=0.107, (0 split) ## chas &lt; 0.5 to the left, agree=0.687, adj=0.071, (0 split) ## age &lt; 74.15 to the left, agree=0.687, adj=0.071, (0 split) ## ## Node number 23: 7 observations ## mean=33.17143, MSE=125.3192 ## ## Node number 44: 55 observations ## mean=23.26545, MSE=8.880443 ## ## Node number 45: 28 observations ## mean=27.79286, MSE=9.189949 rpart.plot(boston.rpart) Rysunek 5.1: Drzewo regresyjne pełne Przycinamy drzewo… printcp(boston.rpart) ## ## Regression tree: ## rpart(formula = medv ~ ., data = boston.train) ## ## Variables actually used in tree construction: ## [1] age crim lstat rm ## ## Root node error: 26734/337 = 79.33 ## ## n= 337 ## ## CP nsplit rel error xerror xstd ## 1 0.498398 0 1.00000 1.00869 0.102595 ## 2 0.157251 1 0.50160 0.54429 0.061257 ## 3 0.074856 2 0.34435 0.40320 0.051393 ## 4 0.036724 3 0.26949 0.31278 0.045992 ## 5 0.035527 4 0.23277 0.29745 0.045608 ## 6 0.016952 5 0.19724 0.25532 0.040230 ## 7 0.014226 6 0.18029 0.27138 0.040991 ## 8 0.011035 7 0.16607 0.27448 0.041078 ## 9 0.010000 8 0.15503 0.27204 0.041193 plotcp(boston.rpart) boston.rpart2 &lt;- prune(boston.rpart, cp = 0.016952) rpart.plot(boston.rpart2) Rysunek 5.2: Drzewo regresyjne przycięte Predykcja na podstawie drzewa na zbiorze testowym. boston.pred &lt;- predict(boston.rpart2, newdata = boston.test) rmse &lt;- function(pred, obs) sqrt(1/length(pred)*sum((pred-obs)^2)) rmse(boston.pred, boston.test$medv) ## [1] 5.830722 Teraz zbudujemy model metodą bagging. library(randomForest) boston.bag &lt;- randomForest(medv~., data = boston.train, mtry = ncol(boston.train)-1) boston.bag ## ## Call: ## randomForest(formula = medv ~ ., data = boston.train, mtry = ncol(boston.train) - 1) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 13 ## ## Mean of squared residuals: 12.03374 ## % Var explained: 84.83 Predykcja na podstawie modelu boston.pred2 &lt;- predict(boston.bag, newdata = boston.test) rmse(boston.pred2, boston.test$medv) ## [1] 4.359119 Zatem predykcja na podstawie modelu bagging jest nico lepsza niż z pojedynczego drzewa. Dodatkowo możemy ocenić ważność zmiennych użytych w budowie drzew. varImpPlot(boston.bag) Rysunek 5.3: Wykres ważności predyktorów importance(boston.bag) ## IncNodePurity ## crim 1335.62584 ## zn 21.35274 ## indus 134.28748 ## chas 24.07230 ## nox 423.26229 ## rm 15413.69291 ## age 380.78172 ## dis 1204.86690 ## rad 88.28151 ## tax 454.99800 ## ptratio 309.58412 ## black 216.15512 ## lstat 6217.95834 x$variable.importance ## rm lstat indus ptratio crim age ## 16276.30598 9170.91941 4427.10554 4039.00112 3412.53062 3170.82658 ## nox dis zn tax rad chas ## 3063.70694 2681.24858 1306.29569 800.17910 539.07271 262.60146 ## black ## 63.78554 W porównaniu do ważności zmiennych dla pojedynczego drzewa widać pewne różnice. 5.2 Lasy losowe Lasy losowe są uogólnieniem metody bagging, polegającą na losowaniu dla każdego drzewa wchodzącego w skład lasu \\(m\\) predyktorów spośród \\(p\\) dostępnych, a następnie budowaniu drzew z wykorzystaniem tylko tych predyktorów (Ho 1995). Dzięki temu za każdy razem drzewo jest budowane w oparciu o nowy zestaw cech (najczęściej przyjmujemy \\(m=\\sqrt{p}\\)). W przypadku modeli bagging za każdym razem najsilniejszy predyktor wchodził w skład zbioru uczącego, a co za tym idzie również uczestniczył w tworzeniu reguł podziału. Wówczas wiele drzew zawierało reguły stosujące dany atrybut, a wtedy predykcje otrzymywane za pomocą drzew były skorelowane. Dlatego nawet duża liczba prób bootstrapowych nie zapewniała poprawy precyzji. Implementacja tej metody znajduje się w pakiecie randomForest. Przykład 5.2 Kontynuując poprzedni przykład 5.1 możemy zbudować las losowy aby przekonać się czy nastąpi poprawa predykcji zmiennej wynikowej. boston.rf &lt;- randomForest(medv~., data = boston.train) boston.rf ## ## Call: ## randomForest(formula = medv ~ ., data = boston.train) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 4 ## ## Mean of squared residuals: 12.05123 ## % Var explained: 84.81 Porównanie MSE na próbach uczących pomiędzy lasem losowym i modelem bagging wypada nieco na korzyść bagging. boston.pred3 &lt;- predict(boston.rf, newdata = boston.test) rmse(boston.pred3, boston.test$medv) ## [1] 3.79973 Ważność zmiennych również się nieco różni. varImpPlot(boston.rf) 5.3 Boosting Rozważania na temat metody boosting zaczęły się od pytań postawionych w publikacji Kearns and Valiant (1989), czy da się na podstawie na podstawie zbioru słabych modeli stworzyć jeden dobry? Odpowiedzi pozytywnej na nie udzielili, najpierw Schapire (1990), a potem Breiman (1998). W metodzie boosting nie stosuje się prób bootstrapowych ale odpowiednio modyfikuje się drzewo wyjściowe w kolejnych krokach na tym samym zbiorze uczącym. Algorytm dla drzewa regresyjnego jest następujący: Ustal \\(\\hat{f}(x)=0\\) i \\(r_i=y_i\\) dla każdego \\(i\\) w zbiorze uczącym. Dla \\(b=1,2,\\ldots, B\\) powtarzaj: naucz drzewo \\(\\hat{f}^b\\) o \\(d\\) regułach podziału (czyli \\(d+1\\) liściach) na zbiorze \\((X_i, r_i)\\), zaktualizuj drzewo do nowej “skurczonej” wersji \\[\\begin{equation} \\hat{f}(x)\\leftarrow \\hat{f}(x)+\\lambda\\hat{f}^b(x), \\end{equation}\\] zaktualizuj reszty \\[\\begin{equation} r_i\\leftarrow r_i-\\lambda\\hat{f}^b(x_i). \\end{equation}\\] Wyznacz boosted model \\[\\begin{equation} \\hat{f}(x) = \\sum_{b=1}^B\\lambda\\hat{f}^b(x) \\end{equation}\\] Uczenie drzew klasyfikacyjnego metoda boosting przebiega w podobny sposób. Wynik uczenia drzew metodą boosting zależy od trzech parametrów: Liczby drzew \\(B\\). W przeciwieństwie do metody bagging i lasów losowych, zbyt duże \\(B\\) może doprowadzić do przeuczenia modelu. \\(B\\) ustala się najczęściej na podstawie walidacji krzyżowej. Parametru “kurczenia” (ang. shrinkage) \\(\\lambda\\). Kontroluje on szybkość uczenia się kolejnych drzew. Typowe wartości \\(\\lambda\\) to 0.01 lub 0.001. Bardzo małe \\(\\lambda\\) może wymagać dobrania większego \\(B\\), aby zapewnić dobrą jakość predykcyjną modelu. Liczby podziałów w drzewach \\(d\\), która decyduje o złożoności drzewa. Bywa, że nawet \\(d=1\\) daje dobre rezultaty, ponieważ model wówczas uczy się powoli. Implementację metody boosting można znaleźć w pakiecie gbm (Greenwell et al. 2019) Przykład 5.3 Metodę boosting zastosujemy do zadania predykcji ceny mieszkań na przedmieściach Bostonu. Dobór parametrów modelu będzie arbitralny, więc niekoniecznie model będzie najlepiej dopasowany. library(gbm) boston.boost &lt;- gbm(medv~., data = boston.train, distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 2, shrinkage = 0.01) boston.boost ## gbm(formula = medv ~ ., distribution = &quot;gaussian&quot;, data = boston.train, ## n.trees = 5000, interaction.depth = 2, shrinkage = 0.01) ## A gradient boosted model with gaussian loss function. ## 5000 iterations were performed. ## There were 13 predictors of which 13 had non-zero influence. summary(boston.boost) ## var rel.inf ## rm rm 38.3955886 ## lstat lstat 29.4805422 ## dis dis 9.0886721 ## crim crim 5.7399540 ## nox nox 3.7754214 ## ptratio ptratio 3.2740541 ## black black 3.1164954 ## age age 2.9063950 ## tax tax 1.8433918 ## chas chas 0.9067974 ## indus indus 0.7627923 ## rad rad 0.5523485 ## zn zn 0.1575472 Predykcja na podstawie metody boosting boston.pred4 &lt;- predict(boston.boost, newdata = boston.test, n.trees = 5000) rmse(boston.pred4, boston.test$medv) ## [1] 3.801233 \\(RMSE\\) jest w tym przypadku nieco większe niż w lasach losowych ale sporo mniejsze niż w metodzie bagging. Wszystkie metody wzmacnianych drzew dają wyniki lepsze niż pojedyncze drzewa. Bibliografia "],
["klasyfikatory-liniowe.html", "6 Klasyfikatory liniowe 6.1 Reprezentacja progowa 6.2 Reprezentacja logitowa 6.3 Wady klasyfikatorów liniowych", " 6 Klasyfikatory liniowe Obszerną rodzinę klasyfikatorów stanowią modele liniowe (ang. linear classification models). Klasyfikacji w tej rodzinie technik dokonuje się na podstawie modeli funkcji kombinacji liniowej predyktorów. Jest to ujęcie parametryczne, w którym klasyfikacji nowej wartości dokonujemy na podstawie atrybutów obserwacji i wektora parametrów. Uczenie na podstawie zestawu treningowego polega na oszacowaniu parametrów modelu. W odróżnieniu od metod nieparametrycznych postać modelu tym razem jest znana. Każdy klasyfikator liniowy skład się z funkcji wewnętrznej (ang. inner representation function) i funkcji zewnętrznej (ang. outer representation function). Pierwsza jest funkcją rzeczywistą parametrów modelu i wartości atrybutów obserwacji \\[\\begin{equation} g(x) = F(\\mathbf{a}(x),\\mathbf{w})=\\sum_{i=0}^pw_ia_i(x)=\\mathbf{w}\\circ \\mathbf{a}(x), \\end{equation}\\] przyjmując, że \\(a_0(x)=1\\). Funkcja zewnętrzna przyporządkowuje binarnie klasy na podstawie wartości funkcji wewnętrznej. Istnieją dwa główne typy tych klasyfikacji: brzegowa - przyjmujemy, że funkcje wewnętrzne tworzą granice zbiorów obserwacji różnych klas, probabilistyczna - bazująca na tym, że funkcje wewnętrzne mogą pośrednio wykazywać prawdopodobieństwo przynależności do danej klasy. Pierwsza dzieli przestrzeń obserwacji za pomocą hiperpłaszczyzn na obszary jednorodne pod względem przynależności do klas. Druga jest próbą parametrycznej reprezentacji prawdopodobieństw przynależności do klas. Klasyfikacji na podstawie prawdopodobieństw można dokonać na różne sposoby, stosując: największe prawdopodobieństwo, funkcję najmniejszego kosztu błędnej klasyfikacji, krzywych ROC (ang. Receiver Operating Characteristic - o tym później). Podejście brzegowe lub probabilistyczne prowadzi najczęściej do dwóch typów reprezentacji funkcji zewnętrznej: reprezentacji progowej (ang. threshold representation) - najczęściej przy podejściu brzegowym, reprezentacji logistycznej (ang. logit representation) - przy podejściu probabilistycznym. 6.1 Reprezentacja progowa W przypadku klasyfikacji dwustanowej, dziedzina jest dzielona na dwa regiony (pozytywny i negatywny) poprzez porównanie funkcji zewnętrznej z wartością progową. Bez straty ogólności można sprawić, że będzie to wartość 0 \\[\\begin{equation} h(x)=H(g(x))= \\begin{cases} 1, &amp;\\text{ jeśli } g(x)\\geq 0\\\\ 0, &amp;\\text{ w przeciwnym przypadku.} \\end{cases} \\end{equation}\\] Czasami używa się parametryzacji \\(\\{-1,1\\}\\). Przez porównanie \\(g(x)\\) z 0 definiuje się hiperpłaszczyznę w \\(p\\) wymiarowej przestrzeni, która rozdziela dziedzinę na regiony pozytywne i negatywne. W tym ujęciu mówimy o liniowej separowalności obserwacji różnych klas, jeśli istnieje hiperpłaszczyzna je rozdzielająca. 6.2 Reprezentacja logitowa Najbardziej popularną reprezentacją parametryczną stosowaną w klasyfikacji jest reprezentacja logitowa \\[\\begin{equation} \\P(y=1|x)=\\frac{e^{g(x)}}{e^{g(x)}+1}. \\end{equation}\\] Wówczas \\(g(x)\\) nie reprezentuje bezpośrednio \\(\\P(y=1|x)\\) ale jego logit \\[\\begin{equation} g(x)=\\logit(\\P(y=1|x)), \\end{equation}\\] gdzie \\(\\logit(p)=\\ln\\frac{p}{1-p}\\). Dlatego właściwa postać reprezentacji jest następująca \\[\\begin{equation} \\P(y=1|x)=\\logit^{-1}(g(x)). \\end{equation}\\] W ten sposób reprezentacja logitowa jest równoważna reprezentacji progowej, ponieważ \\[\\begin{equation} g(x)=\\ln\\frac{\\P(y=1|x)}{1-\\P(y=1|x)}=\\ln\\frac{\\P(y=1|x)}{\\P(y=0|x)}&gt;0. \\end{equation}\\] Jednak zaletą reprezentacji logitowej, w porównaniu do progowej, jest to, że można wyznaczyć prawdopodobieństwa przynależności do obu klas. W przypadku klasyfikacji wielostanowej uczymy tyle funkcji \\(h\\) ile jest klas. 6.3 Wady klasyfikatorów liniowych tylko w przypadku prostych funkcji wewnętrznych jesteśmy w stanie ocenić wpływ poszczególnych predykorów na klasyfikację, jakość predykcji zależy od doboru funkcji wewnętrznej (liniowa w ścisłym sensie jest najczęściej niewystarczająca), nie jest w stanie klasyfikować poprawnie stanów (nie jest liniowo separowalna) w zagadnieniach typu XOR. "],
["regresja-logistyczna.html", "7 Regresja logistyczna 7.1 Model 7.2 Estymacja parametrów modelu 7.3 Interpretacja", " 7 Regresja logistyczna 7.1 Model Regresja logistyczna (ang. logistic regression) jest techniką z rodziny klasyfikatorów liniowych z reprezentacją logistyczną, a formalnie należy do rodziny uogólnionych modeli liniowych (GLM). Stosowana jest wówczas, gdy zmienna wynikowa posiada dwa stany (sukces i porażka), kodowane najczęściej za pomocą 1 i 0. W tej metodzie modelowane jest warunkowe prawdopodobieństwo sukcesu za pomocą kombinacji liniowej predyktorów \\(X\\). Ogólna postać modelu \\[\\begin{align} Y\\sim &amp;B(1, p)\\\\ p(X)=&amp;\\E(Y|X)=\\frac{\\exp(\\beta X)}{1+\\exp(\\beta X)}, \\end{align}\\] gdzie \\(B(1,p)\\) jest rozkładem dwumianowym o prawdopodobieństwie sukcesu \\(p\\), a \\(\\beta X\\) oznacza kombinację liniową parametrów modelu i wartości zmiennych niezależnych, przyjmując, że \\(x_0=1\\). Jako funkcji łączącej (czyli opisującej związek między kombinacją liniową predyktorów i prawdopodobieństwem sukcesu) użyto logitu. Pozwala on na wygodną interpretację wyników w terminach szans. Szansą (ang. odds) nazywamy stosunek prawdopodobieństwa sukcesu do prawdopodobieństwa porażki \\[\\begin{equation} o = \\frac{p}{1-p}. \\end{equation}\\] Ponieważ będziemy przyjmowali, że \\(p\\in (0,1)\\), to \\(o\\in (0,\\infty)\\), a jej logarytm należy do przedziału \\((-\\infty, \\infty)\\). Zatem logarytm szansy jest kombinacją liniową predyktorów \\[\\begin{equation} \\log\\left[\\frac{p(X)}{1-p(X)}\\right]=\\beta_0+\\beta_1x_1+\\ldots+\\beta_dx_d. \\end{equation}\\] 7.2 Estymacja parametrów modelu Estymacji parametrów modelu logistycznego dokonujemy za pomocą metody największej wiarogodności. Funkcja wiarogodności w tym przypadku przyjmuje postać \\[\\begin{equation} L(X_1,\\ldots,X_n,\\beta)=\\prod_{i=1}^{n}p(X_i)^Y_i[1-p(X_i)]^{1-Y_i}, \\end{equation}\\] gdzie wektor \\(\\beta\\) jest uwikłany w funkcji \\(p(X_i)\\). Maksymalizacji dokonujemy raczej po nałożeniu na funkcję wiarogodności logarytmu, bo to ułatwia szukanie ekstremum. \\[\\begin{equation} \\log L(X_1,\\ldots,X_n,\\beta) = \\sum_{i=1}^n(Y_i\\log p(X_i)+(1-Y_i)\\log(1-p(X_i))). \\end{equation}\\] 7.3 Interpretacja Interpretacja (lat. ceteris paribus - “inne takie samo”) poszczególnych parametrów modelu jest następująca: jeśli \\(b_i&gt;0\\) - to zmienna \\(x_i\\) ma wpływ stymulujący pojawienie się sukcesu, jeśli \\(b_i&lt;0\\) - to zmienna \\(x_i\\) ma wpływ ograniczający pojawienie się sukcesu, jeśli \\(b_i=0\\) - to zmienna \\(x_i\\) nie ma wpływu na pojawienie się sukcesu. Iloraz szans (ang. odds ratio) stosuje się w przypadku porównywania dwóch klas obserwacji. Jest on jak sama nazwa wskazuje ilorazem szans zajścia sukcesu w obu klasach \\[\\begin{equation} OR = \\frac{p_1}{1-p_1}\\frac{1-p_2}{p_2}, \\end{equation}\\] gdzie \\(p_i\\) oznacza zajście sukcesu w \\(i\\)-tej klasie. Interpretujemy go następująco: jeśli \\(OR&gt;1\\) - to w pierwszej grupie zajście sukcesu jest bardziej prawdopodobne, jeśli \\(OR&lt;1\\) - to w drugiej grupie zajście sukcesu jest bardziej prawdopodobne, jeśli \\(OR=1\\) - to w obu grupach zajście sukcesu jest jednakowo prawdopodobne. Przykład 7.1 Jako ilustrację działania regresji logistycznej użyjemy modelu dla danych ze zbioru Default pakietu ISLR. library(ISLR) head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 Zmienną zależną jest default, a pozostałe są predyktorami. najpierw dokonamy podziału próby na ucząca i testową, a następnie zbudujemy model. set.seed(2019) ind &lt;- sample(1:nrow(Default), size = 2/3*nrow(Default)) dt.ucz &lt;- Default[ind,] dt.test &lt;- Default[-ind,] mod.logit &lt;- glm(default~., dt.ucz, family = binomial(&quot;logit&quot;)) summary(mod.logit) ## ## Call: ## glm(formula = default ~ ., family = binomial(&quot;logit&quot;), data = dt.ucz) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1913 -0.1410 -0.0537 -0.0192 3.7527 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.128e+01 6.169e-01 -18.287 &lt;2e-16 *** ## studentYes -4.627e-01 2.862e-01 -1.617 0.106 ## balance 5.830e-03 2.883e-04 20.221 &lt;2e-16 *** ## income 9.460e-06 9.833e-06 0.962 0.336 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1967.2 on 6665 degrees of freedom ## Residual deviance: 1046.5 on 6662 degrees of freedom ## AIC: 1054.5 ## ## Number of Fisher Scoring iterations: 8 Tylko income nie ma żadnego wpływu na prawdopodobieństwo stanu Yes zmiennej default. Zmienna balance wpływa stymulująco na prawdopodobieństwo pojawienia się sukcesu. Natomiast jeśli badana osoba jest studentem (studentYes), to ma wpływ ograniczający na pojawienie się sukcesu. Chcąc porównać dwie grupy obserwacji, przykładowo studentów z nie-studentami, możemy wykorzystać iloraz szans. exp(cbind(OR = coef(mod.logit), confint(mod.logit))) %&gt;% kable(digits = 4) OR 2.5 % 97.5 % (Intercept) 0.0000 0.0000 0.0000 studentYes 0.6296 0.3598 1.1060 balance 1.0058 1.0053 1.0064 income 1.0000 1.0000 1.0000 Z powyższej tabeli wynika, że bycie studentem zmniejsza szanse na Yes w zmiennej default o około 37% (w stosunku do nie-studentów). Natomiast wzrost zmiennej balance przy zachowaniu pozostałych zmiennych na tym samym poziomie skutkuje wzrostem szans na Yes o około 0.6%. Chcąc przeprowadzić predykcję na podstawie modelu dla ustalonych wartości cech (np. student = Yes, balance = $1000 i income = $40000) postępujemy następująco dt.new &lt;- data.frame(student = &quot;Yes&quot;, balance = 1000, income = 40000) predict(mod.logit, newdata = dt.new, type = &quot;response&quot;) ## 1 ## 0.003931398 Otrzymany wynik jest oszacowanym prawdopodobieństwem warunkowym wystąpienia sukcesu (default = Yes). Widać zatem, że poziomy badanych cech sprzyjają raczej porażce. Jeśli chcemy sprawdzić jakość klasyfikacji na zbiorze testowym, to musimy ustalić na jakim poziomie prawdopodobieństwa będziemy uznawać obserwację za sukces. W zależności od tego, na predykcji jakiego stanu zależy nam bardziej, możemy różnie dobierać ten próg (bez żadnych dodatkowych przesłanek najczęściej jest to 0.5). pred &lt;- predict(mod.logit, newdata = dt.test, type = &quot;response&quot;) pred.class &lt;- ifelse(pred &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) (tab &lt;- table(pred.class, dt.test$default)) ## ## pred.class No Yes ## No 3211 71 ## Yes 15 37 (acc &lt;- sum(diag(prop.table(tab)))) ## [1] 0.9742052 Klasyfikacja na poziomie 97% wskazuje na dobre dopasowanie modelu. "],
["LDA.html", "8 Analiza dyskryminacyjna 8.1 Liniowa analiza dyskryminacyjna Fisher’a 8.2 Liniowa analiza dyskryminacyjna - podejście probabilistyczne 8.3 Analiza dyskryminacyjna metodą częściowych najmniejszych kwadratów 8.4 Regularyzowana analiza dyskryminacyjna 8.5 Analiza dyskryminacyjna mieszana 8.6 Elastyczna analiza dyskryminacyjna", " 8 Analiza dyskryminacyjna Analiza dyskryminacyjna (ang. discriminant analysis) jest grupą technik dyskryminacji obserwacji względem przynależności do klas. Część z nich należy do klasyfikatorów liniowych (choć nie zawsze w ścisłym sensie). Za autorów tej metody uważa się Fisher’a (1936) i Welch’a (1939). Każdy z nich prezentował nieco inne podejście do tematu klasyfikacji. Welch poszukiwał klasyfikacji minimalizującej prawdopodobieństwo błędnej klasyfikacji, znane jako klasyfikatory bayesowskie. Podejście Fisher’a skupiało się raczej na porównaniu zmienności międzygrupowej do zmienności wewnątrzgrupowej. Wychodząc z założenia, że iloraz tych wariancji powinien być stosunkowo duży przy różnych klasach, jeśli do ich opisu użyjemy odpowiednich zmiennych niezależnych. W istocie chodzi o znalezienie takiego wektora, w kierunku którego wspomniany iloraz wariancji jest największy. 8.1 Liniowa analiza dyskryminacyjna Fisher’a 8.1.1 Dwie kategorie zmiennej grupującej Niech \\(\\boldsymbol D\\) będzie zbiorem zawierającym \\(n\\) punktów \\(\\{\\boldsymbol x, y\\}\\), gdzie \\(\\boldsymbol x\\in \\mathbb{R}^d\\), a \\(y\\in \\{c_1,\\ldots,c_k\\}\\). Niech \\(\\boldsymbol D_i\\) oznacza podzbiór punktów zbioru \\(\\boldsymbol D\\), które należą do klasy \\(c_i\\), czyli \\(\\boldsymbol D_i=\\{\\boldsymbol x|y=c_i\\}\\) i niech \\(|\\boldsymbol D_i|=n_i\\). Na początek załóżmy, że \\(\\boldsymbol D\\) składa się tylko z \\(\\boldsymbol D_1\\) i \\(\\boldsymbol D_2\\). Niech \\(\\boldsymbol w\\) będzie wektorem jednostkowym (\\(\\boldsymbol w&#39;\\boldsymbol w=1\\)), wówczas rzut ortogonalny punku \\(\\boldsymbol x_i\\) na wektor \\(\\boldsymbol w\\) można zapisać następująco \\[\\begin{equation} \\tilde{\\boldsymbol x}=\\left(\\frac{\\boldsymbol w&#39;\\boldsymbol x}{\\boldsymbol w&#39;\\boldsymbol w}\\right)\\boldsymbol w=(\\boldsymbol w&#39;\\boldsymbol x)\\boldsymbol w = a\\boldsymbol w, \\end{equation}\\] gdzie \\(a\\) jest współrzędną punktu \\(\\tilde{\\boldsymbol x}\\) w kierunku wektora \\(\\boldsymbol w\\), czyli \\[\\begin{equation} a=\\boldsymbol w&#39;\\boldsymbol x. \\end{equation}\\] Zatem \\((a_1,\\ldots,a_n)\\) reprezentują odwzorowanie \\(\\mathbb{R}^d\\) w \\(\\mathbb{R}\\), czyli z \\(d\\)-wymiarowej przestrzeni w przestrzeń generowaną przez \\(\\boldsymbol w\\). Rysunek 8.1: Rzut ortogonalny punktów w kierunku wektora \\(\\boldsymbol w\\) Każdy punkt należy do pewnej klasy, dlatego możemy wyliczyć \\[\\begin{align} m_1=&amp;\\frac{1}{n_1}\\sum_{ \\boldsymbol x\\in \\boldsymbol D_1}a=\\\\ =&amp;\\frac{1}{n_1}\\sum_{ \\boldsymbol x\\in \\boldsymbol D_1} \\boldsymbol w&#39; \\boldsymbol x=\\\\ =&amp; \\boldsymbol w&#39;\\left(\\frac{1}{n_1}\\sum_{ \\boldsymbol x\\in \\boldsymbol D_1} \\boldsymbol x \\right)=\\\\ =&amp; \\boldsymbol w&#39; \\boldsymbol{\\mu}_1, \\tag{8.1} \\end{align}\\] gdzie \\(\\boldsymbol \\mu_1\\) jest wektorem średnich punktów z \\(\\boldsymbol D_1\\). W podobny sposób można policzyć \\(m_2 = \\boldsymbol w&#39; \\boldsymbol \\mu_2\\). Oznacza to, że średnia projekcji jest projekcją średnich. Rozsądnym wydaje się teraz poszukać takiego wektora, aby \\(|m_1-m_2|\\) była maksymalnie duża przy zachowaniu niezbyt dużej zmienności wewnątrz grup. Dlatego kryterium Fisher’a przyjmuje postać \\[\\begin{equation} \\max_{ \\boldsymbol w}J(\\boldsymbol w)=\\frac{(m_1-m_2)^2}{ss_1^2+ss_2^2}, \\tag{8.2} \\end{equation}\\] gdzie \\(ss_j^2=\\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}(a-m_j)^2=n_j\\sigma_j^2.\\) Zauważmy, że licznik w (8.2) da się zapisać jako \\[\\begin{align} (m_1-m_2)^2=&amp; ( \\boldsymbol w&#39;( \\boldsymbol \\mu_1- \\boldsymbol \\mu_2))^2=\\\\ =&amp; \\boldsymbol w&#39;(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)&#39;\\boldsymbol w=\\\\ =&amp; \\boldsymbol w&#39; \\boldsymbol B \\boldsymbol w \\end{align}\\] gdzie \\(\\boldsymbol B=(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)&#39;\\) jest macierzą \\(d\\times d\\). Ponadto \\[\\begin{align} ss_j^2=&amp;\\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}(a-m_j)^2=\\\\ =&amp;\\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}( \\boldsymbol w&#39; \\boldsymbol x- \\boldsymbol w&#39; \\boldsymbol\\mu_j)^2=\\\\ =&amp; \\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}( \\boldsymbol{w}&#39;( \\boldsymbol{x}- \\boldsymbol{\\mu}_j))^2=\\\\ =&amp; \\boldsymbol{w}&#39;\\left(\\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}(\\boldsymbol{x}-\\boldsymbol \\mu_j)(\\boldsymbol x- \\boldsymbol \\mu_j)&#39;\\right) \\boldsymbol{w}=\\\\ =&amp; \\boldsymbol{w}&#39; \\boldsymbol{S}_j \\boldsymbol{w}, \\tag{8.3} \\end{align}\\] gdzie \\(\\boldsymbol{S}_j=n_j \\boldsymbol{\\Sigma}_j\\). Zatem mianownik (8.2) możemy zapisać jako \\[\\begin{equation} ss_1^2+ss_2^2= \\boldsymbol{w}&#39;(\\boldsymbol{S}_1+ \\boldsymbol{S}_2) \\boldsymbol{w}= \\boldsymbol{w}&#39; \\boldsymbol{S} \\boldsymbol{w}, \\end{equation}\\] gdzie \\(\\boldsymbol{S}=\\boldsymbol{S}_1+\\boldsymbol{S}_2\\). Ostatecznie warunek Fisher’a przyjmuje postać \\[\\begin{equation} \\max_{ \\boldsymbol{w}}J( \\boldsymbol{w})=\\frac{ \\boldsymbol{w}&#39; \\boldsymbol{B} \\boldsymbol{w}}{ \\boldsymbol{w}&#39; \\boldsymbol{S} \\boldsymbol{w}}. \\tag{8.4} \\end{equation}\\] Różniczkując (8.4) po \\(\\boldsymbol{w}\\) i przyrównując go do 0, otrzymamy warunek \\[\\begin{equation} \\boldsymbol{B} \\boldsymbol{w} = \\lambda \\boldsymbol{S} \\boldsymbol{w}, \\tag{8.5} \\end{equation}\\] gdzie \\(\\lambda=J(\\boldsymbol{w})\\). Maksimum (8.5) jest osiągane dla wektora \\(\\boldsymbol{w}\\) równego wektorowi własnemu odpowiadającemu największej wartości własnej równania charakterystycznego \\(|\\boldsymbol{B}-\\lambda\\boldsymbol{S}|=0\\). Jeśli \\(\\boldsymbol{S}\\) nie jest osobliwa, to rozwiązanie (8.5) otrzymujemy przez znalezienie największej wartości własnej macierzy \\(\\boldsymbol{B}\\boldsymbol{S}^{-1}\\) lub bez wykorzystania wartości i wektorów własnych. Ponieważ \\(\\boldsymbol{B}\\boldsymbol w=\\left((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\right)\\boldsymbol{w}\\) jest macierzą \\(d \\times 1\\) rzędu 1, to \\(\\boldsymbol{B}\\boldsymbol{w}\\) jest punktem na kierunku wyznaczonym przez wektor \\(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2\\), bo \\[\\begin{align} \\boldsymbol{B}\\boldsymbol{w}=&amp; \\left((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\right)\\boldsymbol{w}=\\\\ =&amp;(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\boldsymbol{w})=\\\\ =&amp; b(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2), \\end{align}\\] gdzie \\(b = (\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\boldsymbol{w}\\) jest skalarem. Wówczas (8.5) zapiszemy jako \\[\\begin{gather} b(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2) = \\lambda\\boldsymbol{S}\\boldsymbol{w}\\\\ \\boldsymbol{w}= \\frac{b}{\\lambda}\\boldsymbol{S}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2) \\end{gather}\\] A ponieważ \\(b/\\lambda\\) jest liczbą, to kierunek najlepszej dyskryminacji grup wyznacza wektor \\[\\begin{equation} \\boldsymbol{w}=\\boldsymbol{S}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2). \\end{equation}\\] Rysunek 8.2: Rzut ortogonalny w kierunku wektora \\(\\boldsymbol{w}\\), będącego najlepiej dyskryminującym obie grupy obserwacji 8.1.2 \\(k\\)-kategorii zmiennej grupującej Uogólnieniem tej teorii na przypadek \\(k\\) klas otrzymujemy przez uwzględnienie \\(k-1\\) funkcji dyskryminacyjnych. Zmienność wewnątrzgrupowa przyjmuje wówczas postać \\[\\begin{equation} \\boldsymbol{S}_W=\\sum_{i=1}^k\\boldsymbol{S}_i, \\end{equation}\\] gdzie \\(\\boldsymbol{S}_i\\) jest zdefiniowane jak w (8.3). Niech średnia i zmienność całkowita będą dane wzorami \\[\\begin{equation} \\boldsymbol{m}=\\frac{1}{n}\\sum_{i=1}^kn_i\\boldsymbol{m}_i, \\end{equation}\\] \\[\\begin{equation} \\boldsymbol{S}_T=\\sum_{j=1}^k\\sum_{\\boldsymbol{x}\\in D_j}(\\boldsymbol{x}-\\boldsymbol{m})(\\boldsymbol{x}-\\boldsymbol{m})&#39; \\end{equation}\\] gdzie \\(\\boldsymbol{m}_i\\) jest określone jak w (8.1). Wtedy zmienność międzygrupową możemy wyrazić jako \\[\\begin{equation} \\boldsymbol{S}_B=\\sum_{i=1}^kn_i(\\boldsymbol{m}_i-\\boldsymbol{m})(\\boldsymbol{m}_i-\\boldsymbol{m})&#39;, \\end{equation}\\] bo \\(\\boldsymbol{S}_T=\\boldsymbol{S}_W+\\boldsymbol{S}_B.\\) Określamy projekcję \\(d\\)-wymiarowej przestrzeni na \\(k-1\\)-wymiarową przestrzeń za pomocą \\(k-1\\) funkcji dyskryminacyjnych postaci \\[\\begin{equation} a_j=\\boldsymbol{w}_j&#39;\\boldsymbol{x}, \\quad j=1,\\ldots,k-1. \\end{equation}\\] Połączone wszystkie \\(k-1\\) rzutów możemy zapisać jako \\[\\begin{equation} \\boldsymbol{a}=\\boldsymbol{W}&#39;\\boldsymbol{x}. \\end{equation}\\] W nowej przestrzeni \\(k-1\\)-wymiarowej możemy zdefiniować \\[\\begin{equation} \\tilde{\\boldsymbol{m}}=\\frac{1}{n}\\sum_{i=1}^kn_i\\tilde{\\boldsymbol{m}}_i, \\end{equation}\\] gdzie \\(\\tilde{\\boldsymbol{m}}_i= \\frac{1}{n_i}\\sum_{\\boldsymbol{a}\\in A_i}\\boldsymbol{a}\\), a \\(A_i\\) jest projekcją obiektów z \\(i\\)-tej klasy na hiperpowierzchnię generowaną przez \\(\\boldsymbol{W}\\). Dalej możemy zdefiniować zmienności miedzy- i wewnątrzgrupowe dla obiektów przekształconych przez \\(\\boldsymbol{W}\\) \\[\\begin{align} \\tilde{\\boldsymbol{S}}_W=&amp;\\sum_{i=1}^k\\sum_{\\boldsymbol{a}\\in A_i}(\\boldsymbol{a}-\\tilde{\\boldsymbol{m}})(\\boldsymbol{a}-\\tilde{\\boldsymbol{m}})&#39;\\\\ \\tilde{\\boldsymbol{S}}_B=&amp;\\sum_{i=1}^kn_i(\\tilde{\\boldsymbol{m}}_i-\\tilde{\\boldsymbol{m}})(\\tilde{\\boldsymbol{m}}_i-\\tilde{\\boldsymbol{m}})&#39;. \\end{align}\\] Łatwo można zatem pokazać, że \\[\\begin{align} \\tilde{\\boldsymbol{S}}_W = &amp; \\boldsymbol{W}&#39;\\boldsymbol{S}_W\\boldsymbol{W}\\\\ \\tilde{\\boldsymbol{S}}_B = &amp; \\boldsymbol{W}&#39;\\boldsymbol{S}_B\\boldsymbol{W}. \\end{align}\\] Ostatecznie warunek (8.2) w \\(k\\)-wymiarowym ujęciu można przedstawić jako \\[\\begin{equation} \\max_{\\boldsymbol{W}}J(\\boldsymbol{W})=\\frac{\\tilde{\\boldsymbol{S}}_W}{\\tilde{\\boldsymbol{S}}_B}=\\frac{\\boldsymbol{W}&#39;\\boldsymbol{S}_W\\boldsymbol{W}}{\\boldsymbol{W}&#39;\\boldsymbol{S}_B\\boldsymbol{W}}. \\end{equation}\\] Maksimum można znaleźć poprzez rozwiązanie równania charakterystycznego \\[\\begin{equation} |\\boldsymbol{S}_B-\\lambda_i\\boldsymbol{S}_W|=0 \\end{equation}\\] dla każdego \\(i\\). Przykład 8.1 Dla danych ze zbioru iris przeprowadzimy analizę dyskryminacji. Implementację metody LDA znajdziemy w pakiecie MASS w postaci funkcji lda. Zaczynamy od standaryzacji zmiennych i podziału próby na uczącą i testową. library(MASS) library(tidyverse) iris.std &lt;- iris %&gt;% mutate_if(is.numeric, scale) set.seed(44) ind &lt;- sample(nrow(iris.std), size = 100) dt.ucz &lt;- iris.std[ind,] dt.test &lt;- iris.std[-ind,] Budowa modelu mod.lda &lt;- lda(Species~., data = dt.ucz) mod.lda$prior ## setosa versicolor virginica ## 0.32 0.32 0.36 Prawdopodobieństwa a priori przynależności do klas przyjęto na podstawie próby uczącej. mod.lda$means ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa -1.05240186 0.7790042 -1.2968064 -1.2536554 ## versicolor 0.04201557 -0.6764307 0.2521529 0.1607657 ## virginica 0.87352122 -0.2398799 1.0182730 1.0868584 W części means wyświetlone są średnie poszczególnych zmiennych niezależnych w podziale na grupy. Dzięki temu można określić położenia środków ciężkości poszczególnych klas w oryginalnej przestrzeni. mod.lda$scaling ## LD1 LD2 ## Sepal.Length 0.5884101 0.04738098 ## Sepal.Width 0.7566030 -0.97757574 ## Petal.Length -3.2910346 1.41170784 ## Petal.Width -2.3799488 -1.95325155 Powyższa tabela zawiera współrzędne wektorów wyznaczających funkcje dyskryminacyjne. Na ich podstawie możemy określić, która z nich wpływa najmocniej na tworzenie się nowej przestrzeni. Obiekt svd przechowuje pierwiastki z \\(\\lambda_i\\), dlatego podnosząc je do kwadratu i dzieląc przez ich sumę otrzymamy udział poszczególnych zmiennych w dyskryminacji przypadków. Jak widać pierwsza funkcja dyskryminacyjna w zupełności by wystarczyła. mod.lda$svd^2/sum(mod.lda$svd^2) ## [1] 0.992052359 0.007947641 Klasyfikacja na podstawie modelu pred.lda &lt;- predict(mod.lda, dt.test) Wynik predykcji przechowuje trzy rodzaje obiektów: klasy, które przypisał obiektom model (class); prawdopodobieństwa a posteriori przynależności do klas na podstawie modelu (posterior); współrzędne w nowej przestrzeni LD1, LD2 (x). Sprawdzenie jakości klasyfikacji tab &lt;- table(pred = pred.lda$class, obs = dt.test$Species) tab ## obs ## pred setosa versicolor virginica ## setosa 18 0 0 ## versicolor 0 17 1 ## virginica 0 1 13 sum(diag(prop.table(tab))) ## [1] 0.96 Jak widać z powyższej tabeli model dobrze sobie radzi z klasyfikacją obiektów. Odsetek poprawnych klasyfikacji wynosi 96%. cbind.data.frame(obs = dt.test$Species, pred.lda$x, pred = pred.lda$class) %&gt;% ggplot(aes(x = LD1, y = LD2))+ geom_point(aes(color = pred, shape = obs), size = 2) Rysunek 8.3: Klasyfikacja w przestrzeni LD1, LD2 na podstawie modelu mod.lda 8.2 Liniowa analiza dyskryminacyjna - podejście probabilistyczne Jak wspomniano na wstępie (patrz rozdział 8), podejście prezentowane przez Welcha polegało na minimalizacji prawdopodobieństwa popełnienia błędu przy klasyfikacji. Cała rodzina klasyfikatorów Bayesa (patrz rozdział 9) polega na wyznaczeniu prawdopodobieństw a posteriori, na podstawie których dokonuje się decyzji o klasyfikacji obiektów. Tym razem dodajemy również założenie, że zmienne niezależne \\(\\boldsymbol{x}=(\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_d)\\) charakteryzują się wielowymiarowym rozkładem normalnym \\[\\begin{equation} f(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})&#39;\\boldsymbol{\\Sigma}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right], \\tag{8.6} \\end{equation}\\] gdzie \\(\\boldsymbol{\\mu}\\) jest wektorem średnich \\(\\boldsymbol{x}\\), a \\(\\boldsymbol{\\Sigma}\\) jest macierzą kowariancji \\(\\boldsymbol{x}\\). Uwaga. Liniowa kombinacja zmiennych losowych o normalnym rozkładzie łącznym ma również rozkład łączny normalny. W szczególności, jeśli \\(A\\) jest macierzą wymiaru \\(d\\times k\\) i \\(\\boldsymbol{y} = A&#39;\\boldsymbol{x}\\), to \\(f(\\boldsymbol{y})\\sim N(A&#39;\\boldsymbol{\\mu}, A&#39;\\boldsymbol{\\Sigma}A)\\). Odpowiednia forma macierzy przekształcenia \\(A_w\\), sprawia, że zmienne po transformacji charakteryzują się rozkładem normalnym łącznym o wariancji określonej przez \\(I\\). Jeśli \\(\\boldsymbol{\\Phi}\\) jest macierzą, której kolumny są ortonormalnymi wektorami własnymi macierzy \\(\\boldsymbol{\\Sigma}\\), a \\(\\boldsymbol{\\Lambda}\\) macierzą diagonalną wartości własnych, to transformacja \\(A_w=\\boldsymbol{\\Phi}\\boldsymbol{\\Lambda}^{-1}\\) przekształca \\(\\boldsymbol{x}\\) w \\(\\boldsymbol{y}\\sim N(A_w&#39;\\boldsymbol{\\mu}, I)\\). Rysunek 8.4: Transformacje rozkładu normalnego łącznego. Źródło: Duda, Hart, and Stork (2001) Definicja 8.1 Niech \\(g_i(\\boldsymbol{x}),\\ i=1,\\ldots,k\\) będą pewnymi funkcjami dyskryminacyjnymi. Wówczas obiekt \\(\\boldsymbol{x}\\) klasyfikujemy do grupy \\(c_i\\) jeśli spełniony jest warunek \\[\\begin{equation} g_i(\\boldsymbol{x})&gt;g_j(\\boldsymbol{x}), \\quad j\\neq i. \\end{equation}\\] W podejściu polegającym na minimalizacji prawdopodobieństwa błędnej klasyfikacji, przyjmuje się najczęściej, że \\[\\begin{equation} g_i(\\boldsymbol{x})=\\P(c_i|\\boldsymbol{x}), \\end{equation}\\] czyli jako prawdopodobieństwo a posteriori. Wszystkie trzy poniższe postaci funkcji dyskryminacyjnych są dopuszczalne i równoważne ze względu na rezultat grupowania \\[\\begin{align} g_i(\\boldsymbol{x})=&amp;\\P(c_i|\\boldsymbol{x})=\\frac{\\P(\\boldsymbol{x}|c_i)\\P(c_i)}{\\sum_{i=1}^k\\P(\\boldsymbol{x}|c_i)\\P(c_i)},\\\\ g_i(\\boldsymbol{x})=&amp;\\P(\\boldsymbol{x}|c_i)\\P(c_i),\\\\ g_i(\\boldsymbol{x})=&amp;\\log\\P(\\boldsymbol{x}|c_i)+\\log\\P(c_i) \\tag{8.7} \\end{align}\\] W przypadku gdy \\(\\boldsymbol{x}|c_i\\sim N(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)\\), to na podstawie (8.6) \\(g_i\\) danej jako (8.7) przyjmuje postać \\[\\begin{equation} g_i(\\boldsymbol{x})=-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_i)&#39;\\boldsymbol{\\Sigma}_i^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_i)-\\frac{d}{2}\\log(2\\pi)-\\frac{1}{2}\\log|\\boldsymbol{\\Sigma}_i|+\\log\\P(c_i). \\end{equation}\\] W kolejnych podrozdziałach przeanalizujemy trzy możliwe formy macierzy kowariancji. 8.2.1 Przypadek gdy \\(\\boldsymbol{\\Sigma}_i=\\sigma^2I\\) To najprostszy przypadek, zakładający niezależność zmiennych wchodzących w skład \\(\\boldsymbol x\\), których wariancje są stałe \\(\\sigma^2\\). Wówczas \\(g_i\\) przyjmuje postać \\[\\begin{equation} g_i(\\boldsymbol x)=-\\frac{||\\boldsymbol x-\\boldsymbol \\mu_i||^2}{2\\sigma^2}+\\log\\P(c_i), \\tag{8.8} \\end{equation}\\] gdzie \\(||\\cdot ||\\) jest normą euklidesową. Rozpisując licznik równania (8.8) mamy \\[\\begin{equation} ||\\boldsymbol x-\\boldsymbol \\mu_i||^2=(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;(\\boldsymbol x-\\boldsymbol \\mu_i). \\end{equation}\\] Zatem \\[\\begin{equation} g_i(\\boldsymbol x)=-\\frac{1}{2\\sigma^2}[\\boldsymbol x&#39;\\boldsymbol x-2\\boldsymbol \\mu_i&#39;\\boldsymbol x+\\boldsymbol \\mu_i&#39;\\boldsymbol \\mu_i]+\\log\\P(c_i). \\end{equation}\\] A ponieważ \\(\\boldsymbol x&#39;\\boldsymbol x\\) nie zależy do \\(i\\), to funkcję dyskryminacyjną możemy zapisać jako \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0}, \\end{equation}\\] gdzie \\(\\boldsymbol w_i=\\frac{1}{\\sigma^2}\\boldsymbol \\mu_i\\), a \\(w_{i0}=\\frac{-1}{2\\sigma^2}\\boldsymbol \\mu_i&#39;\\boldsymbol \\mu_i+\\log\\P(c_i).\\) Na podstawie funkcji dyskryminacyjnych wyznaczamy hiperpłaszczyzny decyzyjne jako zbiory punktów dla których \\(g_i(\\boldsymbol x)=g_j(\\boldsymbol x)\\), gdzie \\(g_i, g_j\\) są największe. Możemy to zapisać w następujący sposób \\[\\begin{equation} \\boldsymbol w&#39;(\\boldsymbol x-\\boldsymbol x_0)=0, \\tag{8.9} \\end{equation}\\] gdzie \\[\\begin{equation} \\boldsymbol w = \\boldsymbol \\mu_i-\\boldsymbol \\mu_j, \\end{equation}\\] oraz \\[\\begin{equation} \\boldsymbol x_0 = \\frac12(\\boldsymbol \\mu_i+\\boldsymbol\\mu_j)-\\frac{\\sigma^2}{||\\boldsymbol \\mu_i-\\boldsymbol \\mu_j||^2}\\log\\frac{\\P(c_i)}{\\P(c_j)}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j). \\end{equation}\\] Równanie (8.9) określa hiperpłaszczyznę przechodzącą przez \\(\\boldsymbol x_0\\) i prostopadłą do \\(\\boldsymbol w\\). Rysunek 8.5: Dyskrymiancja hiperpłaszczyznami w sygucaji dwóch klas. Wykres po lewej, to ujęcie jednowymiarowe, wykresy po środu - ujęcie 2-wymiarowe i wykresy po prawej, to ujęcie 3-wymiarowe. Źródło: Duda, Hart, and Stork (2001) 8.2.2 Przypadek gdy \\(\\boldsymbol \\Sigma_i=\\boldsymbol \\Sigma\\) Przypadek ten opisuje sytuację, gdy rozkłady \\(\\boldsymbol x\\) są identyczne we wszystkich grupach ale zmienne w ich skład wchodzące nie są niezależne. W tym przypadku funkcje dyskryminacyjne przyjmują postać \\[\\begin{equation} g_i(\\boldsymbol x)=\\frac12(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;\\boldsymbol\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu_i)+\\log\\P(c_i). \\tag{8.10} \\end{equation}\\] Jeśli \\(\\P(c_i)\\) są identyczne dla wszystkich klas, to można je pominąć we wzorze (8.10). Metryka euklidesowa ze wzoru (8.8) została zastąpiona przez odległość Mahalonobis’a. Podobnie ja w przypadku gdy \\(\\boldsymbol \\Sigma_i=\\sigma^2I\\), tak i teraz można uprościć (8.10) przez rozpisanie formy kwadratowej, aby otrzymać, że \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0}, \\end{equation}\\] gdzie \\(\\boldsymbol w_i=\\boldsymbol\\Sigma^{-1}\\boldsymbol \\mu_i\\), a \\(w_{i0}=-\\frac{1}{2}\\boldsymbol \\mu_i&#39;\\boldsymbol\\Sigma^{-1}\\boldsymbol \\mu_i+\\log\\P(c_i).\\) Ponieważ funkcje dyskryminacyjne są liniowe, to hiperpłaszczyzny są ograniczeniami obszarów obserwacji każdej z klas \\[\\begin{equation} \\boldsymbol w&#39;(\\boldsymbol x-\\boldsymbol x_0)=0, \\tag{8.11} \\end{equation}\\] gdzie \\[\\begin{equation} \\boldsymbol w = \\boldsymbol\\Sigma^{-1} (\\boldsymbol \\mu_i-\\boldsymbol \\mu_j), \\end{equation}\\] oraz \\[\\begin{equation} \\boldsymbol x_0 = \\frac12(\\boldsymbol \\mu_i+\\boldsymbol\\mu_j)-\\frac{\\log[ \\P(c_i)/\\P(c_j)]}{(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;\\boldsymbol\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu_i)}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j). \\end{equation}\\] Tym razem \\(\\boldsymbol{w}=\\Sigma^{-1}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j)\\) nie jest wektorem w kierunku \\(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j\\), więc hiperpłaszczyzna rozdzielająca obiekty różnych klas nie jest prostopadła do wektora \\(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j\\) ale przecina go w połowie (w punkcie \\(\\boldsymbol x_0\\)). Rysunek 8.6: Hiperpłaszczyzna rozdzielająca obszary innych klas może być przesunięta w kierunku bardziej prawdopodobnej klasy, jeśli prawdopodobieństwa a priori są różne. Źródło: Duda, Hart, and Stork (2001) 8.2.3 Przypadek gdy \\(\\boldsymbol \\Sigma_i\\) jest dowolnej postaci Jest to najbardziej ogólny przypadek, kiedy nie nakłada się żadnych ograniczeń na macierze kowariancji grupowych. Postać funkcji dyskryminacyjnych jest następująca \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol x&#39;\\boldsymbol W_i\\boldsymbol x+\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0} \\tag{8.12} \\end{equation}\\] gdzie \\[\\begin{align} \\boldsymbol W_i = &amp;-\\frac12 \\boldsymbol\\Sigma_i^{-1},\\\\ \\boldsymbol w_i=&amp; \\boldsymbol\\Sigma_i^{-1}\\boldsymbol\\mu_i,\\\\ w_{i0} = &amp;-\\frac12\\boldsymbol\\mu_i&#39;\\boldsymbol\\Sigma_i^{-1}\\boldsymbol\\mu_i-\\frac12\\log|\\boldsymbol\\Sigma_i|+\\log\\P(c_i). \\end{align}\\] Ograniczenia w ten sposób budowane są hiperpowierzchniami (nie koniecznie hiperpłaszczyznami). W literaturze ta metoda znana jest pod nazwą kwadratowej analizy dyskryminacyjnej (ang. Quadratic Discriminant Analysis). Rysunek 8.7: Przykład zastosowania kwadratowej analizy dyskryminacyjnej. Pokazane są dopuszczalne postaci zbiorów ograniczających. Źródło: Duda, Hart, and Stork (2001) Przykład 8.2 Przeprowadzimy klasyfikację na podstawie zbioru Smarket pakietu ILSR. Dane zawierają kursy indeksu giełdowego S&amp;P500 w latach 2001-2005. Na podstawie wartości waloru z poprzednich 2 dni będziemy chcieli przewidzieć czy ruch w kolejnym okresie czasu będzie w górę czy w dół. library(ISLR) head(Smarket) ## Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction ## 1 2001 0.381 -0.192 -2.624 -1.055 5.010 1.1913 0.959 Up ## 2 2001 0.959 0.381 -0.192 -2.624 -1.055 1.2965 1.032 Up ## 3 2001 1.032 0.959 0.381 -0.192 -2.624 1.4112 -0.623 Down ## 4 2001 -0.623 1.032 0.959 0.381 -0.192 1.2760 0.614 Up ## 5 2001 0.614 -0.623 1.032 0.959 0.381 1.2057 0.213 Up ## 6 2001 0.213 0.614 -0.623 1.032 0.959 1.3491 1.392 Up set.seed(44) dt.ucz &lt;- Smarket %&gt;% mutate_if(is.numeric, scale) %&gt;% sample_frac(size = 2/3) dt.test &lt;- Smarket[-as.numeric(rownames(dt.ucz)),] mod.qda &lt;- qda(Direction~Lag1+Lag2, data = dt.ucz) mod.qda ## Call: ## qda(Direction ~ Lag1 + Lag2, data = dt.ucz) ## ## Prior probabilities of groups: ## Down Up ## 0.4909964 0.5090036 ## ## Group means: ## Lag1 Lag2 ## Down 0.0328367 0.06722714 ## Up -0.0671615 -0.08814914 Ponieważ funkcje dyskryminacyjne mogą być nieliniowe, to podsumowanie modelu nie zawiera współczynników funkcji. Podsumowanie zawiera tylko prawdopodobieństwa a priori i średnie poszczególnych zmiennych niezależnych w klasach. pred.qda &lt;- predict(mod.qda, dt.test) tab &lt;- table(pred = pred.qda$class, dt.test$Direction) tab ## ## pred Down Up ## Down 42 34 ## Up 139 202 sum(diag(prop.table(tab))) ## [1] 0.5851319 library(klaR) partimat(Direction ~ Lag1+Lag2, data = dt.ucz, method = &quot;qda&quot;, col.correct=&#39;blue&#39;, col.wrong=&#39;red&#39;) Rysunek 8.8: Wykres klasyfikacji na podstawie QDA. Obserwacje zaznczone kolorem niebieskim są prawidłowo zaklasyfikowane, a czerwonym źle 8.3 Analiza dyskryminacyjna metodą częściowych najmniejszych kwadratów Analiza dyskryminacyjna metodą częściowych najmniejszych kwadratów (ang. Partial Least Squares Discriminant Analysis) jest wykorzystywana szczególnie w sytuacjach gdy zestaw predyktorów zwiera zmienne silnie ze sobą skorelowane. Jak wiadomo z wcześniejszych rozważań, metody dyskryminacji obserwacji są mało odporne na nadmiarowość zmiennych niezależnych. Stąd powstał pomysł zastosowania połączenia LDA z PLS (Partial Least Squares), której celem jest redukcja wymiaru przestrzeni jednocześnie maksymalizując korelację zmiennych niezależnych ze zmienną wynikową. Parametrem, który jest kontrolowany podczas budowy modelu jest liczba ukrytych zmiennych. Metoda PLSDA ma kilka implementacji w R, ale najbardziej znana jest funkcja plsda z pakietu caret (Jed Wing et al. 2018). Przykład 8.3 Kontynując poprzedni przykład przeprowadzimy klasyfikacje ruchu waloru korzystając z metody PLSDA. W przeciwieństwie do poprzednich funkcji plsda potrzebuje przekazania zbioru predyktorów i wektora zmiennej wynikowej oddzielnie, a nie za pomocą formuły. Doboru liczby zmiennych latentnych dokonamy arbitralnie. library(caret) mod.plsda &lt;- plsda(dt.ucz[,-c(1,7:9)], as.factor(dt.ucz$Direction), ncomp = 2) mod.plsda$loadings ## ## Loadings: ## Comp 1 Comp 2 ## Lag1 -0.548 0.375 ## Lag2 -0.805 -0.204 ## Lag3 -0.520 ## Lag4 -0.143 0.652 ## Lag5 0.186 0.351 ## ## Comp 1 Comp 2 ## SS loadings 1.004 1.001 ## Proportion Var 0.201 0.200 ## Cumulative Var 0.201 0.401 Dwie ukryte zmienne użyte do redukcji wymiaru przestrzeni wyjaśniają około 40% zmienności pierwotnych zmiennych. Ładunki (Loadings) pokazują kontrybucje poszczególnych zmiennych w tworzenie się zmiennych ukrytych. pred.plsda &lt;- predict(mod.plsda, dt.test[,-c(1,7:9)]) tab &lt;- table(pred.plsda, dt.test$Direction) tab ## ## pred.plsda Down Up ## Down 87 97 ## Up 94 139 sum(diag(prop.table(tab))) ## [1] 0.5419664 Ponieważ korelacje pomiędzy predyktorami w naszym przypadku nie były duże, to zastosowanie PLSDA nie poprawiło klasyfikacji w stosunku do metody QDA. cor(dt.ucz[,2:6]) ## Lag1 Lag2 Lag3 Lag4 Lag5 ## Lag1 1.000000000 -0.003318026 -0.004329303 0.02574559 0.01831679 ## Lag2 -0.003318026 1.000000000 -0.057166931 0.01209305 0.02127975 ## Lag3 -0.004329303 -0.057166931 1.000000000 0.01574896 -0.07541592 ## Lag4 0.025745592 0.012093049 0.015748958 1.00000000 -0.04607207 ## Lag5 0.018316786 0.021279754 -0.075415921 -0.04607207 1.00000000 8.4 Regularyzowana analiza dyskryminacyjna Regularyzowana analiza dyskryminacyjna (ang. Regularized Discriminant Analysis) powstała jako technika równoważąca zalety i wady LDA i QDA. Ze względu na zdolności generalizacyjne model LDA jest lepszy od QDA (mniejsza wariancja modelu), ale jednocześnie QDA ma bardziej elastyczną postać hiperpowierzchni brzegowych rozdzielających obiekty różnych klas. Dlatego Friedman (1989) wprowadził technikę będącą kompromisem pomiędzy LDA i QDA poprzez odpowiednie określenie macierzy kowariancji \\[\\begin{equation} \\tilde{\\boldsymbol \\Sigma}_i(\\lambda) = \\lambda\\boldsymbol\\Sigma_i + (1-\\lambda)\\boldsymbol\\Sigma, \\end{equation}\\] gdzie \\(\\boldsymbol \\Sigma_i\\) jest macierzą kowariancji dla \\(i\\)-tej klasy, a \\(\\boldsymbol \\Sigma\\) jest uśrednioną macierzą kowariancji wszystkich klas. Zatem odpowiedni dobór parametru \\(\\lambda\\) decyduje czy poszukujemy modelu prostszego (\\(\\lambda = 0\\) odpowiada LDA), czy bardziej elastycznego (\\(\\lambda=1\\) oznacza QDA). Dodatkowo metoda RDA pozwala na elastyczny wybór pomiędzy postaciami macierzy kowariancji wspólnej dla wszystkich klas \\(\\boldsymbol\\Sigma\\). Może ona być macierzą jednostkową, jak w przypadku 8.2.1, co oznacza niezależność predyktorów modelu, może też być jak w przypadku 8.2.2, gdzie dopuszcza się korelacje między predyktorami. Dokonuje się tego przez odpowiedni dobór parametru \\(\\gamma\\) \\[\\begin{equation} \\boldsymbol \\Sigma(\\gamma) = \\gamma\\boldsymbol \\Sigma+(1-\\gamma)\\sigma^2I. \\end{equation}\\] Przykład 8.4 Funkcja rda pakietu klaR jest implementacją powyższej metody. Ilustrają jej działania będzie klasyfikacja stanów z poprzedniego przykładu. library(klaR) mod.rda &lt;- rda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, dt.ucz) mod.rda ## Call: ## rda(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = dt.ucz) ## ## Regularization parameters: ## gamma lambda ## 0.713621809 0.004013224 ## ## Prior probabilities of groups: ## Down Up ## 0.4909964 0.5090036 ## ## Misclassification rate: ## apparent: 44.058 % ## cross-validated: 46.482 % Model został oszacowany z parametrami wyznaczonymi na podstawie sprawdzianu krzyżowego zastosowanego w funkcji rda. pred.rda &lt;- predict(mod.rda, dt.test) (tab &lt;- table(pred = pred.rda$class, dt.test$Direction)) ## ## pred Down Up ## Down 24 31 ## Up 157 205 sum(diag(prop.table(tab))) ## [1] 0.5491607 Jakość klasyfikacji jest na zbliżonym poziomie jak przy poprzednich metodach. 8.5 Analiza dyskryminacyjna mieszana Liniowa analiza dyskryminacyjna zakładała, że średnie (centroidy) w klasach są różne ale macierz kowariancji wszystkich klas jest jednakowa. Analiza dyskryminacyjna mieszana (ang. Mixture Discriminant Analysis) prezentuje jeszcze inne podejście ponieważ zakłada, że każda klasa może być charakteryzowana przez wiele wielowymiarowych rozkładów normalnych, których centroidy mogą się różnic, ale macierze kowariancji nie. Wówczas rozkład dla danej klasy jest mieszaniną rozkładów składowych, a funkcja dyskryminacyjna dla \\(i\\)-tej klasy przyjmuje postać \\[\\begin{equation} g_i(\\boldsymbol x)\\propto \\sum_{k=1}^{L_i}\\phi_{ik}g_{ik}(\\boldsymbol x), \\end{equation}\\] gdzie \\(L_i\\) jest liczbą rozkładów składających się na \\(i\\)-tą klasę, a \\(\\phi_{ik}\\) jest współczynnikiem proporcji estymowanych w czasie uczenia modelu. Przykład 8.5 Funkcja mda pakietu mda (Trevor Hastie et al. 2017) jest implementacją tej techniki w R. Jej zastosowanie pokażemy na przykładzie danych giełdowych z poprzedniego przykładu. Użyjemy domyślnych ustawień funkcji (trzy rozkłady dla każdej klasy). library(mda) mod.mda &lt;- mda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, dt.ucz) mod.mda ## Call: ## mda(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = dt.ucz) ## ## Dimension: 5 ## ## Percent Between-Group Variance Explained: ## v1 v2 v3 v4 v5 ## 63.78 95.81 98.97 99.84 100.00 ## ## Degrees of Freedom (per dimension): 6 ## ## Training Misclassification Error: 0.43337 ( N = 833 ) ## ## Deviance: 1134.288 pred.mda &lt;- predict(mod.mda, dt.test) (tab &lt;- table(pred = pred.mda, dt.test$Direction)) ## ## pred Down Up ## Down 38 46 ## Up 143 190 sum(diag(prop.table(tab))) ## [1] 0.5467626 Kolejny raz model dyskryminacyjny charakteryzuje się podobną jakością klasyfikacji. 8.6 Elastyczna analiza dyskryminacyjna Zupełnie inne podejście w stosunku do wcześniejszych rozwiązań, prezentuje elastyczna analiza dyskryminacyjna (ang. Flexible Discriminant Analysis) . Kodując klasy wynikowe jako zmienne dychotomiczne (dla każdej klasy jest odrębna zmienna wynikowa) dla każdej z nich budowanych jest \\(k\\) modeli regresji. Mogą to być modele regresji penalizowanej, jak regresja grzbietowa lub LASSO, modele regresji wielomianowej albo modele regresji sklejanej (MARS), o których będzie mowa w dalszej części tego opracowania. Przykładowo, jeśli modelem bazowym jest MARS, to funkcja dyskryminacyjna \\(i\\)-tej klasy może być postaci \\[\\begin{equation} g_i(\\boldsymbol x)=\\beta_0+\\beta_1h(1-x_1)+\\beta_2h(x_2-1)+\\beta_3h(1-x_3)+\\beta_4h(x_1-1), \\end{equation}\\] gdzie \\(h\\) są tzw. funkcjami bazowymi postaci \\[\\begin{equation} h(x)= \\begin{cases} x, &amp; x&gt; 0\\\\ 0, &amp; x\\leq 0. \\end{cases} \\end{equation}\\] Klasyfikacji dokonujemy sprawdzając znak funkcji dyskryminacyjnej \\(g_i\\), jeśli jest dodatni, to funkcja przypisuje obiekt do klasy \\(i\\)-tej. W przeciwnym przypadku nie należy do tej klasy. Rysunek 8.9: Przykład klasyfikacji dwustanowej za pomocą metody FDA Przykład 8.6 Funkcja fda pakietu mda jest implementacją techniki FDA w R. Na postawie danych z poprzedniego przykładu zostanie przedstawiona zasada dziełania. Przyjmiemy domyślne ustawienia funkcji, z wyjątkiem metody estymacji modelu, jako którą przyjmiemy MARS. mod.fda &lt;- fda(Direction ~ Lag1+Lag2, dt.ucz, method = mars) mod.fda ## Call: ## fda(formula = Direction ~ Lag1 + Lag2, data = dt.ucz, method = mars) ## ## Dimension: 1 ## ## Percent Between-Group Variance Explained: ## v1 ## 100 ## ## Training Misclassification Error: 0.44418 ( N = 833 ) Ponieważ, zmienna wynikowa jest dwustanowa, to powstała tylko jedna funkcja dyskryminacyjna. Parametry modelu są następujące mod.fda$fit$coefficients ## [,1] ## [1,] 0.05846465 ## [2,] -0.17936208 pred.fda &lt;- predict(mod.fda, dt.test) (tab &lt;- table(pred = pred.fda, dt.test$Direction)) ## ## pred Down Up ## Down 40 50 ## Up 141 186 sum(diag(prop.table(tab))) ## [1] 0.5419664 Jakość klasyfikacji jest tylko nieco lepsza niż w przypadku poprzednich metod. Bibliografia "],
["bayes.html", "9 Klasyfikatory bayesowskie 9.1 Klasyfikator maximum a posteriori (MAP) 9.2 Klasyfikator największej wiarogodności (ML) 9.3 Naiwny klasyfikator Bayesa (NB)18 9.4 Zalety i wady", " 9 Klasyfikatory bayesowskie Całą gamę klasyfikatorów opartych na twierdzeniu Bayesa nazywać będziemy bayesowskimi. \\[\\begin{equation}\\label{bayes} P(A|B)=\\frac{P(A)P(B|A)}{P(B)}, \\end{equation}\\] gdzie \\(P(B)&gt;0\\). Bayesowskie reguły podejmowania decyzji dały podstawy takich metod jak: liniowa analiza dyskryminacyjna; kwadratowa analiza dyskryminacyjna; W ustaleniu klasyfikatora bayesowskiego będzie nam przyświecała cały czas ta sama reguła: jeśli znam wartości cech charakteryzujących badane obiekty oraz klasy do których należą (w próbie uczącej), to na ich podstawie mogę wyznaczyć miary prawdopodobieństw a posteriori, które pomogą mi w ustaleniu klasy do której należy nowy testowy element. W dalszej części będziemy przyjmowali następujące oznaczenia: \\(T\\) - zbiór danych uczących (treningowych), \\(T^j\\) - zbiór danych uczących dla których przyjęliśmy decyzję o przynależności do \\(j\\)-tej klasy, \\(T^j_{a_i=v}\\) - zbiór danych uczących o wartości atrybutu \\(a_i\\) równej \\(v\\) i klasy \\(j\\)-tej, \\(\\mathbb{H}\\) - przestrzeń hipotez, \\(P(h|a_1=v_1, a_2=v_2,\\ldots,a_p=v_p)\\) - prawdopodobieństwo a posteriori, że prawdziwa jest hipoteza \\(h\\in \\mathbb{H}\\), jeśli znamy atrybuty obiektu, \\(P(h)\\) - prawdopodobieństwo a priori zajścia hipotezy \\(h\\in \\mathbb{H}\\), \\(c\\) - prawdziwy stan obiektu. 9.1 Klasyfikator maximum a posteriori (MAP) Na podstawie wiedzy o atrybutach obiektu \\(x\\) podejmujemy decyzję o klasyfikacji tego obiektu zgodnie z hipotezą \\(h_{MAP}\\in \\mathbb{H}\\), która przyjmuje postać \\[\\begin{align}\\label{MAP} h_{MAP}=&amp;\\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(h|a_1=v_1, a_2=v_2,\\ldots,a_p=v_p)\\\\ =&amp; \\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h)\\cdot P(h), \\end{align}\\] gdzie ostatnia równość wynika z twierdzenia Bayesa oraz faktu, że dla konkretnego obiektu \\(x\\) wielkości atrybutów nie zależą od postawionej hipotezy. 9.2 Klasyfikator największej wiarogodności (ML) Na podstawie wiedzy o atrybutach obiektu \\(x\\) podejmujemy decyzję o klasyfikacji tego obiektu zgodnie z hipotezą \\(h_{ML}\\in \\mathbb{H}\\), która przyjmuje postać \\[\\begin{equation}\\label{ML} h_{ML}=\\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h). \\end{equation}\\] Uwaga. Obie wspomniane metody wymagają znajomości prawdopodobieństwa \\(P(a_1=v_1,a_2=v_2,\\ldots,a_p=v_p|h)\\), ale różnią się podejściem do wiedzy o prawdopodobieństwach a priori. W metodzie MAP brana pod uwagę jest wiedza o prawdopodobieństwie przynależności do poszczególnych klas, a w ML nie. Dla klasyfikacji, w których prawdopodobieństwa przynależności do klas są takie same, klasyfikatory MAP i ML są równoważne. 9.3 Naiwny klasyfikator Bayesa (NB)18 Największy problem w wyznaczeniu klasyfikatorów MAP i ML stanowi wyznaczenie rozkładu łącznego \\(P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h)\\). W naiwnym klasyfikatorze Bayesa zakłada się niezależność warunkową poszczególnych atrybutów względem klasy do której ma należeń wg hipotezy obiekt. Założenie to często nie jest spełnione i stąd nazwa przymiotnik “naiwny”. Definicja naiwnego klasyfikatora bayesowskiego różni się od klasyfikatora MAP tylko podejściem do prawdopodobieństwa a posteriori. \\[\\begin{equation}\\label{naiwny_bayes} h_{NB}=\\operatorname{arg}\\max_{h_j\\in \\mathbb{H}}P(h_j)\\prod_{i=1}^{p}P(a_i=v_i|h_j), \\end{equation}\\] gdzie \\(h_j\\) oznacza hipotezę (decyzję), że badany obiekt należy do \\(j\\)-tej klasy. Oczywiście zarówno prawdopodobieństwo a priori jak i a posteriori są wyznaczane na podstawie próby, i tak prawdopodobieństwo a priori wynosi \\[\\begin{equation}\\label{apriori} P(h_j)=P_T(h_j)=\\frac{|T^j|}{|T|}, \\end{equation}\\] gdzie \\(|A|\\) oznacza moc zbioru \\(A\\). Natomiast prawdopodobieństwo a posteriori dla \\(i\\)-tego atrybutu wynosi \\[\\begin{equation}\\label{aposteriori} P(a_i=v_i|h_j)=P_{T^j}(a_i=v_i)=\\frac{|T^j_{a_i=v_i}|}{|T^j|}. \\end{equation}\\] Na mocy powyższego możemy zauważyć, że jeżeli założenie o warunkowej niezależności jest spełnione, to klasyfikatory NB i MAP są równoważne. Chcąc przypisać klasę nowemu obiektowi powstaje problem praktyczny, polegający na tym, że dla pewnych konfiguracji atrybutów nie ma odpowiedników w nauczonym modelu. Powodem takiego stanu rzeczy jest fakt, że takie kombinacje nie wystąpiły w próbie uczącej. Istnieją dwa sposoby predykcji w takiej sytuacji: \\[\\begin{equation}\\label{pred1} P(a_i=v_i|h_j)= \\begin{cases} \\frac{|T^j_{a_i=v_i}|}{|T^j|}, &amp; T^j_{a_i=v_i}\\neq \\emptyset\\\\ \\epsilon, &amp; \\text{w przeciwnym przypadku.} \\end{cases} \\end{equation}\\] W tym przypadku przyjmuje się, że \\(\\epsilon \\ll 1/|T_j|\\). Drugi sposób wykorzystuje estymację z poprawką \\[\\begin{equation}\\label{pred2} P(a_i=v_i|h_j)=\\frac{|T^j_{a_i=v_i}|+mp}{|T^j|+mp}, \\end{equation}\\] gdzie \\(p\\) oznacza prawdopodobieństwo a priori przyjęcia przez atrybut \\(a\\) wartości \\(v\\) (najczęściej \\(p=1/|A|\\), \\(A\\) - zbiór wszystkich możliwych wartości atrybutu \\(a\\)), \\(m\\) - waga (najczęściej \\(m=|A|\\)). W przypadku gdy atrybuty są mierzone na skali ciągłej najczęściej stosuje się dyskretyzację ich do zmiennych ze skali przedziałowej. Inna metoda stosowana w przypadku ciągłych atrybutów, to użycie gęstości \\(g_i^j\\) o rozkładzie normalnym w miejsce \\(P(a_i=v_i|h_j)\\). Przy czym do obliczenia parametrów rozkładu stosujemy wzory \\[\\begin{equation}\\label{sred} m_i^j=\\frac{1}{|T^j|}\\sum_{x\\in T^j}a_i(x), \\end{equation}\\] oraz \\[\\begin{equation}\\label{odch} (s_i^j)^2=\\frac{1}{|T^j|-1}\\sum_{x\\in T^j}(a_i(x)-m_i^j)^2. \\end{equation}\\] Obsługa braków danych przez naiwny klasyfikator Bayesa jest dość prosta i opiera się na liczeniu prawdopodobieństw a posteriori wyłącznie dla obiektów, których wartości atrybutów są znane. Dlatego prawdopodobieństwa warunkowe liczy się wg wzoru \\[\\begin{equation}\\label{pr_war} P(a_i=v_i|h_j)=\\frac{|T^j_{a_i=v_i}|}{|T^j|-|T^j_{a_i=NA}|}. \\end{equation}\\] Jeśli brakujące dane nie niosą w sobie istotnych informacji dotyczących klasyfikacji obiektów, to naiwny klasyfikator Bayesa będzie działał poprawnie. Naiwny klasyfikator Bayesa jest implementowany w pakietach e1071 (Meyer et al. 2019) i klaR (Weihs et al. 2005). Przykład 9.1 Przeprowadzimy klasyfikację dla zbioru Titanic. W przypadku funkcji z pakietu e1071 nie potrzeba zamieniać tabeli na przypadki. W pakiecie klaR istnieje inna funkcja budująca klasyfikator Bayesa NaiveBayes, ale w tym przypadku jeśli zbiór jest w formie tabeli, to należy go zamienić na ramkę danych z oddzielnymi przypadkami. library(e1071) Titanic ## , , Age = Child, Survived = No ## ## Sex ## Class Male Female ## 1st 0 0 ## 2nd 0 0 ## 3rd 35 17 ## Crew 0 0 ## ## , , Age = Adult, Survived = No ## ## Sex ## Class Male Female ## 1st 118 4 ## 2nd 154 13 ## 3rd 387 89 ## Crew 670 3 ## ## , , Age = Child, Survived = Yes ## ## Sex ## Class Male Female ## 1st 5 1 ## 2nd 11 13 ## 3rd 13 14 ## Crew 0 0 ## ## , , Age = Adult, Survived = Yes ## ## Sex ## Class Male Female ## 1st 57 140 ## 2nd 14 80 ## 3rd 75 76 ## Crew 192 20 nb &lt;- naiveBayes(Survived ~ ., data = Titanic) nb$apriori ## Survived ## No Yes ## 1490 711 Poniższe tabele zawierają warunkowe prawdopodobieństwa przynależności do poszczególnych klas. nb$tables ## $Class ## Class ## Survived 1st 2nd 3rd Crew ## No 0.08187919 0.11208054 0.35436242 0.45167785 ## Yes 0.28551336 0.16596343 0.25035162 0.29817159 ## ## $Sex ## Sex ## Survived Male Female ## No 0.91543624 0.08456376 ## Yes 0.51617440 0.48382560 ## ## $Age ## Age ## Survived Child Adult ## No 0.03489933 0.96510067 ## Yes 0.08016878 0.91983122 dane &lt;- as.data.frame(Titanic) pred &lt;- predict(nb, dane) pred ## [1] Yes No No No Yes Yes Yes Yes No No No No Yes Yes Yes Yes Yes ## [18] No No No Yes Yes Yes Yes No No No No Yes Yes Yes Yes ## Levels: No Yes tab &lt;- table(pred, dane$Survived) tab ## ## pred No Yes ## No 7 7 ## Yes 9 9 sum(diag(prop.table(tab))) ## [1] 0.5 Naiwny klasyfikator spisał się bardzo słabo, ponieważ klasyfikacja na poziomie 0.5 jest taka jak przy rzucie monetą. Przykład 9.2 Przeprowadzimy klasyfikację gatunków irysów na podstawie szerokości i długości kielicha i płatka. library(klaR) set.seed(2019) uczaca &lt;- sample(1:nrow(iris), 2*nrow(iris)/3) pr.ucz &lt;- iris[uczaca,] pr.test &lt;- iris[-uczaca,] nb2 &lt;- NaiveBayes(Species~., data = pr.ucz) nb2$apriori ## grouping ## setosa versicolor virginica ## 0.35 0.32 0.33 Prawdopodobieństwa a priori zostały oszacowane na podstawie próby uczącej. Poniższe tabele zawierają średnie i odchylenia standardowe zmiennych w poszczególnych klasach. nb2$tables ## $Sepal.Length ## [,1] [,2] ## setosa 4.982857 0.3485143 ## versicolor 6.003125 0.5462390 ## virginica 6.463636 0.5808497 ## ## $Sepal.Width ## [,1] [,2] ## setosa 3.408571 0.3616721 ## versicolor 2.750000 0.3537814 ## virginica 2.960606 0.2838787 ## ## $Petal.Length ## [,1] [,2] ## setosa 1.480000 0.1875539 ## versicolor 4.275000 0.4852668 ## virginica 5.460606 0.5225774 ## ## $Petal.Width ## [,1] [,2] ## setosa 0.2657143 0.1109925 ## versicolor 1.3437500 0.2198790 ## virginica 2.0151515 0.3123712 pred &lt;- predict(nb2, newdata = pr.test) tab &lt;- table(pred$class, pr.test$Species) tab ## ## setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 1 17 sum(diag(prop.table(tab))) ## [1] 0.98 Klasyfikacja na podstawie modelu jest bardzo dobra (98%). 9.4 Zalety i wady Zalety: prostota konstrukcji i prosty algorytm; jeśli jest spełnione założenie warunkowej niezależności, to ten klasyfikator działa szybciej i czasem lepiej niż inne metody klasyfikacji; nie potrzebuje dużych zbiorów danych do estymacji parametrów; Wady: często nie spełnione założenie o warunkowej niezależności powoduje obciążenie wyników; brak możliwości wprowadzania interakcji efektów kilku zmiennych; potrzebuje założenia normalności warunkowych gęstości w przypadku ciągłych atrybutów; często istnieją lepsze klasyfikatory. Bibliografia "],
["metoda-k-najblizszych-sasiadow.html", "10 Metoda \\(k\\) najbliższych sąsiadów", " 10 Metoda \\(k\\) najbliższych sąsiadów Technika \\(k\\) najbliższych sąsiadów (ang. \\(k\\)-Nearest Neighbors) przewiduje wartość zmiennej wynikowej na podstawie \\(k\\) najbliższych obserwacji zbioru uczącego. W przeciwieństwie do wspomnianych wcześniej modeli liniowych, nie posiada ona jawnej formy i należy do klasy technik nazywanych czarnymi skrzynkami (ang. black box). Może być wykorzystywana, zarówno do zadań klasyfikacyjnych, jak i regresyjnych. W obu przypadkach predykcja dla nowych wartości predyktorów przebiega podobnie. Niech \\(\\boldsymbol x_0\\) będzie obserwacją, dla której poszukujemy wartości zmiennej wynikowej \\(y_0\\). Na podstawie zbioru obserwacji \\(\\boldsymbol x\\in T\\) zbioru uczącego wyznacza się \\(k\\) najbliższych sąsiadów19, gdzie \\(k\\) jest z góry ustaloną wartością. Następnie, jeśli zadanie ma charakter klasyfikacyjny, to \\(y_0\\) przypisuje się modę zmiennej wynikowej obserwacji będących \\(k\\) najbliższymi sąsiadami. W przypadku zadań regresyjnych \\(y_0\\) przypisuje się średnią lub medianę. Olbrzymie znaczenie dla wyników predykcji na podstawie metody kNN ma dobór metryki. Nie istnieje obiektywna technika wyboru najlepszej metryki, dlatego jej doboru dokonujemy metodą prób i błędów. Należy dodatkowo pamiętać, że wielkości mierzone \\(\\boldsymbol x\\) mogą się różnić zakresami zmienności, a co za tym idzie, mogą znacząco wpłynąć na mierzone odległości pomiędzy punktami. Dlatego zaleca się standaryzację zmiennych przed zastosowaniem metody kNN. Kolejnym parametrem, który ma znaczący wpływ na predykcję, jest liczba sąsiadów \\(k\\). Wybór zbyt małej liczby \\(k\\) może doprowadzić do przeuczenia modelu jak to jest pokazane na rysunku 10.1 Rysunek 10.1: Przykład klasyfikacji dla \\(k=1\\) Z kolei zbyt duża liczba sąsiadów powoduje obciążenie wyników (patrz rysunek 10.2) Rysunek 10.2: Przykład zastosowania 100 sąsiadów Dopiero dobór odpowiedniego \\(k\\) daje model o stosunkowo niskiej wariancji i obciążeniu. Najczęściej liczby \\(k\\) poszukujemy za pomocą próbkowania. Rysunek 10.3: Model z optymalną liczbą sąsiadów Przykład 10.1 Klasyfikację z wykorzystaniem metody kNN przeprowadzimy na przykładzie danych zbioru spam pakietu ElemStatLearn. Metoda kNN ma wiele implementacji R-owych ale na potrzeby przykładu wykorzystamy funkcję knn3 pakietu caret. Najpierw dokonamy oszacowania optymalnego \\(k\\) library(ElemStatLearn) library(tidyverse) spam.std &lt;- spam %&gt;% mutate_if(is.numeric, scale) set.seed(123) ind &lt;- sample(nrow(spam), size = nrow(spam)*2/3) dt.ucz &lt;- spam.std[ind,] dt.test &lt;- spam.std[-ind,] acc &lt;- function(pred, obs){ tab &lt;- table(pred,obs) acc &lt;- sum(diag(prop.table(tab))) acc } 1:40 %&gt;% map(~knn3(spam~., data = dt.ucz, k = .x)) %&gt;% map(~predict(.x, newdata = dt.test, type = &quot;class&quot;)) %&gt;% map_dbl(~acc(pred = .x, obs = dt.test$spam)) %&gt;% tibble(k = 1:length(.), acc=.) %&gt;% ggplot(aes(k, acc))+ geom_line() Rysunek 10.4: Ocena jakości dopasowania modelu dla różnej liczby sąsiadów Biorąc pod uwagę wykres 10.4 można rozważać 3 lub 5 sąsiadów jako optymalne rozwiązanie, ponieważ wówczas poprawność klasyfikacji jest najwyższa. Proponuje unikać rozwiązania z 1 najbliższym sąsiadem ponieważ, będzie się ono charakteryzowało duża zmiennością. Wybór \\(k=3\\) wydaje się być optymalny. mod.knn &lt;- knn3(spam~., data = dt.ucz, k = 3) mod.knn ## 3-nearest neighbor model ## Training set outcome distribution: ## ## email spam ## 1860 1207 Predykcji dokonujemy w ten sam sposób co w innych modelach klasyfikacyjnych pred.knn.class &lt;- predict(mod.knn, newdata = dt.test, type = &quot;class&quot;) head(pred.knn.class) ## [1] spam spam spam spam spam spam ## Levels: email spam pred.knn &lt;- predict(mod.knn, newdata = dt.test) head(pred.knn) ## email spam ## [1,] 0.0000000 1.0000000 ## [2,] 0.3333333 0.6666667 ## [3,] 0.3333333 0.6666667 ## [4,] 0.0000000 1.0000000 ## [5,] 0.3333333 0.6666667 ## [6,] 0.0000000 1.0000000 (tab &lt;- table(pred.knn.class, dt.test$spam)) ## ## pred.knn.class email spam ## email 869 88 ## spam 59 518 sum(diag(prop.table(tab))) ## [1] 0.9041721 metrykę można wybierać dowolnie, choć najczęściej jest to metryka euklidesowa↩ "],
["uogolnione-modele-addytywne.html", "11 Uogólnione modele addytywne 11.1 Przypadek jednowymiarowy 11.2 Przypadek wielowymiarowy 11.3 Uogólnione modele addytywne", " 11 Uogólnione modele addytywne Modele liniowe, jako techniki klasyfikacji i regresji, mają niewątpliwą zaletę - jawna postać zależności pomiędzy predyktorami i zmienną wynikową. Często w rzeczywistości tak uproszczony model nie potrafi oddać złożoności natury badanego zjawiska. Dlatego powstał pomysł aby w miejsce kombinacji liniowej predyktorów wstawić kombinację liniową ich funkcji, czyli \\[\\begin{equation} \\E(Y|X)=f(X) = \\sum_{i=1}^M\\beta_mh_m(X), \\tag{11.1} \\end{equation}\\] gdzie \\(h_m:\\mathbb{R}^d\\to\\mathbb{R}\\) nazywana często funkcją bazową (ang. linear basis expansion). Wówczas w zależności od postaci funkcji bazowej otrzymujemy modele z różnymi poziomami elastyczności: gdy \\(h_m(X)=X_m,\\ m=1,\\ldots,M\\), to otrzymujemy model liniowy; gdy \\(h_m(X)=X_j^2\\) lub \\(h_m(X)=X_jX_k\\), to otrzymujemy struktury wielomianowe, charakteryzujące się większą elastycznością modelu; gdy \\(h_m(X)=\\log X_j\\) lub \\(h_m(X)=\\sqrt{X_j}\\), to uzyskujemy nieliniowość czynników wchodzących w skład kombinacji liniowej (11.1); dopuszczalne są również kawałkami liniowe funkcje postaci \\(h_m(X)= I(l_m\\leq X_k &lt;u_m)\\), gdzie \\(I\\) jest funkcją charakterystyczną (ang. indicator) przedziału \\([l_m,u_m)\\). Zbiory wszystkich funkcji bazowych definiowanych w ten sposób tworzy słownik funkcji bazowych \\(\\mathcal{D}\\). Aby kontrolować złożoność modeli, mając do dyspozycji tak zasobny słownik, wprowadza się następujące podejścia: ogranicza się klasę dostępnych funkcji bazowych \\[\\begin{equation} f(X) = \\sum_{j=1}^df_j(X_j)=\\sum_{j=1}^d\\sum_{m=1}^{M_j}\\beta_{jm}h_{jm}(X_j), \\end{equation}\\] włącza się do modelu jedynie te funkcje ze słownika \\(\\mathcal{D}\\), które istotnie poprawiają dopasowanie modelu, używa się metod penalizowanych, czyli dopuszcza się stosowanie wszystkich funkcji bazowych ze słownika \\(\\mathcal{D}\\), ale współczynniki przy nich stojące są ograniczane. 11.1 Przypadek jednowymiarowy Dla uproszczenia rozważań przyjmiemy, że \\(X\\) jest jednowymiarowe. Rysunek 11.1: Przykładowe zastosowanie kilku rodzajów funkcji bazowych. Wykres w lewym górnym rogu powstał ze stałych na przedziałach, wykres w górnym prawym rogu powstał z liniowych funkcji bazowych na przedzialach, w lewym dolnym rogu model powstał również z liniowych funkcji bazowych na przedzialach ale z założeniem ciągłości, a prawym dolnym rogu powstał z zastosowania funcji bazowej \\(\\max(X-\\xi_1,0)\\) Rysunek 11.2: Kolejne wykresy przedstawiają coraz bardziej gładkie modele będące efektem dodawania wielomianów trzeciego stopnia na przedziałach. Na każdym kolejnym modelu mymuszone zostały silniejsze założenia dotyczące gładkości Przykładowo sześcienny splajn20 dla dwóch punków węzłowych składa się z następujących funkcji bazowych \\[\\begin{gather} h_1(X)=1,\\quad h_3(X)=X^2,\\quad h_5(X)=(X-\\xi_1)_+^3\\\\ h_2(X)=X,\\quad h_4(X)=X^3,\\quad h_6(X)=(X-\\xi_2)_+^3. \\end{gather}\\] Zachowanie wielomianów poza punktami węzłowymi jest czasami bardzo dziwne. Zdarza się, że charakteryzują się tam dużą zmiennością. Dlatego wprowadza się takie splajny aby w obszarach brzegowych zachowywały się przewidywalnie. Naturalny splajn sześcienny zakłada liniowość modelu poza węzłami brzegowymi. Dla \\(K\\) węzłów naturalny splajn sześcienny składa się z następujących funkcji bazowych \\[\\begin{gather} N_1(X)=1,\\quad N_2(X)=X,\\quad N_{k+2}(X)=d_k(X)-d_{K-1}(X), \\end{gather}\\] gdzie \\(d_k(X)=\\frac{(X-\\xi_k)^3_+-(X-\\xi_K)^3_+}{\\xi_K-\\xi_k}.\\) Estymacji parametrów modelu dokonujemy metodą najmniejszych kwadratów, minimalizując \\[\\begin{equation} RSS(f,\\lambda) = \\sum_{i=1}^N(y_i-f(x_i))^2+\\lambda\\int(f&#39;&#39;(t))^2dt, \\end{equation}\\] gdzie \\(\\lambda\\) jest parametrem wygładzania. Pierwsze wyrażenie po prawej stronie to ocena dopasowania, a drugie to kara za krzywoliniowość. Dla naturalnego splajna \\[\\begin{equation} f(x)=\\sum_{j=1}^NN_j(x)\\beta_j \\end{equation}\\] minimalizujemy \\[\\begin{equation} RSS(\\beta, \\lambda)=(\\boldsymbol y -\\boldsymbol N\\beta)&#39;(\\boldsymbol y-\\boldsymbol N\\beta)+\\lambda\\beta&#39;\\boldsymbol \\Omega \\beta, \\end{equation}\\] gdzie \\(\\{\\boldsymbol N\\}_{ij}= N_j(x_i)\\) i \\(\\{\\boldsymbol \\Omega\\}_{jk}=\\int N&#39;&#39;_j(t)N&#39;&#39;_k(t)dt\\). Rozwiązaniem zaganienia minimalizacji \\(RSS(\\beta,\\lambda)\\) jest \\[\\begin{equation} \\hat{\\beta}=((\\boldsymbol N&#39;\\boldsymbol N)+\\lambda\\boldsymbol \\Omega)^{-1}\\boldsymbol N&#39;\\boldsymbol y. \\end{equation}\\] 11.2 Przypadek wielowymiarowy W przypadku gdy \\(X\\in \\mathbb{R}^d\\) poszukujemy takiej \\(d\\)-wymiarowej regresji \\(f(x)\\), która będzie minimalizowała wyrażenie \\[\\begin{equation} \\min_f\\sum_{i=1}^N(y_i-f(x_i))^2+\\lambda J(f), \\end{equation}\\] gdzie \\(J\\) jest odpowiednią funkcją wyrażającą krzywoliniowość modelu. Dla \\(X\\in \\mathbb{R}^2\\) przyjmuje postać \\[\\begin{equation} J(f)=\\iint_{\\mathbb{R}^2}\\left[\\left(\\frac{\\partial^2 f(x)}{\\partial^2 x_1}\\right)^2+2\\left(\\frac{\\partial^2 f(x)}{\\partial x_1\\partial x_2}\\right)^2+ \\left(\\frac{\\partial^2 f(x)}{\\partial^2 x_2}\\right)^2\\right]dx_1dx_2. \\end{equation}\\] Rozwiązanie przyjmuje postać \\[\\begin{equation} f(x) = \\beta_0+\\beta&#39;x+\\sum_{i=1}^N \\alpha_ih_i(x), \\end{equation}\\] gdzie \\(h_i(x)=||x-x_j||^2\\log||x-x_j||\\). 11.3 Uogólnione modele addytywne Przez uogólnione modele addytywne (ang. Generalized Additive Models) rozumiemy klasę modeli, które poprzez funkcję łączącą, opisują warunkową wartość zmiennej wynikowej w następujący sposób \\[\\begin{equation} g(\\E(Y|X))=g(\\mu(X))=\\alpha+f_1(X_1)+\\dots+f_d(X_d), \\end{equation}\\] gdzie \\(g\\) jest funkcją łączącą. Najczęściej stosowanymi funkcjami łączącymi są: \\(g(\\mu)=\\mu\\) - stosowana w modelach, gdy zmienna wynikowa ma rozkład normalny; \\(g(\\mu)=\\logit\\mu\\) - stosowana, gdy zmienna wynikowa ma rozkład dwumianowy; \\(g(\\mu)=\\probit\\mu\\) - stosowana również w przypadku gdy zmienna ma rozkład dwumianowy, a \\(\\Phi^{-1}\\) oznacza odwrotność dystrybuanty standaryzowanego rozkładu normalnego; \\(g(\\mu)=\\log\\mu\\) - stosowana, gdy zmienna wynikowa jest zmienną typu zliczeniowego (rozkład Poissona). 11.3.1 Algorytm uczenia modelu GAM Algorytm uczenia wstecznego (ang. backfitting) przebiega wg następujących kroków: Ustalamy wstępne oszacowania na \\(\\alpha=\\bar{y}\\) i \\(\\hat{f}_j=0\\). Dla \\(j=1,\\ldots,d,1,\\ldots,d,1,\\ldots\\) powtarzamy szacowanie \\[\\begin{align} \\hat{f}_j\\leftarrow &amp;\\mathcal{S}_j\\left[(y_i-\\hat{\\alpha}-\\sum_{k\\neq j}\\hat{f}_k(x_{ik}))^N_1\\right],\\\\ \\hat{f}_j\\leftarrow &amp;\\hat{f}_j-\\frac{1}{N}\\sum_{i=1}^N\\hat{f}_j(x_{ij}) \\end{align}\\] dopóki \\(\\hat{f}_j\\) osiągnie zbieżność. Funkcja \\(\\mathcal{S}_j\\left[(y_i-\\hat{\\alpha}-\\sum_{k\\neq j}\\hat{f}_k(x_{ik}))^N_1\\right]\\) jest jednowymiarowym sześciennym splajnem o \\(N\\) węzłach. W jej miejsce można przyjąć również inne funkcje, takie jak: jednowymiarowe lokalne regresje wielomianowe (ang. LOESS - locally estimated scatterplot smoothing), regresje liniowe, wielomianowe. Przykład 11.1 Dla zilustrowania zasady działania uogólnionych modeli addytywnych przeprowadzimy analizę stężenia ozonu (\\(O_3\\)) w zależności od wybranych parametrów meteorologicznych. Do zbudowania modelu GAM wykorzystamy funkcję gam pakietu mgcv (Wood 2003). library(faraway) head(ozone) ## O3 vh wind humidity temp ibh dpg ibt vis doy ## 1 3 5710 4 28 40 2693 -25 87 250 33 ## 2 5 5700 3 37 45 590 -24 128 100 34 ## 3 5 5760 3 51 54 1450 25 139 60 35 ## 4 6 5720 4 69 35 1568 15 121 60 36 ## 5 4 5790 6 19 45 2631 -33 123 100 37 ## 6 4 5790 3 25 55 554 -28 182 250 38 library(mgcv) mod.gam &lt;- gam(O3~s(temp, bs = &quot;cr&quot;, m = 2)+s(ibh)+s(ibt), data = ozone) summary(mod.gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## O3 ~ s(temp, bs = &quot;cr&quot;, m = 2) + s(ibh) + s(ibt) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.7758 0.2382 49.44 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(temp) 3.357 4.216 20.758 5.99e-16 *** ## s(ibh) 4.171 5.072 7.344 1.37e-06 *** ## s(ibt) 2.111 2.729 1.403 0.213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.708 Deviance explained = 71.7% ## GCV = 19.348 Scale est. = 18.724 n = 330 W powyższym modelu użyto splajnów jako funkcji \\(f_i\\). W przypadku zmiennej temp był to sześcienny splajn z regularyzacją w postaci ciągłości drugiej pochodnej, a pozostałe są prostymi splajnami. Dopasowanie modelu sięga 71.7% a wartość uogólnionego sprawdzianu krzyżowego 19.35. Poniższy wykres pokazuje rodzaje transformacji użyte przy dopasowaniu modelu. par(mfrow = c(1,3)) plot(mod.gam, shade = T, residuals = T) par(mfrow=c(1,1)) Bibliografia "],
["metoda-wektorow-nosnych.html", "12 Metoda wektorów nośnych 12.1 Wprowadzenie 12.2 Definicja modelu dla klas liniowo separowalnych 12.3 Definicja modelu dla klas nieliniowo separowalnych 12.4 Definicja modelu jądrowego 12.5 Zalety i wady", " 12 Metoda wektorów nośnych 12.1 Wprowadzenie Metoda wektorów nośnych21 (ang. Support Vector Machines) to kolejna metoda klasyfikacji obserwacji na podstawie cech (atrybutów). Jest techniką z nauczycielem tzn., że w próbie uczącej występują zarówno cechy charakteryzujące badane obiekty jak i ich przynależność do klasy. Rysunek 12.1: Przykład prostych separujących obiekty obu grup 12.2 Definicja modelu dla klas liniowo separowalnych Istotą tej metody jest znalezienie wektorów nośnych, definiujących hiperpowierzchnie optymalnie separujące obiekty w homogeniczne grupy. Niech \\(D\\) będzie zbiorem \\(n\\) punktów w \\(d\\)-wymiarowej przestrzeni określonych następująco \\((\\vec{x}_i, y_i)\\), \\(i=1,\\ldots, d\\), gdzie \\(y_i\\) przyjmuje wartości -1 lub 1 w zależności od tego do której grupy należy (zakładamy istnienie tylko dwóch grup). Poszukujemy takiej hiperpłaszczyzny, która maksymalizuje margines pomiędzy punktami obu klas w przestrzeni cech \\(\\vec{x}\\). Rysunek 12.2: Płaszczyzna najlepiej rozdzielająca obiekty obu grup (białe i czarne kropki) wraz z prostymi wyznaczającymi maksymalny margines separujący obie grupy Margines ten jest określany jako najmniejsza odległość pomiędzy hiperpłaszczyzną i elementami z każdej z grup. Dowolna hiperpłaszczyzna może być zapisana równaniem \\(\\vec{w}\\vec{x}-b=0\\), gdzie \\(\\vec{w}\\) jest wektorem normalnym do hiperpłaszczyzny. Jeśli dane są liniowo separowalne to, można wybrać takie dwie hiperpłaszczyzny, że odległość pomiędzy nimi jest największa. Równania tych hiperpłaszczyzn dane są wzorami \\[\\begin{equation} \\vec{w}\\vec{x}-b=1, \\quad \\vec{w}\\vec{x}-b=-1 \\tag{12.1} \\end{equation}\\] Odległość pomiędzy tymi hiperpłaszczyznami wynosi \\(\\tfrac{2}{\\|\\vec{w}\\|}\\). Zatem żeby zmaksymalizować odległość pomiędzy hiperpłaszczyznami (margines) musimy zminimalizować \\(\\tfrac{\\|\\vec{w}\\|}{2}\\). Dodatkowo, żeby nie pozwolić aby punkty wpadały do marginesu musimy nałożyć dodatkowe ograniczenia \\[\\begin{align} \\vec{w}\\vec{x}_i-b\\geq&amp; 1, \\quad y_i=1\\\\ \\vec{w}\\vec{x}_i-b\\leq&amp; -1, \\quad y_i=-1 \\tag{12.2} \\end{align}\\] Co można zapisać prościej \\[\\begin{equation} y_i(\\vec{w}\\vec{x}_i-b)\\geq 1,\\quad 1\\leq i\\leq n. \\tag{12.3} \\end{equation}\\] Zatem \\(\\vec{w}\\) i \\(b\\) minimalizujące \\(\\|\\vec{w}\\|\\) przy jednoczesnym spełnieniu warunku definiują klasyfikator postaci \\[\\begin{equation} \\vec{x}\\rightarrow \\operatorname{sgn}(\\vec{w}\\vec{x}-b). \\tag{12.4} \\end{equation}\\] Z racji, że \\(\\|\\vec{w}\\|\\) jest określona jako pierwiastek sumy kwadratów poszczególnych współrzędnych wektora, to częściej w minimalizacji stosuje się \\(\\|\\vec{w}\\|^2\\). Sformułowany powyżej problem należy do grupy optymalizacji funkcji kwadratowej przy liniowych ograniczeniach. Rozwiązuje się go metodą mnożników Lagrange’a. Minimalizujemy funkcję \\[\\begin{equation} L(w, b, \\alpha) = \\frac{1}{2}\\|\\vec{w}\\|^2-\\sum_{i=1}^{n}\\alpha_i\\big(y_i(\\vec{w}\\vec{x}_i-b)-1\\big), \\tag{12.5} \\end{equation}\\] gdzie \\(\\alpha_i\\) są mnożnikami Lagrange’a. Niestety rozwiązanie takiego równania różniczkując po \\(\\vec{w}\\) i \\(b\\) i przyrównując do zera nie jest łatwe. Dlatego Karush-Kuhn Tucker wprowadzili ograniczenia na mnożniki \\(\\alpha_i\\geq 0\\) oraz \\(\\alpha_i\\big(y_i(\\vec{w}\\vec{x}_i-b)-1\\big)=0\\). Co w konsekwencji powoduje, że \\(\\alpha_i\\) są niezerowe jedynie dla wektorów nośnych, a dla pozostałych 0. Dalej jednak poszukiwanie rozwiązania zagadnienia minimalizacji funkcji \\(L\\) ze względu na tak wiele parametrów może być uciążliwe. Wówczas stosuje się maksymalizację dualnej wersji22 \\[\\begin{equation} L_D(\\alpha) = \\sum_{i=1}^{n}\\alpha_i+\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j\\vec{x}_i&#39;\\vec{x}_j \\tag{12.6} \\end{equation}\\] przy ograniczeniach \\(\\alpha_i\\geq 0\\) i \\(\\sum_{i=1}^{n}\\alpha_iy_i=0\\). Rozwiązaniem powyższego zagadnienia jest \\[\\begin{align} \\vec{w}=&amp;\\sum_{i=1}^{n}\\alpha_iy_i\\vec{x}_i,\\tag{12.7}\\\\ b=&amp;y_i-\\vec{w}\\vec{x}_i, \\tag{12.8} \\end{align}\\] a hiperpłaszczyzna decyzyjna \\[\\begin{equation} \\sum_{i=1}^{n}\\alpha_iy_i\\vec{x}_i\\vec{x}-b=0, \\tag{12.9} \\end{equation}\\] gdzie \\(\\vec{x}_i\\) są wektorami nośnymi ze zbioru uczącego, a \\(\\vec{x}\\) jest nowym wektorem dla którego przeprowadzamy klasyfikację. Należy również zauważyć, że im większa wartość \\(\\alpha_i\\), tym większy wpływ wektora na granicę decyzyjną. 12.3 Definicja modelu dla klas nieliniowo separowalnych Niestety rzadko przestrzeń atrybutów jest liniowo separowalna. Stosuje się wówczas modyfikację powyższej metody przez wprowadzenie następującej funkcji straty \\[\\begin{equation} \\zeta_i=\\max\\big(0,1-y_i(\\vec{w}\\vec{x}_i-b)\\big). \\tag{12.10} \\end{equation}\\] Zauważmy, że \\(\\zeta_i\\) jest najmniejszą liczbą nieujemną spełniającą nierówność \\[\\begin{equation} y_i(\\vec{w}\\vec{x}_i-b)\\geq 1-\\zeta_i. \\tag{12.11} \\end{equation}\\] Możemy ją interpretować tak, że jeśli warunek (12.3) jest spełniony, czyli punkty leżą na zewnątrz marginesu (po właściwych stronach), to funkcja straty przyjmuje wartość 0. W przeciwnym przypadku wartość funkcji jest proporcjonalna do odległości od brzegu marginesu. Dlatego wystarczy zminimalizować wartość \\[\\begin{equation} \\frac{1}{n}\\sum_{i=1}^{n}\\zeta_i+\\lambda\\|\\vec{w}\\|^2, \\tag{12.12} \\end{equation}\\] przy warunku (12.11) i \\(\\zeta_i\\geq 0\\) oraz gdzie \\(\\lambda\\) jest wagą kompromisu pomiędzy szerokością marginesu a zapewnieniem, że punkty leżą po właściwych stronach marginesu. Przy dostatecznie małych wartościach \\(\\lambda\\) i separowalności liniowej punktów przestrzeni atrybutów powyższy klasyfikator będzie się zachowywał podobnie jak (12.4). Rozwiązanie problemu minimalizacji funkcji straty określonej w (12.12) za pomocą dualnej wersji mnożników Lagrange’a sprowadza się do minimalizacji funkcji \\[\\begin{equation} L(\\alpha_i) = \\sum_{i=1}^{n}\\alpha_i+\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j\\vec{x}_i&#39;\\vec{x}_j, \\tag{12.13} \\end{equation}\\] przy warunkach \\[\\begin{equation} \\sum_{i=1}^{n}\\alpha_iy_i=0,\\quad 0\\leq \\alpha_i\\leq \\frac{1}{2n\\lambda}. \\tag{12.14} \\end{equation}\\] Wektor normalny do hiperpłaszczyzny jest postaci \\[\\begin{equation} \\vec{w}=\\sum_{i=1}^{n}\\alpha_iy_i\\vec{x}_i, \\tag{12.15} \\end{equation}\\] a parametr \\(b\\) taki jak w (12.8). Powyższy algorytm został przedstawiony przez Vapnika w 1963 roku jako klasyfikator liniowy ale dopiero po wprowadzeniu funkcji jądrowych przekształcających liniowy brzeg decyzyjny na nieliniowy, metoda ta zyskała w oczach statystyków. 12.4 Definicja modelu jądrowego W roku 1992 Boser, Guyon i Vapnik wprowadzili pojęcie nieliniowego klasyfikatora opartego na metodzie wektorów nośnych, który było uogólnieniem techniki przedstawionej przez Vapnika w 1963 roku. Pozwala ona na nieliniowy kształt brzegu obszaru decyzyjnego. Zasada działania polega na znalezieniu takiego jądra przekształcenia (ang. kernel) \\(\\phi\\), które odwzoruje przestrzeń \\(d\\)-wymiarową w \\(d&#39;\\)-wymiarową, gdzie \\(d&#39;&gt;d\\) taką, że \\(D_{\\phi}=\\{\\phi(\\vec{x}_i), y_i\\}\\) jest możliwie jak najbardziej separowalna. Rysunek 12.3: Przykład zastosowania takiego przekształcenia jądrowego aby z sytuacji braku liniowej separowalności do niej doprowadzić Dla funkcji jądrowej określonej wzorem \\(k(\\vec{x}_i,\\vec{x}_j)=\\phi(\\vec{x}_i)\\phi(\\vec{x}_j)\\) przeprowadzamy identyczne rozumowanie jak w przypadku liniowych brzegów obszarów decyzyjnych. Minimalizujemy zatem wyrażenie \\[\\begin{align} L(\\alpha_i) =&amp; \\sum_{i=1}^{n}\\alpha_i+\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j\\phi(\\vec{x}_i)\\phi(\\vec{x}_j)\\\\ =&amp;\\sum_{i=1}^{n}\\alpha_i+\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_jk(\\vec{x}_i,\\vec{x}_j), \\tag{12.16} \\end{align}\\] przy warunkach \\[\\begin{equation} \\sum_{i=1}^{n}\\alpha_iy_i=0,\\quad 0\\leq \\alpha_i\\leq \\frac{1}{2n\\lambda}. \\tag{12.17} \\end{equation}\\] Rozwiązanie powyższego problemu są również podobne do ich liniowych odpowiedników \\[\\begin{equation} \\vec{w}=\\sum_{i=1}^{n}\\alpha_iy_i\\phi(\\vec{x}_i), \\tag{12.18} \\end{equation}\\] a parametr \\(b=\\vec{w}\\phi(\\vec{x}_i)-y_i\\). Najczęściej stosowanymi funkcjami jądrowymi są: wielomianowa \\(k(\\vec{x}_i,\\vec{x}_j)=(a\\vec{x}_i&#39;\\vec{x}_j+b)^q\\), gaussowska \\(k(\\vec{x}_i,\\vec{x}_j)=\\exp(-\\gamma\\|\\vec{x}_i-\\vec{x}_j\\|^2)\\), Laplace’a \\(k(\\vec{x}_i,\\vec{x}_j)=\\exp(-\\gamma\\|\\vec{x}_i-\\vec{x}_j\\|)\\), hiperboliczna \\(k(\\vec{x}_i,\\vec{x}_j)=\\tanh(\\vec{x}_i&#39;\\vec{x}_j+b)\\), sigmoidalna \\(k(\\vec{x}_i,\\vec{x}_j)=\\tanh(a\\vec{x}_i&#39;\\vec{x}_j+b)\\), Bessel’a \\(k(\\vec{x}_i,\\vec{x}_j)=\\frac{Bessel^n_{(\\nu+1)}(\\sigma\\|\\vec{x}_i-\\vec{x}_j\\|)}{(\\|\\vec{x}_i-\\vec{x}_j\\|)^{n(\\nu+1)}}\\), ANOVA \\(k(\\vec{x}_i,\\vec{x}_j)=\\left(\\sum_{k=1}^{n}\\exp\\big(-\\sigma(x^k_i-x^k_j)^2\\big)\\right)^d\\), sklejana dla jednowymiarowej przestrzeni \\(k(x_i,x_j)=1+x_ix_j\\min(x_i,x_j)-\\frac{x_i+x_j}{2}\\big(\\min(x_i,x_j)\\big)^2+\\frac{(\\min(x_i,x_j))^3}{3}\\). W przypadku braku wiedzy o danych funkcja gaussowska, Laplace’a i Bessel’a są zalecane. Przykłady obszarów zastosowań: w kategoryzacji tekstu i hipertekstu; klasyfikacji obrazów - rezultaty eksperymentów pokazują, że SVM daje lepsze rezultaty niż inne techniki; rozpoznawanie obiektów 3D; odnajdowanie włamań do systemu; rozpoznawanie pisma ręcznego; odkrywanie ukrytych treści na zdjęciach; klasyfikacja protein; odnajdowanie sekwencji kodu genetycznego itp… 12.5 Zalety i wady Mocne strony: stopień skomplikowania nie jest zależny od wymiaru przestrzeni atrybutów; optymalny klasyfikator (znajduje minimum globalne); nie jest czuły na przetrenowanie; bardzo duża skuteczność w praktyce. Słabe strony: przy dużej ilości danych estymacja modelu może trwać długo; estymacja poprawnego modelu wymaga pewnej wiedzy; nie ma miejsca na wprowadzenie własnej wiedzy. lub podpierających↩ w przypadku przestrzeni wypukłej oba rozwiązania się pokrywają↩ "],
["bibliografia.html", "Bibliografia", " Bibliografia "]
]
