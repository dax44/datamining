[
["index.html", "Eksploracja danych Wstęp O książce Zakres przedmiotu Zakres technik stosowanych w data mining Etapy eksploracji danych", " Eksploracja danych Dariusz Majerek Katedra Matematyki Stosowanej Wydział Podstaw Techniki Politechnika Lubelskad.majerek@pollub.pl 2019-04-02 Wstęp O książce Niniejsza książka powstała na bazie doświadczeń autora, a głównym jej celem jest przybliżenie czytelnikowi podstaw z dziedziny Data mining studentom kierunku Matematyka Politechniki Lubelskiej. Będzie łączyć w sobie zarówno treści teoretyczne związane z przedstawianymi etapami eksploracji danych i budową modeli, jak i praktyczne wskazówki dotczące budowy modeli w środowisku R (R Core Team 2018). Podane zostaną również wskazówki, jak raportować wyniki analiz i jak dokonać właściwych ilustracji wyników. Bardzo użyteczny w napisaniu książki były pakiety programu R: bookdown (Xie 2018a), knitr (Xie 2018b) oraz pakiet rmarkdown (Allaire et al. 2018). Zakres przedmiotu Przedmiot Eksploracja danych będzie obejmował swoim zakresem eksplorację i wizualizację danych oraz uczenie maszynowe. Eksploracja danych ma na celu pozyskiwanie i systematyzację wiedzy pochodzącej z danych. Odbywa się ona głównie przy użyciu technik statystycznych, rachunku prawdopodobieństwa i metod z zakresu baz danych. Natomiast uczenie maszynowe, to gałąź nauki (obejmuje nie tylko statystykę, choć to na niej się głównie opiera) dotyczącej budowy modeli zdolnych do rozpoznawania wzorców, przewidywania wartości i klasyfikacji obiektów. Data mining to szybko rosnaca grupa metod analizy danych rozwijana nie tylko przez statystyków ale również przez biologów, genetyków, cybernetyków, informatyków, ekonomistów, osoby pracujace nad rozpoznawaniem obrazów i wiele innych grup zawodowych. W dzisiejszych czasch trudno sobie wyobrazić życie bez sztucznej inteligencji. Towarzyszy ona nam w codziennym, życiu kiedy korzystamy z telefonów komórkowych, wyszukiwarek internetowych, robotów sprzątających, automatycznych samochodów, nawigacji czy gier komputerowych. Lista ta jest niepełna i stale się wydłuża. href=“https://twitter.com/i/status/1091069356367200256”&gt;January 31, 2019 Zakres technik stosowanych w data mining statystyka opisowa wielowymiarowa analiza danych analiza szeregów czasowych analiza danych przestrzennych reguły asocjacji uczenie maszynowe1, w tym: klasyfikacja predykcja analiza skupień text mining i wiele innych Rysunek .: Przykład nienadzorowanego uczenia maszynowego. Źródło:https://analyticstraining.com/cluster-analysis-for-business/ href=“https://twitter.com/i/status/1097199751072690176”&gt;Ferbruary 17, 2019 Etapy eksploracji danych Rysunek .: Etapy eksploracji danych (Kavakiotis et al. 2017) Czyszczenie danych - polega na usuwaniu braków danych, usuwaniu stałych zmiennych, imputacji braków danych oraz przygotowaniu danych do dalszych analiz. Integracja danych - łączenie danych pochodzących z różnych źródeł. Selekcja danych - wybór z bazy tych danych, które są potrzebne do dalszych analiz. Transformacja danych - przekształcenie i konsolidacja danych do postaci przydatnej do eksploracji. Eksploracja danych - zastosowanie technik wymienionych wcześniej w celu odnalezienia wzorców2 i zależności. Ewaluacja modeli - ocena poprawności modeli oraz wzorców z nich uzyskanych. Wizualizacja wyników - graficzne przedstawienie odkrytych wzorców. Wdrażanie modeli - zastosowanie wyznaczonych wzorców. Bibliografia "],
["roz1.html", "1 Import danych", " 1 Import danych Środowisko R pozwala na import i export plików o różnych rozszerzeniach (txt, csv, xls, xlsx, sav, xpt, dta, itd.)3. W tym celu czasami trzeba zainstalować pakiety rozszerzające podstawowe możliwości R-a. Najnowsza4 wersja programu RStudio (v. 1.1.463)5 pozwala na wczytanie danych z popularnych źródeł za pomocą GUI. Rysunek 1.1: Narzędzie do importu plików programu RStudio Jeśli dane są zapisane w trybie tekstowym (np. txt, csv), to wczytujemy je w następujący sposób dane1 &lt;- read.table(&quot;data/dane1.txt&quot;, header = T) head(dane1) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane2 &lt;- read.csv2(&quot;data/dane1.csv&quot;, header = T) head(dane2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # funkcja pakietu readr wczytuje plik jako ramkę danych w formacie tibble # pakiet readr jest częsią większego pakietu tidyverse, # który został wczytany wczsniej dane3 &lt;- read_csv2(&quot;data/dane1.csv&quot;) dane3 ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows Jeśli dane są przechowywane w pliku Excel (np. xlsx), to importujemy je za pomocą funkcji read_excel pakietu readxl. Domyślnie jest wczytywany arkusz pierwszy ale jeśli zachodzi taka potrzeba, to można ustalić, który arkusz pliku Excel ma być wczytany za pomocą paramteru sheet, np. sheet=3, co oznacza, że zostanie wczytany trzeci arkusz pliku. Rysunek 1.2: Fragment pliku Excel Ponieważ w pliku dane1.xlsx braki danych zostały zakodowane znakami BD oraz -, to należy ten fakt przekazać funkcji, aby poprawnie wczytać braki danych. W przeciwnym przypadku zmienne zawierające braki tak kodowane, będą wczytane jako zmienne znakowe. library(readxl) dane4 &lt;- read_excel(&quot;data/dane1.xlsx&quot;, na = c(&quot;BD&quot;, &quot;-&quot;)) dane4 ## # A tibble: 150 x 5 ## `Długość kielic~ `Szerokość kiel~ `Długość płatka` `Szerokość płat~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; Istniej oczywiście jeszcze wiele innych fomatów danych, charakterystycznych dla programów, w których są traktowane jako domyślne.6 W szczególny sposób należy zwrócić uwagę na pliki o rozszerzeniu RData lub rda7 oraz pliki rds. Pliki rda służą do przechowywania obiektów programu R. Mogą to być pliki danych ale również obiekty graficzne (typu wyniki funkcji ggplot), modele (np. wynik funkcji lm()), zdefiniowane funkcje i wszystkie inne obiekty, które da się zapisać w środowisku R. Ponadto pliki rda pozawalają na zapisanie wielu obiektów w jednym pliku. Pliki o rozszerzeniu rds mają podobną funkcję z tym, że pozwalają na przechowywanie tylko jednego obiektu. # wszystkie wczytane wcześniej pliki zapisuje w jednym pliku save(dane1, dane2, dane3, dane4, file = &quot;data/dane.rda&quot;) # plik rda został zapisany list.files(path = &quot;data/&quot;) ## [1] &quot;algae.csv&quot; &quot;Analysis.txt&quot; &quot;dane.rda&quot; &quot;dane1.csv&quot; ## [5] &quot;dane1.txt&quot; &quot;dane1.xlsx&quot; &quot;dane4.rds&quot; &quot;dane4.sav&quot; # usuwam dane ze środowiska R rm(dane1, dane2, dane3, dane4) # sprawdzam co jest wczytane do R ls() ## character(0) # wczytuję plik rda load(&quot;data/dane.rda&quot;) # jeszcze raz sprawdzam co jest wczytane do R ls() ## [1] &quot;dane1&quot; &quot;dane2&quot; &quot;dane3&quot; &quot;dane4&quot; Zapisując obiekty jako oddzielne pliki, można przy wczytywaniu nadawać im nazwy. rm(dane1, dane2, dane3) ls() ## [1] &quot;dane4&quot; saveRDS(dane4, file = &quot;data/dane4.rds&quot;) nowe_dane &lt;- readRDS(&quot;data/dane4.rds&quot;) nowe_dane ## # A tibble: 150 x 5 ## `Długość kielic~ `Szerokość kiel~ `Długość płatka` `Szerokość płat~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; Oprócz wielu zalet takiego sposobu importu i eksportu danych jest jedna poważna wada, pliki te można odczytać jedynie za pomocą R. Osobiście polecam stosować do importu i eksportu danych plików w takich formatach, które mogą przeczytać wszyscy. Jak dotąd widać do importu różnych formatów danych potrzebujemy różnych funkcji, czasami nawet z różnych pakietów. Istnieje rozwiązanie tej niedogodności 😀 library(rio) dane1 &lt;- import(&quot;data/dane1.txt&quot;) head(dane1) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane2 &lt;- import(&quot;data/dane1.csv&quot;, dec = &quot;,&quot;) # dane1.csv miały , jako znak rozdzielający cechę i mantysę liczb # dlatego włączamy parametr dec head(dane2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane3 &lt;- import(&quot;data/dane1.xlsx&quot;, na=c(&quot;BD&quot;,&quot;-&quot;)) head(dane3) ## Długość kielicha Szerokość kielicha Długość płatka Szerokość płatka ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## Gatunki ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa dane4 &lt;- import(&quot;data/dane4.rds&quot;) dane4 ## # A tibble: 150 x 5 ## `Długość kielic~ `Szerokość kiel~ `Długość płatka` `Szerokość płat~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; Lista możliwości jaką daje nam pakiet rio (Chan and Leeper 2018) jest niemal nieograniczona:8 Comma-separated data (.csv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE Pipe-separated data (.psv), using fread or, if fread = FALSE, read.table with sep = ‘|’, row.names = FALSE and stringsAsFactors = FALSE Tab-separated data (.tsv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE SAS (.sas7bdat), using read_sas. SAS XPORT (.xpt), using read_xpt or, if haven = FALSE, read.xport. SPSS (.sav), using read_sav. If haven = FALSE, read.spss can be used. Stata (.dta), using read_dta. If haven = FALSE, read.dta can be used. SAS XPORT (.xpt), using read.xport. SPSS Portable Files (.por), using read_por. Excel (.xls and .xlsx), using read_excel. Use which to specify a sheet number. For .xlsx files, it is possible to set readxl = FALSE, so that read.xlsx can be used instead of readxl (the default). R syntax object (.R), using dget Saved R objects (.RData,.rda), using load for single-object .Rdata files. Use which to specify an object name for multi-object .Rdata files. This can be any R object (not just a data frame). Serialized R objects (.rds), using readRDS. This can be any R object (not just a data frame). Epiinfo (.rec), using read.epiinfo Minitab (.mtp), using read.mtp Systat (.syd), using read.systat “XBASE” database files (.dbf), using read.dbf Weka Attribute-Relation File Format (.arff), using read.arff Data Interchange Format (.dif), using read.DIF Fortran data (no recognized extension), using read.fortran Fixed-width format data (.fwf), using a faster version of read.fwf that requires a widths argument and by default in rio has stringsAsFactors = FALSE. If readr = TRUE, import will be performed using read_fwf, where widths should be: NULL, a vector of column widths, or the output of fwf_empty, fwf_widths, or fwf_positions. gzip comma-separated data (.csv.gz), using read.table with row.names = FALSE and stringsAsFactors = FALSE CSVY (CSV with a YAML metadata header) using read_csvy. Feather R/Python interchange format (.feather), using read_feather Fast storage (.fst), using read.fst JSON (.json), using fromJSON Matlab (.mat), using read.mat EViews (.wf1), using readEViews OpenDocument Spreadsheet (.ods), using read_ods. Use which to specify a sheet number. Single-table HTML documents (.html), using read_html. The data structure will only be read correctly if the HTML file can be converted to a list via as_list. Shallow XML documents (.xml), using read_xml. The data structure will only be read correctly if the XML file can be converted to a list via as_list. YAML (.yml), using yaml.load Clipboard import (on Windows and Mac OS), using read.table with row.names = FALSE Google Sheets, as Comma-separated data (.csv) Przykład 1.1 Poniższa ilustracja przedstawia fragment pliku danych Analysis.txt zawierającego pewne błędy, które należy naprawić na etapie importu danych. Po pierwsze brakuje w nim nazw zmiennych (choć nie widać tego na rysunku). Poszczególne kolumny nazywają się następująco: season, size, speed, mxPH, mnO2, Cl, NO3, NH4, oPO4, PO4, Chla, a1, a2, a3, a4, a5, a6, a7. Naszym zadaniem jest import tego pliku z jednoczesną obsługą braków (braki danych są zakodowane przez XXXXXXX) oraz nadaniem nagłówków kolumn. Plik Analisis.txt jest umieszczony w kagalogu data/. Z racji, że plik dotyczy glonów, to dane zapiszemy pod nazwą algae. Rysunek 1.3: Fragment pliku danych Analisis.txt algae &lt;- import(&#39;data/Analysis.txt&#39;, header=F, dec=&#39;.&#39;, col.names=c(&#39;season&#39;,&#39;size&#39;,&#39;speed&#39;,&#39;mxPH&#39;,&#39;mnO2&#39;,&#39;Cl&#39;, &#39;NO3&#39;,&#39;NH4&#39;,&#39;oPO4&#39;,&#39;PO4&#39;,&#39;Chla&#39;,&#39;a1&#39;,&#39;a2&#39;, &#39;a3&#39;,&#39;a4&#39;,&#39;a5&#39;,&#39;a6&#39;,&#39;a7&#39;), na.strings=c(&#39;XXXXXXX&#39;)) head(algae) ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla ## 1 winter small medium 8.00 9.8 60.800 6.238 578.000 105.000 170.000 50.0 ## 2 spring small medium 8.35 8.0 57.750 1.288 370.000 428.750 558.750 1.3 ## 3 autumn small medium 8.10 11.4 40.020 5.330 346.667 125.667 187.057 15.6 ## 4 spring small medium 8.07 4.8 77.364 2.302 98.182 61.182 138.700 1.4 ## 5 autumn small medium 8.06 9.0 55.350 10.416 233.700 58.222 97.580 10.5 ## 6 winter small high 8.25 13.1 65.750 9.248 430.000 18.250 56.667 28.4 ## a1 a2 a3 a4 a5 a6 a7 ## 1 0.0 0.0 0.0 0.0 34.2 8.3 0.0 ## 2 1.4 7.6 4.8 1.9 6.7 0.0 2.1 ## 3 3.3 53.6 1.9 0.0 0.0 0.0 9.7 ## 4 3.1 41.0 18.9 0.0 1.4 0.0 1.4 ## 5 9.2 2.9 7.5 0.0 7.5 4.1 1.0 ## 6 15.1 14.6 1.4 0.0 22.5 12.6 2.9 summary(algae) ## season size speed mxPH ## Length:200 Length:200 Length:200 Min. :5.600 ## Class :character Class :character Class :character 1st Qu.:7.700 ## Mode :character Mode :character Mode :character Median :8.060 ## Mean :8.012 ## 3rd Qu.:8.400 ## Max. :9.700 ## NA&#39;s :1 ## mnO2 Cl NO3 NH4 ## Min. : 1.500 Min. : 0.222 Min. : 0.050 Min. : 5.00 ## 1st Qu.: 7.725 1st Qu.: 10.981 1st Qu.: 1.296 1st Qu.: 38.33 ## Median : 9.800 Median : 32.730 Median : 2.675 Median : 103.17 ## Mean : 9.118 Mean : 43.636 Mean : 3.282 Mean : 501.30 ## 3rd Qu.:10.800 3rd Qu.: 57.824 3rd Qu.: 4.446 3rd Qu.: 226.95 ## Max. :13.400 Max. :391.500 Max. :45.650 Max. :24064.00 ## NA&#39;s :2 NA&#39;s :10 NA&#39;s :2 NA&#39;s :2 ## oPO4 PO4 Chla a1 ## Min. : 1.00 Min. : 1.00 Min. : 0.200 Min. : 0.00 ## 1st Qu.: 15.70 1st Qu.: 41.38 1st Qu.: 2.000 1st Qu.: 1.50 ## Median : 40.15 Median :103.29 Median : 5.475 Median : 6.95 ## Mean : 73.59 Mean :137.88 Mean : 13.971 Mean :16.92 ## 3rd Qu.: 99.33 3rd Qu.:213.75 3rd Qu.: 18.308 3rd Qu.:24.80 ## Max. :564.60 Max. :771.60 Max. :110.456 Max. :89.80 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :12 ## a2 a3 a4 a5 ## Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 3.000 Median : 1.550 Median : 0.000 Median : 1.900 ## Mean : 7.458 Mean : 4.309 Mean : 1.992 Mean : 5.064 ## 3rd Qu.:11.375 3rd Qu.: 4.925 3rd Qu.: 2.400 3rd Qu.: 7.500 ## Max. :72.600 Max. :42.800 Max. :44.600 Max. :44.400 ## ## a6 a7 ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 0.000 Median : 1.000 ## Mean : 5.964 Mean : 2.495 ## 3rd Qu.: 6.925 3rd Qu.: 2.400 ## Max. :77.600 Max. :31.600 ## export(algae, file = &quot;data/algae.csv&quot;) Bibliografia "],
["przygotowanie-danych.html", "2 Przygotowanie danych 2.1 Identyfikacja braków danych 2.2 Zastępowanie braków danych", " 2 Przygotowanie danych Dane, które importujemy z zewnętrznego źródła najczęściej nie spełniają formatów obowiązujących w R. Często zmienne zawierają niedopuszczalne znaki szczególne, odstępy w nazwach, powtórzone nazwy kolumn, nazwy zmiennych zaczynające się od liczby, czy puste wiersze lub kolumny. Przed przystąpieniem do analizy zbioru należy rozważyć ewentualne poprawki nazw zmiennych, czy usunięcie pustych kolumn i wierszy. Niektórych czynności można dokonać już na etapie importu danych, stosując pewne pakiety oraz nowe funkcjonalności środowiska RStudio. W większości przypadków uchroni nas to od żmudnego przekształcania typów zmiennych. Oczywiście wszystkie te czynności czyszczenia danych można również dokonać już po imporcie danych, za pomocą odpowiednich komend R. ## przykładowe niepożądane nazwy zmiennych test_df &lt;- as.data.frame(matrix(rnorm(18),ncol = 6)) names(test_df) &lt;- c(&quot;hIgHlo&quot;, &quot;REPEAT VALUE&quot;, &quot;REPEAT VALUE&quot;, &quot;% successful (2009)&quot;, &quot;abc@!*&quot;, &quot;&quot;) test_df ## hIgHlo REPEAT VALUE REPEAT VALUE % successful (2009) abc@!* ## 1 1.6790755 0.6661603 -0.8146708 0.4423675 0.9755845 ## 2 1.3229556 -0.9828055 -0.2940081 -0.6778910 -0.5879578 ## 3 0.4403683 1.1473081 -0.5402885 0.5296637 -0.4781025 ## ## 1 -0.6536868 ## 2 1.0560019 ## 3 0.7178906 ## do poprawy nazw zmiennych użyjemy funkcji make.names names(test_df) &lt;- make.names(names(test_df)) test_df ## hIgHlo REPEAT.VALUE REPEAT.VALUE X..successful..2009. abc... ## 1 1.6790755 0.6661603 -0.8146708 0.4423675 0.9755845 ## 2 1.3229556 -0.9828055 -0.2940081 -0.6778910 -0.5879578 ## 3 0.4403683 1.1473081 -0.5402885 0.5296637 -0.4781025 ## X ## 1 -0.6536868 ## 2 1.0560019 ## 3 0.7178906 Efekt końcowy choć skuteczny, to nie jest zadowalający. Czyszczenia nazw zmiennych można też dokonać stosując funkcję clean_names pakietu janitor (Firke 2018). Pozwala on również na usuwanie pustych wierszy i kolumn, znajdowanie zduplikowanych rekordów, itp. library(janitor) test_df %&gt;% # aby na stałe zmienić nazwy zmiennych trzeba podstawienia clean_names() ## h_ig_hlo repeat_value repeat_value_2 x_successful_2009 abc ## 1 1.6790755 0.6661603 -0.8146708 0.4423675 0.9755845 ## 2 1.3229556 -0.9828055 -0.2940081 -0.6778910 -0.5879578 ## 3 0.4403683 1.1473081 -0.5402885 0.5296637 -0.4781025 ## x ## 1 -0.6536868 ## 2 1.0560019 ## 3 0.7178906 # przykładowe dane x &lt;- data.frame(w1=c(1,4,2,NA),w2=c(NA,2,3,NA), w3=c(1,NA,1,NA)) x ## w1 w2 w3 ## 1 1 NA 1 ## 2 4 2 NA ## 3 2 3 1 ## 4 NA NA NA x %&gt;% remove_empty(&quot;rows&quot;) ## w1 w2 w3 ## 1 1 NA 1 ## 2 4 2 NA ## 3 2 3 1 2.1 Identyfikacja braków danych Zanim usuniemy jakiekolwiek braki w zbiorze, powinniśmy je najpierw zidentyfikować, określić ich charakter, a dopiero potem ewentualnie podjąć decyzję o uzupełnianiu braków. algae &lt;- rio::import(&quot;data/algae.csv&quot;) # najprościej jest wywołać summary summary(algae) ## season size speed mxPH ## Length:200 Length:200 Length:200 Min. :5.600 ## Class :character Class :character Class :character 1st Qu.:7.700 ## Mode :character Mode :character Mode :character Median :8.060 ## Mean :8.012 ## 3rd Qu.:8.400 ## Max. :9.700 ## NA&#39;s :1 ## mnO2 Cl NO3 NH4 ## Min. : 1.500 Min. : 0.222 Min. : 0.050 Min. : 5.00 ## 1st Qu.: 7.725 1st Qu.: 10.981 1st Qu.: 1.296 1st Qu.: 38.33 ## Median : 9.800 Median : 32.730 Median : 2.675 Median : 103.17 ## Mean : 9.118 Mean : 43.636 Mean : 3.282 Mean : 501.30 ## 3rd Qu.:10.800 3rd Qu.: 57.824 3rd Qu.: 4.446 3rd Qu.: 226.95 ## Max. :13.400 Max. :391.500 Max. :45.650 Max. :24064.00 ## NA&#39;s :2 NA&#39;s :10 NA&#39;s :2 NA&#39;s :2 ## oPO4 PO4 Chla a1 ## Min. : 1.00 Min. : 1.00 Min. : 0.200 Min. : 0.00 ## 1st Qu.: 15.70 1st Qu.: 41.38 1st Qu.: 2.000 1st Qu.: 1.50 ## Median : 40.15 Median :103.29 Median : 5.475 Median : 6.95 ## Mean : 73.59 Mean :137.88 Mean : 13.971 Mean :16.92 ## 3rd Qu.: 99.33 3rd Qu.:213.75 3rd Qu.: 18.308 3rd Qu.:24.80 ## Max. :564.60 Max. :771.60 Max. :110.456 Max. :89.80 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :12 ## a2 a3 a4 a5 ## Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 3.000 Median : 1.550 Median : 0.000 Median : 1.900 ## Mean : 7.458 Mean : 4.309 Mean : 1.992 Mean : 5.064 ## 3rd Qu.:11.375 3rd Qu.: 4.925 3rd Qu.: 2.400 3rd Qu.: 7.500 ## Max. :72.600 Max. :42.800 Max. :44.600 Max. :44.400 ## ## a6 a7 ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 0.000 Median : 1.000 ## Mean : 5.964 Mean : 2.495 ## 3rd Qu.: 6.925 3rd Qu.: 2.400 ## Max. :77.600 Max. :31.600 ## ## wyświetl niekompletne wiersze algae[!complete.cases(algae),] %&gt;% head() ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 28 autumn small high 6.8 11.1 9.00 0.630 20 4.0 NA 2.7 30.3 1.9 0.0 ## 38 spring small high 8.0 NA 1.45 0.810 10 2.5 3.0 0.3 75.8 0.0 0.0 ## 48 winter small low NA 12.6 9.00 0.230 10 5.0 6.0 1.1 35.5 0.0 0.0 ## 55 winter small high 6.6 10.8 NA 3.245 10 1.0 6.5 NA 24.3 0.0 0.0 ## 56 spring small medium 5.6 11.8 NA 2.220 5 1.0 1.0 NA 82.7 0.0 0.0 ## 57 autumn small medium 5.7 10.8 NA 2.550 10 1.0 4.0 NA 16.8 4.6 3.9 ## a4 a5 a6 a7 ## 28 0.0 2.1 1.4 2.1 ## 38 0.0 0.0 0.0 0.0 ## 48 0.0 0.0 0.0 0.0 ## 55 0.0 0.0 0.0 0.0 ## 56 0.0 0.0 0.0 0.0 ## 57 11.5 0.0 0.0 0.0 ## policz niekompletne wiersze nrow(algae[!complete.cases(algae),]) ## [1] 16 ## sprawdzenie liczby braków w wierszach apply(algae, 1, function(x) sum(is.na(x))) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ## 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## 2 2 2 2 2 2 2 6 1 0 0 0 0 0 0 0 0 0 ## 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 ## 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ## 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ## 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 199 200 ## 6 0 Wiele ciekawych funkcji do eksploracji danych znajduje się w pakiecie DMwR (Torgo 2013), który został przygotowany przy okazji publikacji książki Data Mining with R. ## poszukiwanie wierszy zawierających wiele braków ## w tym przypadku próg wyświetlania ustawiony jest na 0.2 ## czyli 20% wszystkich kolumn library(DMwR) manyNAs(algae) ## 62 199 ## 62 199 ## tworzenie zbioru pozbawionego wierszy zawierających wiele braków algae2 &lt;- algae[-manyNAs(algae), ] ## sprawdzamy liczbę wybrakowanych wierszy które pozostały nrow(algae2[!complete.cases(algae2),]) ## [1] 14 ## usuwamy wszystkie wiersze z brakami algae3 &lt;- na.omit(algae) ## wyświetl wiersze z brakami algae3[!complete.cases(algae3),] %&gt;% head() ## [1] season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 ## [11] Chla a1 a2 a3 a4 a5 a6 a7 ## &lt;0 rows&gt; (or 0-length row.names) ## liczba pozostałych wybrakowanych wierszy nrow(algae3[!complete.cases(algae3),]) ## [1] 0 ## można oczywiście też ręcznie usuwać wiersze (nie polecam) algae4 &lt;- algae[-c(62,199),] Można też zbudować funkcję, która będzie usuwała braki danych wg naszego upodobania. ## najpierw budujemy funkcję i ją kompilujemy aby R mógł ja stosować ## parametr prog ustala próg odcięcia wierszy czysc.dane &lt;- function(dt, prog = 0){ licz.braki &lt;- apply(dt, 1, function(x) sum(is.na(x))) czyste.dt &lt;- dt[!(licz.braki/ncol(dt)&gt;prog), ] return(czyste.dt) } ## potem ją możemy stosować algae4 &lt;- czysc.dane(algae) nrow(algae4[!complete.cases(algae4),]) ## [1] 0 ## czyścimy wiersze, których liczba braków przekracza 20% wszystkich kolumn algae5 &lt;- czysc.dane(algae, prog = 0.2) nrow(algae5[!complete.cases(algae5),]) ## [1] 14 Bardzo ciekawym narzędziem do znajdowania braków danych jest funkcja md.pattern pakietu mice (van Buuren and Groothuis-Oudshoorn 2018). Wskazuje on ile braków występuje w ramach każdej zmiennej. library(mice) md.pattern(algae) Rysunek 2.1: Na czerwono zaznaczone są zmienne, które zwierają braki danych. Liczba w wierszu po lewej stronie wykresu wskazuje ile wierszy w bazie ma daną charakterystykę, a liczba po prawej oznacza ile zmiennych było wybrakowanych ## season size speed a1 a2 a3 a4 a5 a6 a7 mxPH mnO2 NO3 NH4 oPO4 PO4 Cl ## 184 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 ## 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 ## 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 ## 0 0 0 0 0 0 0 0 0 0 1 2 2 2 2 2 10 ## Chla ## 184 1 0 ## 3 0 1 ## 1 1 1 ## 7 0 2 ## 1 1 1 ## 1 0 6 ## 1 1 1 ## 1 0 6 ## 1 1 1 ## 12 33 2.2 Zastępowanie braków danych Zastępowanie braków danych (zwane także imputacją danych) jest kolejnym etapem procesu przygotowania danych do analiz. Nie można jednak wyróżnić uniwersalnego sposobu zastępowania braków dla wszystkich możliwych sytuacji. Wśród statystyków panuje przekonanie, że w przypadku wystąpienia braków danych można zastosować trzy strategie: nic nie robić z brakami - co wydaje się niedorzeczne ale wcale takie nie jest, ponieważ istnieje wiele modeli statystycznych (np. drzewa decyzyjne), które świetnie radzą sobie w sytuacji braków danych. Niestety nie jest to sposób, który można stosować zawsze, ponieważ są również modele wymagające kompletności danych jak na przykład sieci neuronowe. usuwać braki wierszami9 - to metoda, która jest stosowana domyślnie w przypadku kiedy twórca modelu nie zadecyduje o innym sposobie obsługi luk. Metoda ta ma swoją niewątpliwą zaletę w postaci jasnej i prostej procedury, ale szczególnie w przypadku niewielkich zbiorów może skutkować obciążeniem estymatorów. Nie wiemy bowiem jaka wartość faktycznie jest przypisana danej cesze. Jeśli jest to wartość bliska np. średniej, to nie wpłynie znacząco na obciążenie estymatora wartości oczekiwanej. W przypadku, gdy różni się ona znacznie od średniej tej cechy, to estymator może już wykazywać obciążenie. Jego wielkość zależy również od liczby usuniętych elementów. Nie jest zalecane usuwanie wielu wierszy ze zbioru danych i na podstawie okrojonego zbioru wyciąganie wniosków o populacji, ponieważ próba jest wówczas znacząco inna niż populacja. Dodatkowo jeśli estymatory są wyznaczane na podstawie zbioru wyraźnie mniej licznego, to precyzja estymatorów wyrażona wariancją spada. Reasumując, jeśli liczba wierszy z brakującymi danymi jest niewielka w stosunku do całego zbioru, to usuwanie wierszy jest sensownym rozwiązaniem. uzupełnianie braków - to procedura polegająca na zastępowaniu braków różnymi technikami. Jej niewątpliwą zaletą jest fakt posiadania kompletnych danych bez konieczności usuwania wierszy. Niestety wiąże się to również z pewnymi wadami. Zbiór posiadający wiele braków uzupełnianych nawet bardzo wyrafinowanymi metodami może cechować się zaniżoną wariancją poszczególnych cech oraz tzw. przeuczeniem10. Uzupełnianie średnią - braki w zakresie danej zmiennej uzupełniamy średnią tej zmiennej przypadków uzupełnionych. algae[is.na(algae$mxPH), ] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 a4 a5 ## 48 winter small low NA 12.6 9 0.23 10 5 6 1.1 35.5 0 0 0 0 ## a6 a7 ## 48 0 0 m &lt;- mean(algae$mxPH, na.rm = T) algae[is.na(algae$mxPH), &quot;mxPH&quot;] &lt;- m algae[is.na(algae$mxPH), ] ## [1] season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 ## [11] Chla a1 a2 a3 a4 a5 a6 a7 ## &lt;0 rows&gt; (or 0-length row.names) Uzupełnianie medianą - braki w zakresie danej zmiennej uzupełniamy medianą tej zmiennej przypadków uzupełnionych. algae %&gt;% filter(is.na(Chla)) %&gt;% head ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 1 winter small high 6.6 10.8 NA 3.245 10 1 6.5 NA 24.3 0.0 0.0 ## 2 spring small medium 5.6 11.8 NA 2.220 5 1 1.0 NA 82.7 0.0 0.0 ## 3 autumn small medium 5.7 10.8 NA 2.550 10 1 4.0 NA 16.8 4.6 3.9 ## 4 spring small high 6.6 9.5 NA 1.320 20 1 6.0 NA 46.8 0.0 0.0 ## 5 summer small high 6.6 10.8 NA 2.640 10 2 11.0 NA 46.9 0.0 0.0 ## 6 autumn small medium 6.6 11.3 NA 4.170 10 1 6.0 NA 47.1 0.0 0.0 ## a4 a5 a6 a7 ## 1 0.0 0 0.0 0 ## 2 0.0 0 0.0 0 ## 3 11.5 0 0.0 0 ## 4 28.8 0 0.0 0 ## 5 13.4 0 0.0 0 ## 6 0.0 0 1.2 0 algae[is.na(algae$Chla), &quot;Chla&quot;] &lt;- median(algae$Chla, na.rm = T) Wypełnianie zmiennych typu wyliczeniowego, logicznego lub znakowego odbywa się najczęściej przez dobranie w miejsce brakującej wartości, elementu powtarzającego się najczęściej wśród obiektów obserwowanych. W pakiecie DMwR istnieje funkcja centralImputation, która wypełnia braki wartością centralną (w przypadku zmiennych typu liczbowego - medianą, a dla wartości logicznych, wyliczeniowych lub tekstowych - modą). algae[48, &quot;season&quot;] ## [1] &quot;winter&quot; algae[48, &quot;season&quot;] &lt;- NA algae.uzup &lt;- centralImputation(algae) algae.uzup[48,] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 winter small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 Jeszcze innym sposobem imputacji danych są algorytmy oparte o metodę \\(k\\)-najbliższych sąsiadów. Algorytm opiera się na prostej zasadzie, uzupełniania brakujących wartości medianą (w przypadku zmiennych ilościowych) lub modą (w przypadku zmiennych jakościowych) elementów, które są \\(k\\)-tymi najbliższymi sąsiadami w metryce \\[\\begin{equation}\\label{knn} d(x,y)=\\sqrt{\\sum_{i=1}^{p}\\delta_i(x_i,y_i)}, \\end{equation}\\] gdzie \\(\\delta_i\\) jest odległością pomiędzy dwoma elementami ze względu na \\(i\\)-tą cech, określoną następująco \\[\\begin{equation}\\label{metryka} \\delta_i(v_1, v_2)=\\begin{cases} 1,&amp; \\text{jeśli zmienna jest jakościowa i }v_1\\neq v_2\\\\ 0,&amp; \\text{jeśli zmienna jest jakościowa i }v_1=v_2\\\\ (v_1-v_2)^2,&amp; \\text{jeśli zmienna jest ilościowa.} \\end{cases} \\end{equation}\\] Odległości są mierzone dla zmiennych standaryzowanych. Istnieje też odmiana z wagami, które maleją wraz ze wzrostem odległości pomiędzy sąsiadem a uzupełnianym elementem (np. \\(w(d)=\\exp(d)\\)). algae[48, ] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 &lt;NA&gt; small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 algae &lt;- algae %&gt;% mutate_if(is.character, as.factor) algae.uzup &lt;- knnImputation(algae, k = 5, scale = F, meth = &quot;median&quot;) algae.uzup[48,] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 summer small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 Istnieją również dużo bardziej złożone algorytmy imputacji danych oparte na bardziej wyrafinowanych technikach, takich jak: predykcja modelami liniowymi, nieliniowymi, analiza dyskryminacyjna, drzewa klasyfikacyjne. Dwa najbardziej znane pakiety zawierające funkcje do imputacji w sposób złożony, to Amelia i mice. Imputacja danych z zastosowaniem pakietu mice wymaga podjęcia kilku decyzji przed przystąpieniem do uzupełniania danych: Czy dane są MAR (ang. Missing At Random) czy MNAR (ang. Missing Not At Random), co oznacza, że musimy się zastanowić jakie mogły być źródła braków danych, przypadkowe czy systematyczne? Należy się zdecydować na formę imputacji, określając strukturę zależności pomiędzy cechami oraz rozkład błędu danej cechy? Wybrać zbiór danych, który posłuży nam za predyktory w imputacji (nie mogą zawierać braków). Określenie, które niepełne zmienne są funkcjami innych wybrakowanych zmiennych. Określić w jakiej kolejności dane będą imputowane. Określić parametry startowe imputacji (liczbę iteracji, warunek zbieżności). Określić liczę imputowanych zbiorów. Ad 1. Wyróżniamy następujące rodzaje braków danych: MCAR (ang. Missing Completely At Random) - z definicji to braki, których pojawienie się jest kompletnie losowe. Przykładowo gdy osoba poproszona o wypełnienie wieku w ankiecie będzie rzucać monetą czy wypełnić tą zmienną. MAR - oznacza, że obserwowane wartości i wybrakowane mają inne rozkłady ale da się je oszacować na podstawie danych obserwowanych. Przykładowo ciśnienie tętnicze u osób, które nie wypełniły tej wartości jest wyższe niż u osób, które wpisały swoje ciśnienie. Okazuje się, że osoby starsze z nadciśnieniem nie wypełniały ankiety w tym punkcie. MNAR - jeśli nie jest spełniony warunek MCAR i MAR, wówczas brak ma charakter nielosowy. Przykładowo respondenci osiągający wyższe zarobki sukcesywnie nie wypełniają pola “zarobki” i dodatkowo nie ma w ankiecie zmiennych, które pozwoliłyby nam ustalić, jakie to osoby. Ad 2. Decyzja o algorytmie imputacji wynika bezpośrednio ze skali w jakiej jest mierzona dana zmienna. Ze względu na rodzaj cechy używać będziemy następujących metod: Tabela 2.1: Zestaw metod imputacji danych stosowanych w pakiecie mice method type description pmm any Predictive.mean.matching midastouch any Weighted predictive mean matching sample any Random sample from observed values cart any Classification and regression trees rf any Random forest imputations mean numeric Unconditional mean imputation norm numeric Bayesian linear regression norm.nob numeric Linear regression ignoring model error norm.boot numeric Linear regression using bootstrap norm.predict numeric Linear regression, predicted values quadratic numeric Imputation of quadratic terms ri numeric Random indicator for nonignorable data logreg binary Logistic regression logreg.boot binary Logistic regression with bootstrap polr ordered Proportional odds model polyreg unordered Polytomous logistic regression lda unordered Linear discriminant analysis 2l.norm numeric Level-1 normal heteroscedastic 2l.lmer numeric Level-1 normal homoscedastic, lmer 2l.pan numeric Level-1 normal homoscedastic, pan 2l.bin binary Level-1 logistic, glmer 2lonly.mean numeric Level-2 class mean 2lonly.norm numeric Level-2 class normal 2lonly.pmm any Level-2 class predictive mean matching Każdy z czterech typów danych ma swój domyślny algorytm przeznaczony do imputacji: zmienna ilościowa - pmm zmienna dychotomiczna (stany 0 lub 1) - logreg zmienna typu wyliczeniowego (nieuporządkowana) - polyreg zmienna typu wyliczeniowego (uporządkowana) - polr Niewątpliwą zaletą metody pmm jest to, że wartości imputowane są ograniczone jedynie do obserwowanych wartości. Metody norm i norm.nob uzupełniają brakujące wartości w oparciu o model liniowy. Są one szybkie i efektywne w przypadku gdy reszty modelu są zbliżone rozkładem do normalności. Druga z tych technik nie bierze pod uwagę niepewności związanej z modelem imputującym. Metoda 2L.norm opiera się na dwupoziomowym heterogenicznym modelu liniowym (skupienia są włączone jako efekt do modelu). Technika polyreg korzysta z funkcji multinom pakietu nnet tworzącej model wielomianowy. polr opiera się o proporcjonalny model logitowy z pakietu MASS. lda to model dyskryminacyjny klasyfikujący obiekty na podstawie prawdopodobieństw a posteriori. Metoda sample zastępuje braki losowa wybranymi wartościami spośród wartości obserwowanych. Ad 3. Do ustalenia predyktorów w modelu mice służy funkcja predictorMatrix. Po pierwsze wyświetla ona domyślny układ predyktorów włączanych do modelu. Można go dowolnie zmienić i podstawić do modelu imputującego dane parametrem predictorMatrix. Zera występujące w kolejnych wierszach macierzy predyktorów oznaczają pominięcie tej zmiennej przy imputacji innej zmiennej. Jeśli dodatkowo chcemy by jakaś zmienna nie była imputowana, to oprócz usunięcia jej z listy predyktorów, należy wymazać ją z listy metod predykcji (method). Ogólne zalecenia co do tego jakie zmienne stosować jako predyktory jest takie, żeby brać ich jak najwięcej. Spowoduje to, że bardziej prawdopodobny staje się brak typu MAR a nie MNAR. Z drugiej jednak strony, nierzadko zbiory zawierają olbrzymią liczbę zmiennych i włączanie ich wszystkich do modelu imputującego nie będzie miało sensu. Zalecenia doboru zmiennych są następujące: weź wszystkie te zmienne, które są włączane do modelu właściwego, czyli tego za pomocą którego chcesz poznać strukturę zależności; czasem do modelu imputującego należy też włączyć interakcje zmiennych z modelu właściwego; dodaj zmienne, które mogą mieć wpływ na wybrakowane cechy; włącz zmienne istotnie podnoszące poziom wyjaśnionej wariancji modelu; na koniec usuń te zmienne spośród predyktorów, które same zawierają zbyt wiele braków. Ad 4-7. Decyzje podejmowane w tych punktach zależą istotnie od analizowanego zbioru i będą przedmiotem oddzielnych analiz w kontekście rozważanych zbiorów i zadań. Przykład 2.1 Dokonamy imputacji zbioru airquality z wykorzystaniem pakietów mice i VIM (Templ et al. 2019) data &lt;- airquality summary(data) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## # tworzymy dodatkowe braki danych data[4:10,3] &lt;- rep(NA,7) data[1:5,4] &lt;- NA summary(data) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :57.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:73.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.806 Mean :78.28 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 NA&#39;s :7 NA&#39;s :5 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## md.pattern(data) ## Month Day Temp Solar.R Wind Ozone ## 104 1 1 1 1 1 1 0 ## 34 1 1 1 1 1 0 1 ## 3 1 1 1 1 0 1 1 ## 1 1 1 1 1 0 0 2 ## 4 1 1 1 0 1 1 1 ## 1 1 1 1 0 1 0 2 ## 1 1 1 1 0 0 1 2 ## 3 1 1 0 1 1 1 1 ## 1 1 1 0 1 0 1 2 ## 1 1 1 0 0 0 0 4 ## 0 0 5 7 7 37 56 Do ilustracji braków danych można zastosować funkcje pakietu VIM. library(VIM) aggr(data, numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7) ## ## Variables sorted by number of missings: ## Variable Count ## Ozone 0.24183007 ## Solar.R 0.04575163 ## Wind 0.04575163 ## Temp 0.03267974 ## Month 0.00000000 ## Day 0.00000000 Tak przedstawia się wykres rozrzutu zmiennych Ozone i Solar.R z uwzględnieniem położenia braków danych. marginplot(data[c(1,2)]) Dokonamy imputacji metodą pmm. tempData &lt;- mice(data, maxit=50, meth=&#39;pmm&#39;, seed=44, printFlag = F) summary(tempData) ## Class: mids ## Number of multiple imputations: 5 ## Imputation methods: ## Ozone Solar.R Wind Temp Month Day ## &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## Ozone Solar.R Wind Temp Month Day ## Ozone 0 1 1 1 1 1 ## Solar.R 1 0 1 1 1 1 ## Wind 1 1 0 1 1 1 ## Temp 1 1 1 0 1 1 ## Month 1 1 1 1 0 1 ## Day 1 1 1 1 1 0 Ponieważ, funkcja mice domyślnie dokonuje 5 kompletnych imputacji, możemy się przekonać jak bardzo różnią się poszczególne imputacje i zdecydować się na jedną z nich. head(tempData$imp$Ozone) ## 1 2 3 4 5 ## 5 21 20 7 36 13 ## 10 21 16 44 22 21 ## 25 14 14 14 6 8 ## 26 23 18 8 19 14 ## 27 37 23 21 7 9 ## 32 63 23 7 52 39 Ostatecznie imputacji dokonujemy wybierając jeden z zestawów danych uzupełniających (np. pierwszy). completedData &lt;- mice::complete(tempData, 1) summary(completedData) ## Ozone Solar.R Wind Temp ## Min. : 1.0 Min. : 7.0 Min. : 1.700 Min. :57.00 ## 1st Qu.: 20.0 1st Qu.:115.0 1st Qu.: 7.400 1st Qu.:73.00 ## Median : 32.0 Median :212.0 Median : 9.700 Median :79.00 ## Mean : 42.5 Mean :187.9 Mean : 9.931 Mean :78.14 ## 3rd Qu.: 59.0 3rd Qu.:259.0 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.0 Max. :334.0 Max. :20.700 Max. :97.00 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 Za pomocą funkcji pakietu mice możemy również przedstawić graficznie gdzie i jak zostały uzupełnione dane. densityplot(tempData, ~Ozone+Solar.R+Wind+Temp) stripplot(tempData, Ozone+Solar.R+Wind+Temp~.imp, pch = 20, cex = 1.2) Bibliografia "],
["podzia-metod-data-mining.html", "3 Podział metod data mining 3.1 Rodzaje wnioskowania 3.2 Modele regresyjne 3.3 Modele klasyfikacyjne 3.4 Modele grupujące", " 3 Podział metod data mining 3.1 Rodzaje wnioskowania Data mining to zestaw metod pozyskiwania wiedzy na podstawie danych. Ową wiedzę zdobywamy w procesie wnioskowania na podstawie modeli. Wnioskowanie możemy podzielić na dedukcyjne i indukcyjne. I tak z wnioskowaniem dedukcyjnym mamy do czynienia wówczas, gdy na podstawie obecnego stanu wiedzy potrafimy odpowiedzieć na postawione pytanie dotyczące nowej wiedzy, stosując reguły wnioskowania. O wnioskowaniem indukcyjnym powiemy, że jest to metoda pozyskiwania wiedzy na podstawie informacji ze zbioru uczącego. Znajduje ono szerokie zastosowanie w data mining i charakteryzuje się omylnością, ponieważ nawet najlepiej nauczony model na zbiorze uczącym nie zapewnia nam prawdziwości odpowiedzi w przypadku nowych danych, a jedynie je uprawdopodabnia. Esencją wnioskowania indukcyjnego w zakresie data mining, jest poszukiwanie na podstawie danych uczących modelu charakteryzującego się najlepszymi właściwościami predykcyjnymi i dającego się zastosować do zupełnie nowego zbioru danych. Każdy proces uczenia z wykorzystaniem wnioskowania indukcyjnego składa się z następujących elementów. 3.1.1 Dziedzina Dziedzina to zbiór wszystkich obiektów pozostających w zainteresowaniu badacza, będących przedmiotem wnioskowania, oznaczana najczęściej przez \\(X\\). Przykładowo mogą to być zbiory osób, transakcji, urządzeń, instytucji, itp. 3.1.2 Obserwacja Każdy element dziedziny \\(x\\in X\\) nazywamy obserwacją. Obserwacją nazywać będziemy zarówno rekordy danych ze zbioru uczącego, jak i ze zbioru testowego. 3.1.3 Atrybuty obserwacji Każdy obiekt z dziedziny \\(x\\in X\\) można opisać zestawem cech (atrybutów), które w notacji matematycznej oznaczymy przez \\(a:X\\to A\\), gdzie \\(A\\) jest przestrzenią wartości atrybutów. Każda obserwacja \\(x\\) posiadająca \\(k\\) cech da się wyrazić wektorowo jako \\((a_1(x), a_2(x), \\ldots, a_k(x))\\). Dla większości algorytmów uczenia maszynowego wyróżnia się trzy typy atrybutów: nominalne - posiadające skończoną liczbę stanów, które posiadają porządku; porządkowe - posiadające skończoną liczbę stanów z zachowaniem porządku; ciągłe - przyjmujące wartości numeryczne. Często jeden z atrybutów spełnia specjalną rolę, ponieważ stanowi realizację cechy, którą traktujemy jako wyjściową (ang. target value attribute). W tym przypadku powiemy o nadzorowanym uczeniu maszynowym. Jeśli zmiennej wyjściowej nie ma dziedzinie, to mówimy o nienadzorowanym uczeniu maszynowym. 3.1.4 Zbiór uczący Zbiorem uczącym \\(T\\) (ang. training set) nazywamy podzbiór \\(D\\) dziedziny \\(X\\) (czyli \\(T\\subseteq D\\subseteq X\\)), gdzie zbiór \\(D\\) stanowi ogół dostępnych obserwacji z dziedziny \\(X\\). Zbiór uczący zawiera informacje dotyczące badanego zjawiska, na podstawie których, dokonuje się doboru modelu, selekcji cech istotnych z punktu widzenia własności predykcyjnych lub jakości klasyfikacji, budowy modelu oraz optymalizacji jego parametrów. W przypadku uczenia z nauczycielem (nadzorowanego) zbiór \\(T\\) zawiera informację o wartościach atrybutów zmiennej wynikowej. 3.1.5 Zbiór testowy Zbiór testowy \\(T&#39;\\) (ang. test set) będący dopełnieniem zbioru uczącego do zbioru \\(D\\), czyli \\(T&#39;=D\\setminus T\\), stanowi zestaw danych służący do oceny poprawności modelu nadzorowanego. W przypadku metod nienadzorowanych raczej nie stosuje się zbiorów testowych. 3.1.6 Model Model to narzędzie pozyskiwania wiedzy na podstawie zbioru uczącego. Nauczony model jest zbiorem reguł \\(f\\), którego zadaniem jest oszacowanie wielkości wartości wynikowej lub odpowiednia klasyfikacja obiektów. W zadaniu grupowania obiektów (ang. clustering task), celem modelu jest podanie grup możliwie najbardziej jednorodnych przy zadanym zestawie zmiennych oraz ustalonej liczbie skupień (czasami wyznaczenie liczby skupień jest również częścią zadania stawianego przed modelem). 3.1.7 Jakość dopasowania modelu Do oceny jakości dopasowania modelu wykorzystuje się, w zależności od zadania, wiele współczynników (np. dla zadań regresyjnych są to błąd średnio-kwadratowy - ang. Mean Square Error, a dla zadań klasyfikacyjnych - trafność - ang. Accuracy). Możemy mówić dwóch rodzajach dopasowania modeli: poziom dopasowania na zbiorze uczącym poziom dopasowania na zbiorze testowym (oczywiście z punktu widzenia utylitarności modelu ten współczynnik jest ważniejszy). W sytuacji, w której model wykazuje dobre charakterystyki jakości dopasowania na zbiorze uczącym ale słabe na testowym, mówimy o zjawisku przeuczenia modelu (ang. overfitting). Oznacza to, że model wskazuje predykcję poprawnie jedynie dla zbioru treningowego ale ma słaba własności generalizacyjne nowe przypadki danych. Takie model nie przedstawiają znaczącej wartości w odkrywaniu wiedzy w sposób indukcyjny. Z drugiej strony parametry dopasowania modelu mogą pokazywać słabe dopasowanie, zarówno na zbiorze uczącym, jak i testowym. Wówczas również model nie jest użyteczny w pozyskiwaniu wiedzy na temat badanego zjawiska, a sytuację taką nazywamy niedouczeniem (ang. underfitting). Rysunek 3.1: Przykłady niedoucznia (wykresy 1 i 4), poprawego modelu (2 i 5) i przeuczenia (3 i 6). Pierwszy wiersz wykresów pokazuje klasyfikację na podstawie modelu na zbiorze uczącym, a drugi na zbiorze testowym. Wykres na dole pokazuje związek pomiędzy złożonością modelu a wielkością błędu predykcji. Źródło: https://cambridgecoding.wordpress.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/ 3.2 Modele regresyjne Jednym z rodzajów zadań bazującym na wnioskowaniu indukcyjnym jest model regresyjny. Należy on do grupy metod nadzorowanych, których celem jest oszacowanie wartości cechy wyjściowej (która jest ilościowa) na podstawie zestawu predyktorów, które mogą być ilościowe i jakościowe. Uczenie takich modeli odbywa się poprzez optymalizację funkcji celu (np. \\(MSE\\)) na podstawie zbioru uczącego. 3.3 Modele klasyfikacyjne Podobnie jak modele regresyjne, modele klasyfikacyjne należą do grupy metod nadzorowanego uczenia maszynowego. Ich zadaniem jest właściwa klasyfikacja obiektów na podstawie wielkości predyktorów. Odpowiedzią modelu jest zawsze cecha typu jakościowego, natomiast predyktory mogą mieć dowolny typ. Wyróżnia się klasyfikację dwu i wielostanową. Lista modeli realizujących klasyfikację binarną jest nieco dłuższa niż w przypadku modeli z wielostanową cechą wynikową. Proces uczenia modelu klasyfikacyjnego również opiera się na optymalizacji funkcji celu. Tym razem są to zupełnie inne miary jakości dopasowania (np. trafność, czyli odsetek poprawnych klasyfikacji). 3.4 Modele grupujące Bardzo szeroką gamę modeli nienadzorowanych stanowią metody analizy skupień. Ich zadaniem jest grupowanie obiektów w możliwie najbardziej jednorodne grupy, na podstawie wartości atrybutów poddanych analizie. Ponieważ są to metody “bez nauczyciela”, to ocena ich przydatności ma nieco inny charakter i choć istnieją różne wskaźniki jakości grupowania, to trudno tu o obiektywne wskazanie najlepszego rozwiązania. "],
["drzewa-decyzyjne.html", "4 Drzewa decyzyjne 4.1 Węzły i gałęzie 4.2 Rodzaje reguł podziału 4.3 Algorytm budowy drzewa 4.4 Kryteria zatrzymania 4.5 Reguły podziału 4.6 Przycinanie drzewa decyzyjnego 4.7 Obsługa braków danych 4.8 Zalety i wady 4.9 Inne algorytmy budowy drzew decyzyjnych implementowane w R", " 4 Drzewa decyzyjne Drzewo decyzyjne11 jest strukturą hierarchiczną przedstawiającą model klasyfikacyjny lub regresyjny. Stosowane są szczególnie często wówczas, gdy funkcyjna postać związku pomiędzy predyktorami a zmienną wynikową jest nieznana lub ciężka do ustalenia. Każde drzewo decyzyjne składa się z korzenia (ang. root), węzłów (ang. nodes) i liści (ang. leaves). Korzeniem nazywamy początkowy węzeł drzewa, z którego poprzez podziały (ang. splits) powstają kolejne węzły potomne. Końcowe węzły, które nie podlegają podziałom nazywamy liśćmi, a linie łączące węzły nazywamy gałęziami (ang. branches). Jeśli drzewo służy do zadań klasyfikacyjnych, to liście zawierają informację o tym, która klasa w danym ciągu podziałów jest najbardziej prawdopodobna. Natomiast, jeśli drzewo jest regresyjne, to liście zawierają warunkowe miary tendencji centralnej (najczęściej średnią) wartości zmiennej wynikowej. Warunek stanowi szereg podziałów doprowadzający do danego węzła terminalnego (liścia). W obu przypadkach (klasyfikacji i regresji) drzewo “dąży” do takiego podziału by kolejne węzły, a co za tym idzie również liście, były ja najbardziej jednorodne ze względu na zmienną wynikową. Rysunek 4.1: Przykład działania drzewa regresyjnego. Wykes w lewym górnym rogu pokazuje prawdziwą zależność, wyres po prawej stronie jest ilustracją drzewa decyzyjnego, a wykres w lewym dolnym rogu pokazuje dyskretyzację przestrzeni dokonaną przez drzewo, czyli sposób jego działania. 4.1 Węzły i gałęzie Każdy podział rozdziela dziedzinę \\(X\\) na dwa lub więcej podobszarów dziedziny i wówczas każda obserwacja węzła nadrzędnego jest przyporządkowana węzłom potomnym. Każdy odchodzący węzeł potomny jest połączony gałęzią, która to wiąże się ściśle z możliwymi wynikami podziału. Każdy \\(\\mathbf{n}\\)-ty węzeł można opisać jako podzbiór dziedziny w następujący sposób \\[\\begin{equation} X_{\\mathbf{n}}=\\{x\\in X|t_1(x)=r_1,t_2(x)=r_2,\\ldots,t_k(x)=r_k\\}, \\end{equation}\\] gdzie \\(t_1,t_2,\\ldots,t_k\\) są podziałami, które przeprowadzają \\(x\\) w obszary \\(r_1, r_2,\\ldots, r_k\\). Przez \\[\\begin{equation} S_{\\mathbf{n}, t=r}=\\{x\\in S|t(x)=r\\} \\end{equation}\\] rozumiemy, że dokonano takiego ciągu podziałów zbioru \\(S\\), że jego wartości znalazły się w \\(\\mathbf{n}\\)-tym węźle. 4.2 Rodzaje reguł podziału Najczęściej występujące reguły podziału w drzewach decyzyjnych są jednowymiarowe, czyli warunek podziału jest generowany na podstawie jednego atrybutu. Istnieją podziały wielowymiarowe ale ze względu na złożoność obliczeniową są rzadziej stosowane. 4.2.1 Podziały dla atrybutów ze skali nominalnej Istnieją dwa typy reguł podziału dla skali nominalnej: oparte na wartości atrybutu (ang. value based) - wówczas funkcja testowa przyjmuje postać \\(t(x)=a(x)\\), czyli podział generują wartości atrybutu; oparte na równości (ang. equality based) - gdzie funkcja testowa jest zdefiniowana jako \\[\\begin{equation} t(x)= \\begin{cases} 1, &amp;\\text{ gdy } a(x)=\\nu\\\\ 0, &amp; \\text{ w przeciwnym przypadku}, \\end{cases} \\end{equation}\\] gdzie \\(\\nu\\in A\\) i \\(A\\) jest zbiorem możliwych wartości \\(a\\). W tym przypadku podział jest dychotomiczny, albo obiekt ma wartość atrybutu równą \\(\\nu\\), albo go nie ma. 4.2.2 Podziały dla atrybutów ze skali ciągłej Reguły podziału stosowane do skali ciągłej, to: oparta na nierównościach (ang. inequality based) - zdefiniowana jako \\[\\begin{equation} t(x) = \\begin{cases} 1, &amp;\\text{ gdy }a(x)\\leq \\nu\\\\ 0, &amp; \\text{w przeciwnym przypadku}, \\end{cases} \\end{equation}\\] gdzie \\(\\nu\\in A\\); przedziałowa (ang. interval based) - zdefiniowana jako \\[\\begin{equation} t(x) = \\begin{cases} 1, &amp;\\text{ gdy }a(x) \\in I_1\\\\ 2, &amp;\\text{ gdy }a(x) \\in I_2\\\\ \\vdots &amp; \\\\ k, &amp;\\text{ gdy }a(x) \\in I_k\\\\ \\end{cases} \\end{equation}\\] gdzie \\(I_1,I_2,\\ldots,I_k\\subset A\\) stanowią rozłączny podział (przedziałami) przeciwdziedziny \\(A\\). 4.2.3 Podziały dla atrybutów ze skali porządkowej Podziały te mogą wykorzystywać oba wcześniej wspomniane typy, w zależności od potrzeb. 4.3 Algorytm budowy drzewa stwórz początkowy węzeł (korzeń) i oznacz go jako otwarty; przypisz wszystkie możliwe rekordy do węzła początkowego; dopóki istnieją otwarte węzły wykonuj: wybierz węzeł \\(\\mathbf{n}\\), wyznacz potrzebne statystyki opisowe zmiennej zależnej dla tego węzła i przypisz wartość docelową; jeśli kryterium zatrzymania podziału jest spełnione dla węzła \\(n\\), to oznacz go za zamknięty; w przeciwnym przypadku wybierz podział \\(r\\) elementów węzła \\(\\mathbf{n}\\), i dla każdego podzbioru podziału stwórz węzeł niższego rzędu (potomka) \\(\\mathbf{n}_r\\) oraz oznacz go jako otwarty; następnie przypisz wszystkie przypadki generowane podziałem \\(r\\) do odpowiednich węzłów potomków \\(\\mathbf{n}_r\\); oznacza węzeł \\(\\mathbf{n}\\) jako zamknięty. Sposób przypisywania wartości docelowej wiąże się ściśle z rodzajem drzewa. W drzewach regresyjnych chodzi o wyliczenie średniej lub mediany dla obserwacji ujętych w danym węźle. Natomiast w przypadku drzewa klasyfikacyjnego, wyznacza się wartości prawdopodobieństw przynależności obserwacji znajdującej się w danym węźle do poszczególnych klas \\[\\begin{equation} \\P(d|\\mathbf{n})=\\P_{T_\\mathbf{n}}(d)=\\frac{|T_\\mathbf{n}^d|}{|T_\\mathbf{n}|}, \\end{equation}\\] gdzie \\(T_\\mathbf{n}\\) oznaczają obserwacje zbioru uczącego znajdujące się w węźle \\(\\mathbf{n}\\), a \\(T_\\mathbf{n}^d\\) oznacza dodatkowo podzbiór zbioru uczącego w \\(\\mathbf{n}\\) węźle, które należą do klasy \\(d\\). Oczywiście klasyfikacja na podstawie otrzymanych prawdopodobieństw w danym węźle jest dokonana przez wybór klasy charakteryzującej się najwyższym prawdopodobieństwem. 4.4 Kryteria zatrzymania Kryterium zatrzymania jest warunkiem, który decyduje o tym, że dany węzeł uznajemy za zamknięty i nie dokonujemy dalszego jego podziału. Wyróżniamy następujące kryteria zatrzymania: jednorodność węzła - w przypadku drzewa klasyfikacyjnego może zdarzyć się sytuacja, że wszystkie obserwacje węzła będą pochodziły z jednej klasy. Wówczas nie ma sensu dokonywać dalszego podziału węzła; węzeł jest pusty - zbiór przypisanych obserwacji zbioru uczącego do \\(\\mathbf{n}\\)-tego węzła jest pusty; brak reguł podziału - wszystkie reguły podziału zostały wykorzystane, zatem nie da się stworzyć potomnych węzłów, które charakteryzowałyby się większą homogenicznością; Warunki ujęte w pierwszych dwóch kryteriach mogą być nieco złagodzone, poprzez zatrzymanie podziałów wówczas, gdy prawdopodobieństwo przynależenia do pewnej klasy przekroczy ustalony próg lub gdy liczebność węzła spadnie poniżej ustalonej wartości. W literaturze tematu istnieje jeszcze jedno często stosowane kryterium zatrzymania oparte na wielkości drzewa. Węzeł potomny ustala się jako zamknięty, gdy długość ścieżki dojścia do nie go przekroczy ustaloną wartość. 4.5 Reguły podziału Ważnym elementem algorytmu tworzenia drzewa regresyjnego jest reguła podziału. Dobierana jest w taki sposób aby zmaksymalizować zdolności generalizacyjne drzewa. Złożoność drzewa mierzona jest najczęściej przeciętną liczbą podziałów potrzebnych do dotarcia do liścia zaczynając od korzenia. Liście są najczęściej tworzone wówczas gdy dyspersja wartości wynikowej jest stosunkowo mała lub węzeł zawiera w miarę homogeniczne obserwacje ze względu na przynależność do klasy zmiennej wynikowej. W przypadku drzew regresyjnych zmienność na poziomie węzłów jest dobrą miarą służącą do definiowania podziału w węźle. I tak, jeśli pewien podział generuje nam stosunkowo małe dyspersje wartości docelowych w węzłach potomnych, to można ten podział uznać za właściwy. Jeśli \\(T_n\\) oznacza zbiór rekordów należących do węzła \\(n\\), a \\(T_{n,t=r}\\) są podzbiorami generowanymi przez podział \\(r\\) w węzłach potomnych dla \\(n\\), to dyspersję wartości docelowej \\(f\\) będziemy oznaczali następująco \\[\\begin{equation}\\label{dyspersja} \\operatorname{disp}_{T_{n,t=r}}(f). \\end{equation}\\] Regułę podziału możemy określać poprzez minimalizację średniej ważonej dyspersji wartości docelowej następującej postaci \\[\\begin{equation}\\label{reg_podz} \\operatorname{disp}_n(f|t)=\\sum_{r\\in R_t}\\frac{|T_{n,t=r}|}{|T_n|}\\operatorname{disp}_{T_{n,t=r}}(f), \\end{equation}\\] gdzie \\(|\\ |\\) oznacza moc zbioru, a \\(R_t\\) zbiór wszystkich możliwych wartości reguły podziału. Czasami wygodniej będzie maksymalizować przyrost dyspersji (lub spadek) \\[\\begin{equation}\\label{przyrost} \\bigtriangleup \\operatorname{disp}_n(f|t)=\\operatorname{disp}_n(f)-\\sum_{r\\in R_t}\\frac{|T_{n,t=r}|}{|T_n|}\\operatorname{disp}_{T_{n,t=r}}(f). \\end{equation}\\] Miarą heterogeniczności węzłów ze względu na zmienną wynikową (ang. impurity) w drzewach klasyfikacyjnych, która pozwala na tworzenie kolejnych podziałów węzła, są najczęściej wskaźnik Gini’ego i entropia (Breiman 1998). Entropią podzbioru uczącego w węźle \\(\\mathbf{n}\\), wyznaczamy wg wzoru \\[\\begin{equation} E_{T_{\\mathbf{n}}}(c|t) = \\sum_{x\\in R_t} \\frac{|T_{\\mathbf{n}, t=r}|}{|T_{\\mathbf{n}}|}E_{T_{\\mathbf{n}, t=r}}(c), \\end{equation}\\] gdzie \\(t\\) jest podziałem (kandydatem), \\(r\\) potencjalnym wynikiem podziału \\(t\\), \\(c\\) jest oznaczeniem klasy zmiennej wynikowej, a \\[\\begin{equation} E_{T_{\\mathbf{n}, t=r}}(c) = \\sum_{d\\in C}-\\P_{T_{\\mathbf{n}, t=r}}(c=d)\\log\\P_{T_{\\mathbf{n}, t=r}}(c=d), \\end{equation}\\] przy czym \\[\\begin{equation} \\P_{T_{\\mathbf{n}, t=r}}(c=d)= \\P_{T_{\\mathbf{n}}}(c=d|t=r). \\end{equation}\\] Podobnie definiuje się indeks Gini’ego \\[\\begin{equation} Gi_{T_{\\mathbf{n}}}(c|t) = \\sum_{x\\in R_t} \\frac{|T_{\\mathbf{n}, t=r}|}{|T_{\\mathbf{n}}|}Gi_{T_{\\mathbf{n}, t=r}}(c), \\end{equation}\\] gdzie \\[\\begin{equation} Gi_{T_{\\mathbf{n}, t=r}}(c) = \\sum_{d\\in C}\\P_{T_{\\mathbf{n}, t=r}}(c=d)\\cdot(1-\\P_{T_{\\mathbf{n}, t=r}}(c=d))= 1-\\sum_{d\\in C}\\P^2_{T_{\\mathbf{n}, t=r}}(c=d). \\end{equation}\\] Dla tak zdefiniowanych miar “nieczystości” węzłów, podziału dokonujemy w taki sposób, aby zminimalizować współczynnik Gini’ego lub entropię. Im niższe miary nieczystości, tym bardziej obserwacje znajdujące się w węźle są monokulturą12. Nierzadko korzysta się również z współczynnika przyrostu informacji (ang. information gain) \\[\\begin{equation} \\Delta E_{T_{\\mathbf{n}}}(c|t)=E_{T_{\\mathbf{n}}}(c)-E_{T_{\\mathbf{n}}}(c|t). \\end{equation}\\] Istnieje również jego odpowiednik dla indeksu Gini’ego. W obu przypadkach optymalnego podziału szukamy poprzez maksymalizację przyrostu informacji. 4.6 Przycinanie drzewa decyzyjnego Uczenie drzewa decyzyjnego wiąże się z ryzykiem przeuczenia modelu (podobnie jak to się ma w przypadku innych modeli predykcyjnych). Wcześniej przytoczone reguły zatrzymania (np. głębokość drzewa czy zatrzymanie przy osiągnięciu jednorodności na zadanym poziomie) pomagają kontrolować poziom generalizacji drzewa ale czasami będzie dodatkowo potrzebne przycięcie drzewa, czyli usunięcie pewnych podziałów, a co za tym idzie, również liści (węzłów). 4.6.1 Przycinanie redukujące błąd Jedną ze strategii przycinania drzewa jest przycinanie redukujące błąd (ang. reduced error pruning). Polega ono na porównaniu błędów (najczęściej używana jest miara odsetka błędnych klasyfikacji lub MSE) liścia \\(\\mathbf{l}\\) i węzła do którego drzewo przycinamy \\(\\mathbf{n}\\) na całkiem nowym zbiorze uczącym \\(R\\). Niech \\(e_R(\\mathbf{l})\\) i \\(e_R(\\mathbf{n})\\) oznaczają odpowiednio błędy na zbiorze \\(R\\) liścia i węzła. Przez błąd węzła rozumiemy błąd pod-drzewa o korzeniu \\(\\mathbf{n}\\). Wówczas jeśli zachodzi warunek \\[\\begin{equation} e_R(\\mathbf{l})\\leq e_R(\\mathbf{n}), \\end{equation}\\] to zaleca się zastąpić węzeł \\(\\mathbf{n}\\) liściem \\(\\mathbf{l}\\). 4.6.2 Przycinanie minimalizujące błąd Przycinanie minimalizujące błąd opiera się na spostrzeżeniu, że błąd drzewa przyciętego charakteryzuje się zbyt pesymistyczną oceną i dlatego wymaga korekty. Węzeł drzewa klasyfikacyjnego \\(\\mathbf{n}\\) zastępujemy liściem \\(\\mathbf{l}\\), jeśli \\[\\begin{equation} \\hat{e}_T(\\mathbf{l})\\leq \\hat{e}_T(\\mathbf{n}), \\end{equation}\\] gdzie \\[\\begin{equation} \\hat{e}_T(\\mathbf{n})=\\sum_{\\mathbf{n}&#39;\\in N(\\mathbf{n})}\\frac{|T_{\\mathbf{n}&#39;}|}{|T_\\mathbf{n}|}\\hat{e}_T(\\mathbf{n}&#39;), \\end{equation}\\] a \\(N(\\mathbf{n})\\) jest zbiorem wszystkich możliwych węzłów potomnych węzła \\(\\mathbf{n}\\) i \\[\\begin{equation} \\hat{e}_T(\\mathbf{l})=1-\\frac{|\\{x\\in T_\\mathbf{l}|c(x)=d_{\\mathbf{l}}\\}|+mp}{|T_\\mathbf{l}|+m}, \\end{equation}\\] gdzie \\(p\\) jest prawdopodobieństwem przynależności do klasy \\(d_{\\mathbf{l}}\\) ustalona na podstawie zewnętrznej wiedzy (gdy jej nie posiadamy przyjmujemy \\(p=1/|C|\\)). W przypadku drzewa regresyjnego znajdujemy wiele analogii, ponieważ jeśli dla pewnego zbioru rekordów \\(T\\) spełniony jest warunek \\[\\begin{equation}\\label{kryterium1} \\operatorname{mse}_T(\\mathbf{l})\\leq\\operatorname{mse}_T(\\mathbf{n}), \\end{equation}\\] gdzie \\(\\mathbf{l}\\) i \\(\\mathbf{n}\\) oznaczają odpowiednio liść i węzeł, to wówczas zastępujemy węzeł \\(\\mathbf{n}\\) przez liść \\(\\mathbf{l}\\). Estymatory wyznaczone na podstawie niewielkiej próby, mogą być obarczone znaczącym błędem. Wyliczanie błędu średnio-kwadratowego dla podzbioru nowych wartości może się charakteryzować takim obciążeniem. Dlatego stosuje się statystyki opisowe z poprawką, której pochodzenie może mieć trzy źródła: wiedza merytoryczna na temat szukanej wartości, założeń modelu lub na podstawie wyliczeń opartych o cały zbiór wartości. Skorygowany estymator błędu średnio-kwadratowego ma następującą postać \\[\\begin{equation}\\label{mse} \\widehat{\\operatorname{mse}}_T(\\mathbf{l})=\\frac{\\sum_{x\\in T}(f(x)-m_{\\mathbf{l},m,m_0}(f))^2+mS_0^2}{|T_\\mathbf{l}|+m}, \\end{equation}\\] gdzie \\[\\begin{equation}\\label{poprawka} m_{\\mathbf{l},m,m_0}(f)=\\frac{\\sum_{x\\in T_\\mathbf{l}}f(x)+mm_0}{|T_\\mathbf{l}|+m}, \\end{equation}\\] a \\(m_0\\) i \\(S_0^2\\) są średnią i wariancją wyznaczonymi na całej próbie uczącej. Błąd średnio-kwadratowy węzła \\(\\mathbf{n}\\) ma postać \\[\\begin{equation}\\label{propagacja} \\widehat{\\operatorname{mse}}_T(\\mathbf{n})=\\sum_{\\mathbf{n}&#39;\\in N(\\mathbf{n})}\\frac{|T_{\\mathbf{n}&#39;}|}{|T_\\mathbf{n}|}\\widehat{\\operatorname{mse}}_T(\\mathbf{n}&#39;). \\end{equation}\\] Wówczas kryterium podcięcia można zapisać w następujący sposób \\[\\begin{equation}\\label{kryterium2} \\widehat{\\operatorname{mse}}_T(\\mathbf{l}) \\leq \\widehat{\\operatorname{mse}}_T(\\mathbf{n}) \\end{equation}\\] 4.6.3 Przycinanie ze względu na współczynnik złożoności drzewa Przycinanie ze względu na współczynnik złożoności drzewa (ang. cost-complexity pruning) polega na wprowadzeniu “kary” za zwiększoną złożoność drzewa. Drzewa klasyfikacyjne przycinamy gdy spełniony jest warunek \\[\\begin{equation} e_T(\\mathbf{l})\\leq e_T(\\mathbf{n})+\\alpha C(\\mathbf{n}), \\end{equation}\\] gdzie \\(C(\\mathbf{n})\\) oznacza złożoność drzewa mierzoną liczbą liści, a \\(\\alpha\\) parametrem wagi kary za złożoność drzewa. Wspomniane kryterium przycięcia dla drzew regresyjnych bazuje na względnym błędzie średnio-kwadratowym (ang. relative square error), czyli \\[\\begin{equation}\\label{rse} \\widehat{\\operatorname{rse}}_T(\\mathbf{n})=\\frac{|T|\\widehat{\\operatorname{mse}}_T(\\mathbf{n})}{(|T|-1)S^2_T(f)}, \\end{equation}\\] gdzie \\(T\\) oznacza podzbiór \\(X\\), \\(S^2_T\\) wariancję na zbiorze \\(T\\). Wówczas kryterium podcięcia wygląda następująco \\[\\begin{equation}\\label{kryterium3} \\widehat{\\operatorname{rse}}_T(\\mathbf{l})\\leq \\widehat{\\operatorname{rse}}_T(\\mathbf{n})+\\alpha C(\\mathbf{n}). \\end{equation}\\] 4.7 Obsługa braków danych Drzewa decyzyjne wyjątkowo dobrze radzą sobie z obsługa zbiorów z brakami. Stosowane są głównie dwie strategie: udziałów obserwacji (ang. fractional instances) - rozważane są wszystkie możliwe podziały dla brakującej obserwacji i przypisywana jest im odpowiednia waga lub prawdopodobieństwo, w oparciu o zaobserwowany rozkład znanych obserwacji. Te same wagi są stosowane do predykcji wartości na podstawie drzewa z brakami danych. podziałów zastępczych (ang. surrogate splits) - jeśli wynik podziału nie może być ustalony dla obserwacji z brakami, to używany jest podział zastępczy (pierwszy), jeśli i ten nie może zostać ustalony, to stosuje się kolejny. Kolejne podziały zastępcze są generowane tak, aby wynik podziału możliwie najbardziej przypominał podział właściwy. 4.8 Zalety i wady 4.8.1 Zalety łatwe w interpretacji; nie wymagają żmudnego przygotowania danych (brak standaryzacji, wprowadzania zmiennych binarnych, dopuszcza występowanie braków danych); działa na obu typach zmiennych - jakościowych i ilościowych; dopuszcza nieliniowość związku między zmienną wynikową a predyktorami; odporny na odstępstwa od założeń; pozwala na obsługę dużych zbiorów danych. 4.8.2 Wady brak jawnej postaci zależności; zależność struktury drzewa od użytego algorytmu; przegrywa jakością predykcji z innymi metodami nadzorowanego uczenia maszynowego. Przykład 4.1 Przykładem zastosowania drzew decyzyjnych będzie klasyfikacja irysów na podstawie długości i szerokości kielicha i płatka. Przykładem zastosowania drzew decyzyjnych będzie klasyfikacja irysów na podstawie długości i szerokości kielicha i płatka. library(tidyverse) library(rpart) # pakiet do tworzenia drzew typu CART library(rpart.plot) # pakiet do rysowania drzew Każde zadanie ucznia maszynowego zaczynamy od czyszczenia danych i odpowiedniego ich przygotowania ale w tym przypadku skupimy się jedynie na budowie, optymalizacji i ewaluacji modelu. Podział zbioru na próbę uczącą i testową set.seed(44) dt.train &lt;- iris %&gt;% sample_frac(size = 0.7) dt.test &lt;- setdiff(iris, dt.train) str(dt.train) ## &#39;data.frame&#39;: 105 obs. of 5 variables: ## $ Sepal.Length: num 6.4 4.4 6.6 5.4 5 5.4 5.6 4.4 5.4 6.1 ... ## $ Sepal.Width : num 2.7 3.2 3 3 3.6 3.4 2.9 2.9 3.9 2.9 ... ## $ Petal.Length: num 5.3 1.3 4.4 4.5 1.4 1.7 3.6 1.4 1.3 4.7 ... ## $ Petal.Width : num 1.9 0.2 1.4 1.5 0.2 0.2 1.3 0.2 0.4 1.4 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 3 1 2 2 1 1 2 1 1 2 ... str(dt.test) ## &#39;data.frame&#39;: 45 obs. of 5 variables: ## $ Sepal.Length: num 4.7 4.6 5.4 4.8 5.8 5.1 5.1 5.1 5 5.2 ... ## $ Sepal.Width : num 3.2 3.1 3.9 3.4 4 3.8 3.7 3.3 3 3.5 ... ## $ Petal.Length: num 1.3 1.5 1.7 1.6 1.2 1.5 1.5 1.7 1.6 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.4 0.2 0.2 0.3 0.4 0.5 0.2 0.2 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Budowa drzewa Budowy drzewa dokonujemy za pomocą funkcji rpart pakietu rpart (Therneau and Atkinson 2018) stosując zapis formuły zależności. Drzewo zostanie zbudowane z uwzględnieniem kilku kryteriów zatrzymania: minimalna liczebność węzła, który może zostać podzielony to 10 - ze względu na małą liczebność zbioru uczącego; minimalna liczebność liścia to 5 - aby nie dopuścić do przeuczenia modelu; maksymalna głębokość drzewa to 4 - aby nie dopuścić do przeuczenia modelu. mod.rpart &lt;- rpart(Species~., data = dt.train, control = rpart.control(minsplit = 10, minbucket = 5, maxdepth = 4)) summary(mod.rpart) ## Call: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.51470588 0 1.00000000 1.1764706 0.06418173 ## 2 0.41176471 1 0.48529412 0.6617647 0.07457243 ## 3 0.02941176 2 0.07352941 0.1029412 0.03758880 ## 4 0.01000000 3 0.04411765 0.1029412 0.03758880 ## ## Variable importance ## Petal.Width Petal.Length Sepal.Length Sepal.Width ## 35 33 21 12 ## ## Node number 1: 105 observations, complexity param=0.5147059 ## predicted class=setosa expected loss=0.647619 P(node) =1 ## class counts: 37 33 35 ## probabilities: 0.352 0.314 0.333 ## left son=2 (37 obs) right son=3 (68 obs) ## Primary splits: ## Petal.Length &lt; 2.45 to the left, improve=35.95322, (0 missing) ## Petal.Width &lt; 0.8 to the left, improve=35.95322, (0 missing) ## Sepal.Length &lt; 5.45 to the left, improve=25.39467, (0 missing) ## Sepal.Width &lt; 3.35 to the right, improve=12.69596, (0 missing) ## Surrogate splits: ## Petal.Width &lt; 0.8 to the left, agree=1.000, adj=1.000, (0 split) ## Sepal.Length &lt; 5.45 to the left, agree=0.924, adj=0.784, (0 split) ## Sepal.Width &lt; 3.35 to the right, agree=0.819, adj=0.486, (0 split) ## ## Node number 2: 37 observations ## predicted class=setosa expected loss=0 P(node) =0.352381 ## class counts: 37 0 0 ## probabilities: 1.000 0.000 0.000 ## ## Node number 3: 68 observations, complexity param=0.4117647 ## predicted class=virginica expected loss=0.4852941 P(node) =0.647619 ## class counts: 0 33 35 ## probabilities: 0.000 0.485 0.515 ## left son=6 (38 obs) right son=7 (30 obs) ## Primary splits: ## Petal.Width &lt; 1.75 to the left, improve=25.286380, (0 missing) ## Petal.Length &lt; 4.75 to the left, improve=24.879360, (0 missing) ## Sepal.Length &lt; 5.75 to the left, improve= 6.713875, (0 missing) ## Sepal.Width &lt; 3.25 to the left, improve= 1.336180, (0 missing) ## Surrogate splits: ## Petal.Length &lt; 4.75 to the left, agree=0.882, adj=0.733, (0 split) ## Sepal.Length &lt; 6.15 to the left, agree=0.721, adj=0.367, (0 split) ## Sepal.Width &lt; 3.15 to the left, agree=0.618, adj=0.133, (0 split) ## ## Node number 6: 38 observations, complexity param=0.02941176 ## predicted class=versicolor expected loss=0.1315789 P(node) =0.3619048 ## class counts: 0 33 5 ## probabilities: 0.000 0.868 0.132 ## left son=12 (32 obs) right son=13 (6 obs) ## Primary splits: ## Petal.Length &lt; 4.95 to the left, improve=4.0800440, (0 missing) ## Petal.Width &lt; 1.45 to the left, improve=1.2257490, (0 missing) ## Sepal.Width &lt; 2.65 to the right, improve=0.6168705, (0 missing) ## Sepal.Length &lt; 5.95 to the left, improve=0.4736842, (0 missing) ## Surrogate splits: ## Petal.Width &lt; 1.55 to the left, agree=0.868, adj=0.167, (0 split) ## ## Node number 7: 30 observations ## predicted class=virginica expected loss=0 P(node) =0.2857143 ## class counts: 0 0 30 ## probabilities: 0.000 0.000 1.000 ## ## Node number 12: 32 observations ## predicted class=versicolor expected loss=0.03125 P(node) =0.3047619 ## class counts: 0 31 1 ## probabilities: 0.000 0.969 0.031 ## ## Node number 13: 6 observations ## predicted class=virginica expected loss=0.3333333 P(node) =0.05714286 ## class counts: 0 2 4 ## probabilities: 0.000 0.333 0.667 rpart.plot(mod.rpart) Rysunek 4.2: Obraz drzewa klasyfikacyjnego. Powyższy wykres przedstawia strukturę drzewa klasyfikacyjnego. Kolorami są oznaczone klasy, które w danym węźle dominują. Nasycenie barwy decyduje o sile tej dominacji. W każdym węźle podana jest klasa, do której najprawdopodobniej należą jego obserwacje. Ponadto podane są proporcje przynależności do klas zmiennej wynikowej oraz procent obserwacji zbioru uczącego należących do danego węzła. Pod każdym węzłem podana jest reguła podziału. Przycinanie drzewa Zanim przystąpimy do przycinania drzewa należy sprawdzić, jakie są zdolności generalizacyjne modelu. Oceny tej dokonujemy najczęściej sprawdzając macierz klasyfikacji. pred.prob &lt;- predict(mod.rpart, newdata = dt.test) pred.prob[10:20,] ## setosa versicolor virginica ## 10 1 0.00000 0.00000 ## 11 1 0.00000 0.00000 ## 12 1 0.00000 0.00000 ## 13 1 0.00000 0.00000 ## 14 0 0.96875 0.03125 ## 15 0 0.96875 0.03125 ## 16 0 0.96875 0.03125 ## 17 0 0.96875 0.03125 ## 18 0 0.96875 0.03125 ## 19 0 0.96875 0.03125 ## 20 0 0.00000 1.00000 pred.class &lt;- predict(mod.rpart, newdata = dt.test, type = &quot;class&quot;) pred.class ## 1 2 3 4 5 6 ## setosa setosa setosa setosa setosa setosa ## 7 8 9 10 11 12 ## setosa setosa setosa setosa setosa setosa ## 13 14 15 16 17 18 ## setosa versicolor versicolor versicolor versicolor versicolor ## 19 20 21 22 23 24 ## versicolor virginica versicolor versicolor versicolor versicolor ## 25 26 27 28 29 30 ## versicolor versicolor versicolor versicolor versicolor versicolor ## 31 32 33 34 35 36 ## virginica virginica virginica virginica virginica virginica ## 37 38 39 40 41 42 ## virginica virginica virginica virginica virginica virginica ## 43 44 45 ## virginica virginica virginica ## Levels: setosa versicolor virginica tab &lt;- table(predykcja = pred.class, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 16 0 ## virginica 0 1 15 Jak widać z powyższej tabeli, model całkiem dobrze radzi sobie z poprawną klasyfikacją obserwacji do odpowiednich kategorii. Tylko jedna obserwacja została błędnie zaklasyfikowana. W dalszej kolejności sprawdzimy, czy nie jest konieczne przycięcie drzewa. Jednym z kryteriów przycinania drzewa jest przycinanie ze względu na złożoność drzewa. W tym przypadku jest wyrażony parametrem cp. Istnieje powszechnie stosowana reguła jednego odchylenia standardowego, która mówi, że drzewo należy przyciąć wówczas, gdy błąd oszacowany na podstawie sprawdzianu krzyżowego (xerror), pierwszy raz zejdzie poniżej poziomu wyznaczonego przez najniższą wartość błędu powiększonego o odchylenie standardowe tego błędu (xstd). Na podstawie poniższej tabeli można ustalić, że poziomem odcięcia jest wartość \\(0.10294+0.037589=0.140529\\). Pierwszy raz błąd przyjmuje wartość mniejszą od \\(0.140529\\) po drugim podziale (nsplit=2). Temu poziomowi odpowiada cp o wartości \\(0.029412\\) i to jest złożoność drzewa, którą powinniśmy przyjąć do przycięcia drzewa. printcp(mod.rpart) ## ## Classification tree: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## ## Variables actually used in tree construction: ## [1] Petal.Length Petal.Width ## ## Root node error: 68/105 = 0.64762 ## ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.514706 0 1.000000 1.17647 0.064182 ## 2 0.411765 1 0.485294 0.66176 0.074572 ## 3 0.029412 2 0.073529 0.10294 0.037589 ## 4 0.010000 3 0.044118 0.10294 0.037589 plotcp(mod.rpart) Rysunek 4.3: Na wykresie błędów punkt odcięcia zaznaczony jest linią przerywaną Przycięte drzewo wygląda następująco: mod.rpart2 &lt;- prune(mod.rpart, cp = 0.029412) summary(mod.rpart2) ## Call: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.5147059 0 1.00000000 1.1764706 0.06418173 ## 2 0.4117647 1 0.48529412 0.6617647 0.07457243 ## 3 0.0294120 2 0.07352941 0.1029412 0.03758880 ## ## Variable importance ## Petal.Width Petal.Length Sepal.Length Sepal.Width ## 35 31 22 12 ## ## Node number 1: 105 observations, complexity param=0.5147059 ## predicted class=setosa expected loss=0.647619 P(node) =1 ## class counts: 37 33 35 ## probabilities: 0.352 0.314 0.333 ## left son=2 (37 obs) right son=3 (68 obs) ## Primary splits: ## Petal.Length &lt; 2.45 to the left, improve=35.95322, (0 missing) ## Petal.Width &lt; 0.8 to the left, improve=35.95322, (0 missing) ## Sepal.Length &lt; 5.45 to the left, improve=25.39467, (0 missing) ## Sepal.Width &lt; 3.35 to the right, improve=12.69596, (0 missing) ## Surrogate splits: ## Petal.Width &lt; 0.8 to the left, agree=1.000, adj=1.000, (0 split) ## Sepal.Length &lt; 5.45 to the left, agree=0.924, adj=0.784, (0 split) ## Sepal.Width &lt; 3.35 to the right, agree=0.819, adj=0.486, (0 split) ## ## Node number 2: 37 observations ## predicted class=setosa expected loss=0 P(node) =0.352381 ## class counts: 37 0 0 ## probabilities: 1.000 0.000 0.000 ## ## Node number 3: 68 observations, complexity param=0.4117647 ## predicted class=virginica expected loss=0.4852941 P(node) =0.647619 ## class counts: 0 33 35 ## probabilities: 0.000 0.485 0.515 ## left son=6 (38 obs) right son=7 (30 obs) ## Primary splits: ## Petal.Width &lt; 1.75 to the left, improve=25.286380, (0 missing) ## Petal.Length &lt; 4.75 to the left, improve=24.879360, (0 missing) ## Sepal.Length &lt; 5.75 to the left, improve= 6.713875, (0 missing) ## Sepal.Width &lt; 3.25 to the left, improve= 1.336180, (0 missing) ## Surrogate splits: ## Petal.Length &lt; 4.75 to the left, agree=0.882, adj=0.733, (0 split) ## Sepal.Length &lt; 6.15 to the left, agree=0.721, adj=0.367, (0 split) ## Sepal.Width &lt; 3.15 to the left, agree=0.618, adj=0.133, (0 split) ## ## Node number 6: 38 observations ## predicted class=versicolor expected loss=0.1315789 P(node) =0.3619048 ## class counts: 0 33 5 ## probabilities: 0.000 0.868 0.132 ## ## Node number 7: 30 observations ## predicted class=virginica expected loss=0 P(node) =0.2857143 ## class counts: 0 0 30 ## probabilities: 0.000 0.000 1.000 rpart.plot(mod.rpart2) Rysunek 4.4: Drzewo klasyfikacyjne po przycięciu Ocena dopasowania modelu Na koniec budowy modelu należy sprawdzić jego jakość na zbiorze testowym. pred.class2 &lt;- predict(mod.rpart2, newdata = dt.test, type = &quot;class&quot;) tab2 &lt;- table(predykcja = pred.class2, obserwacja = dt.test$Species) tab2 ## obserwacja ## predykcja setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 16 0 ## virginica 0 1 15 Mimo przycięcia drzewa, klasyfikacja pozostaje na niezmienionym poziomie. Odsetek poprawnych klasyfikacji możemy oszacować za pomocą round(sum(diag(tab2))/sum(tab2)*100,1) ## [1] 97.8 4.9 Inne algorytmy budowy drzew decyzyjnych implementowane w R Oprócz najbardziej znanego algorytmu CART implementowanego w postaci funkcji pakietu rpart, istnieją również inne algorytmy, które znalzały swoje implementacje w R. Są to: CHAID13 - algorytm przeznaczony do budowy drzew klafyfikacyjnych, gdzie zarówno zmienna wynikowa, jak i zmienne niezależne muszą być ze skali jakościowej. Główną różnicą w stosunku do drzew typu CART jest sposób budowy podziałów, oparty na teście niezależności \\(\\chi^2\\) Pearsona. Wyboru reguły podziału dokonuje się poprzez testowanie niezależności zmiennej niezależnej z predyktorami. Reguła o największej wartości statystyki \\(\\chi^2\\) jest stosowana w pierwszej kolejności. Implementacja tego algorytmu znajduje się w pakiecie CHAID14 (funkcja do tworzenia drzewa o tej samej nazwie chaid) (Team 2015). Ctree15 - algorytm zbliżony zasadą dzialania do CHAID, ponieważ również wykorzystuje testowanie do wyboru reguły podziału. Różni się jednak tym, że może być stosowany do zmiennych dowolnego typu oraz tym, że może być zarowno drzewem klasyfikacyjnym jak i regresyjnym. Implementację R-ową można znaleźć w pakietach party (Hothorn, Hornik, and Zeileis 2006) lub partykit (Hothorn and Zeileis 2015) - funkcją do tworzenia modelu jest ctree. C4.5 - algorytm stworzony przez Quinlan (1993) w oparciu, o również jego autorstwa, algorytm ID3. Służy jedynie do zadań klasyfikacyjnych. W dużym uproszczeniu, dobór reguł podziału odbywa się na podstawie przyrostu informacji (patrz Reguły podziału). W przeciwieństwie do pierwotnego algorytmu ID3, C4.5 nie raczej nie przeucza drzew. Implementacja R-owa znajduje się w pakiecie RWeka (Hornik, Buchta, and Zeileis 2009) - funkcja do budowy drzewa to J48. C5.0 - kolejny algorytm autorstwa Kuhn and Quinlan (2018) jest usprawnieniem algorytmu C4.5, generującym mniejsze drzewa automatycznie przycinane na podstawie złożności drzewa. Służy jedynie do zadań klasyfikacyjnych. Jest szybszy od poprzednika i pozwala na zastosowanie metody boosting16. Implementacja R-owa znajduje się w pakiecie C50, a funkcja do budowy drzewa to C5.0. Przykład 4.2 W celu porównania wyników klasyfikacji na podstawie drzew decyzyjnych o różnych algorytmach, zostaną nauczone modele w oparciu o funkcje ctree, J48 i C5.0 dla tego samego zestawu danych co w przykładzie wcześniejszym 4.1. Drzewo ctree Na początek ustalamy parametry ograniczające rozrozt drzewa podobne jak w poprzednim przykładzie. library(partykit) tree2 &lt;- ctree(Species~., data = dt.train, control = ctree_control(minsplit = 10, minbucket = 5, maxdepth = 4)) tree2 ## ## Model formula: ## Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width ## ## Fitted party: ## [1] root ## | [2] Petal.Length &lt;= 1.9: setosa (n = 37, err = 0.0%) ## | [3] Petal.Length &gt; 1.9 ## | | [4] Petal.Width &lt;= 1.7 ## | | | [5] Petal.Length &lt;= 4.9: versicolor (n = 32, err = 3.1%) ## | | | [6] Petal.Length &gt; 4.9: virginica (n = 6, err = 33.3%) ## | | [7] Petal.Width &gt; 1.7: virginica (n = 30, err = 0.0%) ## ## Number of inner nodes: 3 ## Number of terminal nodes: 4 plot(tree2) Rysunek 4.5: Wykres drzewa decyzyjnego zbudowanego metodą ctree Wydaje się, że drzewo nie jest optymalne, ponieważ w węźle 6 obserwacje z grup versicolor i virginica są nieco pomieszane. Ostateczne oceny dokonujemy na podstawie próby testowej. pred2 &lt;- predict(tree2, newdata = dt.test) tab &lt;- table(predykcja = pred2, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 16 0 ## virginica 0 1 15 Dopiero ocena jakości klasyfikacji na podstawe próby testowej pokazuje, że model zbudowany za pomocą ctree daje podobną precyzję jak rpart przycięty. Drzewo J48 W tym przypadku model sam poszukuje optymalnego rozwiazania przycinając się automatycznie. library(RWeka) tree3 &lt;- J48(Species~., data = dt.train) tree3 ## J48 pruned tree ## ------------------ ## ## Petal.Width &lt;= 0.6: setosa (37.0) ## Petal.Width &gt; 0.6 ## | Petal.Width &lt;= 1.7 ## | | Petal.Length &lt;= 4.9: versicolor (32.0/1.0) ## | | Petal.Length &gt; 4.9 ## | | | Petal.Width &lt;= 1.5: virginica (3.0) ## | | | Petal.Width &gt; 1.5: versicolor (3.0/1.0) ## | Petal.Width &gt; 1.7: virginica (30.0) ## ## Number of Leaves : 5 ## ## Size of the tree : 9 plot(tree3) Rysunek 4.6: Wykres drzewa decyzyjnego zbudowanego metodą J48 Drzewo jest nieco bardziej rozbudowane niż tree2 i mod.rpart2. summary(tree3) ## ## === Summary === ## ## Correctly Classified Instances 103 98.0952 % ## Incorrectly Classified Instances 2 1.9048 % ## Kappa statistic 0.9714 ## Mean absolute error 0.0208 ## Root mean squared error 0.1019 ## Relative absolute error 4.6776 % ## Root relative squared error 21.628 % ## Total Number of Instances 105 ## ## === Confusion Matrix === ## ## a b c &lt;-- classified as ## 37 0 0 | a = setosa ## 0 33 0 | b = versicolor ## 0 2 33 | c = virginica Podsumowanie dopasowania drzewa na próbie uczącej jest bardzo dobre, bo poprawnych klasyfikacji jest ponad 98%. Oceny dopasowania i tak dokonujemy na zbiorze testowym. pred3 &lt;- predict(tree3, newdata = dt.test) tab &lt;- table(predykcja = pred3, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 16 0 ## virginica 0 1 15 Otrzymujemy identyczną macierz klasyfikacji jak w poprzednich przypadkach. Drzewo C50 Tym razem również nie trzeba ustawiać parametrów drzewa, ponieważ algorytm działa tak aby zapobiec rozrostowi drzewa przy jednoczesnej wysokiej poprawności klasyfikacji. library(C50) tree4 &lt;- C5.0(Species~., data = dt.train) summary(tree4) ## ## Call: ## C5.0.formula(formula = Species ~ ., data = dt.train) ## ## ## C5.0 [Release 2.07 GPL Edition] Tue Apr 02 19:02:06 2019 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 105 cases (5 attributes) from undefined.data ## ## Decision tree: ## ## Petal.Length &lt;= 1.9: setosa (37) ## Petal.Length &gt; 1.9: ## :...Petal.Width &gt; 1.7: virginica (30) ## Petal.Width &lt;= 1.7: ## :...Petal.Length &lt;= 4.9: versicolor (32/1) ## Petal.Length &gt; 4.9: virginica (6/2) ## ## ## Evaluation on training data (105 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 4 3( 2.9%) &lt;&lt; ## ## ## (a) (b) (c) &lt;-classified as ## ---- ---- ---- ## 37 (a): class setosa ## 31 2 (b): class versicolor ## 1 34 (c): class virginica ## ## ## Attribute usage: ## ## 100.00% Petal.Length ## 64.76% Petal.Width ## ## ## Time: 0.0 secs Otrzymujemy identyczne drzewo jak w przypadku zastosowania algorytmu ctree. plot(tree4) Rysunek 4.7: Wykres drzewa decyzyjnego zbudowanego metodą C5.0 Dla pewności przeprowadzimy sprawdzenie na zbiorze testowym. pred4 &lt;- predict(tree4, newdata = dt.test) tab &lt;- table(predykcja = pred4, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 16 0 ## virginica 0 1 15 Bibliografia "],
["pochodne-drzew-decyzyjnych.html", "5 Pochodne drzew decyzyjnych 5.1 Bagging 5.2 Lasy losowe 5.3 Boosting", " 5 Pochodne drzew decyzyjnych Przykład zastosowania drzew decyzyjnych na zbiorze iris w poprzednich przykładach może skłaniać do przypuszczenia, że drzewa decyzyjne zawsze dobrze radzą sobie z predykcją wartości wynikowej. Niestety w przykładach nieco bardziej skomplikowanych, gdzie chociażby klasy zmiennej wynikowej nie są tak wyraźnie separowalne, drzewa decyzyjne wypadają gorzej w porównaniu z innymi modelami nadzorowanego uczenia maszynowego. I tak u podstaw metod bazujących na prostych drzewach decyzyjnych stał pomysł, że skoro jedno drzewo nie ma wystarczających własności predykcyjnych, to może zastosowanie wielu drzew połączonych w pewien sposób poprawi je. Tak powstały metody bagging, random forest i boosting17. Należy zaznaczyć, że metody znajdują swoje zastosowanie również w innych modelach nadzorowanego uczenia maszynowego. 5.1 Bagging Technika ta została wprowadzona przez Breiman (1996) i ma na celu zmniejszenie wariancji modelu pojedynczego drzewa. Podobnie jak technika bootstrap, w której statystyki są wyliczane na wielu próbach pobranych z tego samego rozkładu (próby), w metodzie bagging losuje się wiele prób ze zbioru uczącego (najczęściej poprzez wielokrotne losowanie próby o rozmiarze zbioru uczącego ze zwracaniem), a następnie dla każdej próby bootstrapowej buduje się drzewo. W ten sposób otrzymujemy \\(B\\) drzew decyzyjnych \\(\\hat{f}^1(x), \\hat{f}^2(x),\\ldots, \\hat{f}^B(x)\\). Na koniec poprzez uśrednienie otrzymujemy model charakteryzujący się większą precyzją \\[\\begin{equation} \\hat{f}_{bag}(x)=\\frac1B\\sum_{b=1}^B\\hat{f}^b(x). \\end{equation}\\] Ponieważ podczas budowy drzew na podstawie prób bootstrapowych nie kontrolujemy złożoności, to w rezultacie każde z drzew może charakteryzować się dużą wariancją. Poprzez uśrednianie wyników pojedynczych drzew otrzymujemy mniejsze obciążenie ale również przy dostatecznie dużej liczbie prób (\\(B\\) często liczy się w setkach, czy tysiącach) zmniejszamy wariancję “średniej” predykcji z drzew. Oczywiście metodę tą trzeba dostosować do zadań klasyfikacyjnych, ponieważ nie istnieje średnia klasyfikacji z wielu drzew. W miejsce średniej stosuje się modę, czyli wartość dominującą. Przyjrzyjmy się jak maszyna losuje obserwacje ze zwracaniem n &lt;- NULL m &lt;- NULL for(i in 1:1000){ x &lt;- sample(1:500, size = 500, replace = T) y &lt;- setdiff(1:500, x) z &lt;- unique(x) n[i] &lt;- length(z) m[i] &lt;- length(y) } mean(n)/500*100 ## [1] 63.2574 mean(m)/500*100 ## [1] 36.7426 Faktycznie uczenie modelu metodą bagging odbywa się średnio na 2/3 obserwacji zbioru uczącego wylosowanych do prób bootstrapowych, a pozostała 1/3 (ang. out-of-bag) jest wykorzystana do oceny jakości predykcji. Niewątpliwą zaletą drzew decyzyjnych była ich łatwa interpretacja. W przypadku metody bagging jest ona znacznie utrudniona, ponieważ jej wynik składa się z agregacji wielu drzew. Można natomiast ocenić ważność predyktorów (ang. variable importance). I tak, przez obserwację spadku \\(RSS\\) dla baggingu regresyjnego przy zastosowaniu danego predyktora w podziałach drzewa i uśrednieniu wyniku otrzymamy wskaźnik ważności predyktora dużo lepszy niż dla pojedynczego drzewa. W przypadku baggingu klasyfikacyjnego w miejsce \\(RSS\\) stosujemy indeks Gini’ego. Implementacja R-owa metody bagging znajduje się w pakiecie ipred, a funkcja do budowy modelu nazywa się bagging (Peters and Hothorn 2018). Można również stosować funkcję randomForest pakietu randomForest (Liaw and Wiener 2002) - powody takiego działania wyjaśnią się w podrozdziale Lasy losowe. Przykład 5.1 Tym razem cel zadania jest regresyjny i polega na ustaleniu miary tendencji centralnej ceny mieszkań w Bostonie na podstawie zmiennych umieszczonych w zbiorze Boston pakietu MASS (Venables and Ripley 2002). Zmienną zależną będzie mediana cen mieszkań na przedmieściach Bostonu (medv). library(MASS) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 set.seed(2019) boston.train &lt;- Boston %&gt;% sample_frac(size = 2/3) boston.test &lt;- setdiff(Boston, boston.train) Aby móc porównać wyniki predykcji z metody bagging, najpierw zostanie zbudowane jedno drzewo decyzyjne w oparciu o algorytm CART. library(rpart) library(rpart.plot) boston.rpart &lt;- rpart(medv~., data = boston.train) x &lt;- summary(boston.rpart) ## Call: ## rpart(formula = medv ~ ., data = boston.train) ## n= 337 ## ## CP nsplit rel error xerror xstd ## 1 0.43506104 0 1.0000000 1.0037495 0.10496568 ## 2 0.21114710 1 0.5649390 0.6856438 0.07732133 ## 3 0.05641774 2 0.3537919 0.4393220 0.05974589 ## 4 0.04154842 3 0.2973741 0.3726563 0.05716622 ## 5 0.02707678 4 0.2558257 0.3520312 0.05569786 ## 6 0.01489117 5 0.2287489 0.3238915 0.05681943 ## 7 0.01202564 6 0.2138578 0.2922610 0.05311293 ## 8 0.01057622 7 0.2018321 0.2889364 0.05318206 ## 9 0.01031677 8 0.1912559 0.2838433 0.05152251 ## 10 0.01006729 9 0.1809391 0.2838187 0.05152098 ## 11 0.01000000 10 0.1708718 0.2815210 0.05152993 ## ## Variable importance ## lstat nox indus crim tax rm age dis ptratio ## 24 13 13 13 11 10 10 2 2 ## rad black ## 1 1 ## ## Node number 1: 337 observations, complexity param=0.435061 ## mean=22.61157, MSE=79.33004 ## left son=2 (186 obs) right son=3 (151 obs) ## Primary splits: ## lstat &lt; 10.02 to the right, improve=0.4350610, (0 missing) ## rm &lt; 6.8375 to the left, improve=0.4305766, (0 missing) ## indus &lt; 6.66 to the right, improve=0.2914821, (0 missing) ## ptratio &lt; 19.15 to the right, improve=0.2608119, (0 missing) ## nox &lt; 0.5125 to the right, improve=0.2169607, (0 missing) ## Surrogate splits: ## indus &lt; 7.625 to the right, agree=0.846, adj=0.656, (0 split) ## nox &lt; 0.519 to the right, agree=0.828, adj=0.616, (0 split) ## crim &lt; 0.12995 to the right, agree=0.786, adj=0.523, (0 split) ## age &lt; 63.9 to the right, agree=0.777, adj=0.503, (0 split) ## tax &lt; 377 to the right, agree=0.769, adj=0.483, (0 split) ## ## Node number 2: 186 observations, complexity param=0.05641774 ## mean=17.31828, MSE=19.86042 ## left son=4 (58 obs) right son=5 (128 obs) ## Primary splits: ## crim &lt; 5.84803 to the right, improve=0.4083024, (0 missing) ## dis &lt; 2.0754 to the left, improve=0.3684093, (0 missing) ## lstat &lt; 14.405 to the right, improve=0.3516672, (0 missing) ## nox &lt; 0.657 to the right, improve=0.3255969, (0 missing) ## age &lt; 84.9 to the right, improve=0.2247741, (0 missing) ## Surrogate splits: ## rad &lt; 16 to the right, agree=0.855, adj=0.534, (0 split) ## tax &lt; 551.5 to the right, agree=0.839, adj=0.483, (0 split) ## nox &lt; 0.657 to the right, agree=0.828, adj=0.448, (0 split) ## dis &lt; 2.0754 to the left, agree=0.801, adj=0.362, (0 split) ## lstat &lt; 19.055 to the right, agree=0.796, adj=0.345, (0 split) ## ## Node number 3: 151 observations, complexity param=0.2111471 ## mean=29.13179, MSE=75.5574 ## left son=6 (120 obs) right son=7 (31 obs) ## Primary splits: ## rm &lt; 7.127 to the left, improve=0.4947648, (0 missing) ## lstat &lt; 4.495 to the right, improve=0.4054324, (0 missing) ## nox &lt; 0.574 to the left, improve=0.1389706, (0 missing) ## ptratio &lt; 14.75 to the right, improve=0.1349232, (0 missing) ## age &lt; 89.45 to the left, improve=0.1133301, (0 missing) ## Surrogate splits: ## lstat &lt; 3.21 to the right, agree=0.841, adj=0.226, (0 split) ## ptratio &lt; 14.15 to the right, agree=0.828, adj=0.161, (0 split) ## tax &lt; 207 to the right, agree=0.808, adj=0.065, (0 split) ## nox &lt; 0.639 to the left, agree=0.801, adj=0.032, (0 split) ## ## Node number 4: 58 observations ## mean=13.08793, MSE=14.14485 ## ## Node number 5: 128 observations, complexity param=0.01489117 ## mean=19.23516, MSE=10.66681 ## left son=10 (61 obs) right son=11 (67 obs) ## Primary splits: ## lstat &lt; 14.405 to the right, improve=0.2915760, (0 missing) ## dis &lt; 1.99235 to the left, improve=0.2280873, (0 missing) ## age &lt; 84.15 to the right, improve=0.1950219, (0 missing) ## ptratio &lt; 20.95 to the right, improve=0.1349341, (0 missing) ## rm &lt; 5.706 to the left, improve=0.1194638, (0 missing) ## Surrogate splits: ## age &lt; 91.15 to the right, agree=0.758, adj=0.492, (0 split) ## dis &lt; 2.0418 to the left, agree=0.664, adj=0.295, (0 split) ## nox &lt; 0.607 to the right, agree=0.633, adj=0.230, (0 split) ## indus &lt; 18.84 to the right, agree=0.625, adj=0.213, (0 split) ## rm &lt; 5.703 to the left, agree=0.617, adj=0.197, (0 split) ## ## Node number 6: 120 observations, complexity param=0.04154842 ## mean=26.02417, MSE=34.39883 ## left son=12 (98 obs) right son=13 (22 obs) ## Primary splits: ## lstat &lt; 5.145 to the right, improve=0.2690898, (0 missing) ## dis &lt; 2.0891 to the right, improve=0.2163813, (0 missing) ## rm &lt; 6.543 to the left, improve=0.2036454, (0 missing) ## age &lt; 89.45 to the left, improve=0.1796977, (0 missing) ## tax &lt; 548 to the left, improve=0.1751322, (0 missing) ## Surrogate splits: ## zn &lt; 92.5 to the left, agree=0.833, adj=0.091, (0 split) ## nox &lt; 0.4035 to the right, agree=0.833, adj=0.091, (0 split) ## indus &lt; 1.495 to the right, agree=0.825, adj=0.045, (0 split) ## dis &lt; 1.48495 to the right, agree=0.825, adj=0.045, (0 split) ## ## Node number 7: 31 observations, complexity param=0.02707678 ## mean=41.16129, MSE=52.78882 ## left son=14 (11 obs) right son=15 (20 obs) ## Primary splits: ## rm &lt; 7.437 to the left, improve=0.4423448, (0 missing) ## lstat &lt; 5.185 to the right, improve=0.3125696, (0 missing) ## ptratio &lt; 15.05 to the right, improve=0.1896089, (0 missing) ## black &lt; 392.715 to the right, improve=0.1133472, (0 missing) ## age &lt; 37.6 to the right, improve=0.0737298, (0 missing) ## Surrogate splits: ## lstat &lt; 4.635 to the right, agree=0.774, adj=0.364, (0 split) ## indus &lt; 2.32 to the left, agree=0.742, adj=0.273, (0 split) ## dis &lt; 5.9736 to the right, agree=0.710, adj=0.182, (0 split) ## black &lt; 390.095 to the right, agree=0.710, adj=0.182, (0 split) ## crim &lt; 0.10593 to the left, agree=0.677, adj=0.091, (0 split) ## ## Node number 10: 61 observations ## mean=17.38689, MSE=8.122779 ## ## Node number 11: 67 observations ## mean=20.91791, MSE=7.041172 ## ## Node number 12: 98 observations, complexity param=0.01202564 ## mean=24.58265, MSE=20.9745 ## left son=24 (64 obs) right son=25 (34 obs) ## Primary splits: ## rm &lt; 6.543 to the left, improve=0.1564077, (0 missing) ## black &lt; 364.385 to the right, improve=0.1331323, (0 missing) ## age &lt; 89.45 to the left, improve=0.1241124, (0 missing) ## tax &lt; 223.5 to the right, improve=0.1204819, (0 missing) ## dis &lt; 4.46815 to the right, improve=0.1048755, (0 missing) ## Surrogate splits: ## dis &lt; 3.6589 to the right, agree=0.704, adj=0.147, (0 split) ## rad &lt; 6.5 to the left, agree=0.704, adj=0.147, (0 split) ## age &lt; 68.9 to the left, agree=0.694, adj=0.118, (0 split) ## indus &lt; 1.605 to the right, agree=0.673, adj=0.059, (0 split) ## nox &lt; 0.4045 to the right, agree=0.673, adj=0.059, (0 split) ## ## Node number 13: 22 observations, complexity param=0.01031677 ## mean=32.44545, MSE=43.70884 ## left son=26 (15 obs) right son=27 (7 obs) ## Primary splits: ## tax &lt; 364 to the left, improve=0.2868266, (0 missing) ## lstat &lt; 3.855 to the right, improve=0.2413545, (0 missing) ## age &lt; 31.85 to the left, improve=0.1598075, (0 missing) ## dis &lt; 5.4085 to the right, improve=0.1258591, (0 missing) ## black &lt; 381.59 to the right, improve=0.1052855, (0 missing) ## Surrogate splits: ## crim &lt; 2.6956 to the left, agree=0.773, adj=0.286, (0 split) ## indus &lt; 14 to the left, agree=0.773, adj=0.286, (0 split) ## nox &lt; 0.5875 to the left, agree=0.773, adj=0.286, (0 split) ## age &lt; 89.65 to the left, agree=0.773, adj=0.286, (0 split) ## dis &lt; 2.3371 to the right, agree=0.773, adj=0.286, (0 split) ## ## Node number 14: 11 observations ## mean=34.64545, MSE=3.304298 ## ## Node number 15: 20 observations, complexity param=0.01057622 ## mean=44.745, MSE=43.81147 ## left son=30 (12 obs) right son=31 (8 obs) ## Primary splits: ## ptratio &lt; 15.4 to the right, improve=0.3226860, (0 missing) ## rad &lt; 6 to the right, improve=0.2170243, (0 missing) ## tax &lt; 270 to the right, improve=0.1545997, (0 missing) ## age &lt; 71.85 to the right, improve=0.1331209, (0 missing) ## zn &lt; 10 to the left, improve=0.1328727, (0 missing) ## Surrogate splits: ## zn &lt; 10 to the left, agree=0.80, adj=0.500, (0 split) ## nox &lt; 0.541 to the left, agree=0.80, adj=0.500, (0 split) ## age &lt; 86.7 to the left, agree=0.80, adj=0.500, (0 split) ## dis &lt; 2.5813 to the right, agree=0.80, adj=0.500, (0 split) ## crim &lt; 0.45114 to the left, agree=0.75, adj=0.375, (0 split) ## ## Node number 24: 64 observations, complexity param=0.01006729 ## mean=23.2625, MSE=21.96891 ## left son=48 (57 obs) right son=49 (7 obs) ## Primary splits: ## indus &lt; 14.48 to the left, improve=0.19142190, (0 missing) ## crim &lt; 0.841845 to the left, improve=0.17407590, (0 missing) ## black &lt; 374.635 to the right, improve=0.14590640, (0 missing) ## dis &lt; 2.6499 to the right, improve=0.13374910, (0 missing) ## age &lt; 79.85 to the left, improve=0.08856433, (0 missing) ## Surrogate splits: ## crim &lt; 1.163695 to the left, agree=0.984, adj=0.857, (0 split) ## nox &lt; 0.589 to the left, agree=0.984, adj=0.857, (0 split) ## age &lt; 84.35 to the left, agree=0.984, adj=0.857, (0 split) ## dis &lt; 2.28545 to the right, agree=0.969, adj=0.714, (0 split) ## black &lt; 361.635 to the right, agree=0.969, adj=0.714, (0 split) ## ## Node number 25: 34 observations ## mean=27.06765, MSE=9.646894 ## ## Node number 26: 15 observations ## mean=30.02667, MSE=14.56062 ## ## Node number 27: 7 observations ## mean=37.62857, MSE=66.76776 ## ## Node number 30: 12 observations ## mean=41.675, MSE=48.28521 ## ## Node number 31: 8 observations ## mean=49.35, MSE=1.7575 ## ## Node number 48: 57 observations ## mean=22.54386, MSE=10.87053 ## ## Node number 49: 7 observations ## mean=29.11429, MSE=73.89265 rpart.plot(boston.rpart) Rysunek 5.1: Drzewo regresyjne pełne Przycinamy drzewo… printcp(boston.rpart) ## ## Regression tree: ## rpart(formula = medv ~ ., data = boston.train) ## ## Variables actually used in tree construction: ## [1] crim indus lstat ptratio rm tax ## ## Root node error: 26734/337 = 79.33 ## ## n= 337 ## ## CP nsplit rel error xerror xstd ## 1 0.435061 0 1.00000 1.00375 0.104966 ## 2 0.211147 1 0.56494 0.68564 0.077321 ## 3 0.056418 2 0.35379 0.43932 0.059746 ## 4 0.041548 3 0.29737 0.37266 0.057166 ## 5 0.027077 4 0.25583 0.35203 0.055698 ## 6 0.014891 5 0.22875 0.32389 0.056819 ## 7 0.012026 6 0.21386 0.29226 0.053113 ## 8 0.010576 7 0.20183 0.28894 0.053182 ## 9 0.010317 8 0.19126 0.28384 0.051523 ## 10 0.010067 9 0.18094 0.28382 0.051521 ## 11 0.010000 10 0.17087 0.28152 0.051530 plotcp(boston.rpart) boston.rpart2 &lt;- prune(boston.rpart, cp = 0.012026) rpart.plot(boston.rpart2) Rysunek 5.2: Drzewo regresyjne przycięte Predykcja na podstawie drzewa na zbiorze testowym. boston.pred &lt;- predict(boston.rpart2, newdata = boston.test) rmse &lt;- function(pred, obs) sqrt(1/length(pred)*sum((pred-obs)^2)) rmse(boston.pred, boston.test$medv) ## [1] 4.825862 Teraz zbudujemy model metodą bagging. library(randomForest) boston.bag &lt;- randomForest(medv~., data = boston.train, mtry = ncol(boston.train)-1) boston.bag ## ## Call: ## randomForest(formula = medv ~ ., data = boston.train, mtry = ncol(boston.train) - 1) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 13 ## ## Mean of squared residuals: 13.06701 ## % Var explained: 83.53 Predykcja na podstawie modelu boston.pred2 &lt;- predict(boston.bag, newdata = boston.test) rmse(boston.pred2, boston.test$medv) ## [1] 3.039308 Zatem predykcja na podstawie modelu bagging jest nico lepsza niż z pojedynczego drzewa. Dodatkowo możemy ocenić ważność zmiennych użytych w budowie drzew. varImpPlot(boston.bag) Rysunek 5.3: Wykres ważności predyktorów importance(boston.bag) ## IncNodePurity ## crim 1200.11828 ## zn 24.17836 ## indus 262.33396 ## chas 22.27133 ## nox 417.32236 ## rm 9102.58339 ## age 416.48170 ## dis 1494.79734 ## rad 171.92103 ## tax 403.66309 ## ptratio 411.88528 ## black 331.58495 ## lstat 12137.38999 x$variable.importance ## lstat nox indus crim tax rm ## 15197.8587 8683.8225 8325.2431 8074.7200 6991.0756 6768.5423 ## age dis ptratio rad black zn ## 6538.5039 1305.3786 1193.2073 853.4309 323.8576 242.3521 W porównaniu do ważności zmiennych dla pojedynczego drzewa widać pewne różnice. 5.2 Lasy losowe Lasy losowe są uogólnieniem metody bagging, polegającą na losowaniu dla każdego drzewa wchodzącego w skład lasu \\(m\\) predyktorów spośród \\(p\\) dostępnych, a następnie budowaniu drzew z wykorzystaniem tylko tych predyktorów (Ho 1995). Dzięki temu za każdy razem drzewo jest budowane w oparciu o nowy zestaw cech (najczęściej przyjmujemy \\(m=\\sqrt{p}\\)). W przypadku modeli bagging za każdym razem najsilniejszy predyktor wchodził w skład zbioru uczącego, a co za tym idzie również uczestniczył w tworzeniu reguł podziału. Wówczas wiele drzew zawierało reguły stosujące dany atrybut, a wtedy predykcje otrzymywane za pomocą drzew były skorelowane. Dlatego nawet duża liczba prób bootstrapowych nie zapewniała poprawy precyzji. Implementacja tej metody znajduje się w pakiecie randomForest. Przykład 5.2 Kontynuując poprzedni przykład 5.1 możemy zbudować las losowy aby przekonać się czy nastąpi poprawa predykcji zmiennej wynikowej. boston.rf &lt;- randomForest(medv~., data = boston.train) boston.rf ## ## Call: ## randomForest(formula = medv ~ ., data = boston.train) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 4 ## ## Mean of squared residuals: 13.09902 ## % Var explained: 83.49 Porównanie MSE na próbach uczących pomiędzy lasem losowym i modelem bagging wypada nieco na korzyść bagging. boston.pred3 &lt;- predict(boston.rf, newdata = boston.test) rmse(boston.pred3, boston.test$medv) ## [1] 3.418302 Ważność zmiennych również się nieco różni. varImpPlot(boston.rf) 5.3 Boosting Rozważania na temat metody boosting zaczęły się od pytań postawionych w publikacji Kearns and Valiant (1989), czy da się na podstawie na podstawie zbioru słabych modeli stworzyć jeden dobry? Odpowiedzi pozytywnej na nie udzielili, najpierw Schapire (1990), a potem Breiman (1998). W metodzie boosting nie stosuje się prób bootstrapowych ale odpowiednio modyfikuje się drzewo wyjściowe w kolejnych krokach na tym samym zbiorze uczącym. Algorytm dla drzewa regresyjnego jest następujący: Ustal \\(\\hat{f}(x)=0\\) i \\(r_i=y_i\\) dla każdego \\(i\\) w zbiorze uczącym. Dla \\(b=1,2,\\ldots, B\\) powtarzaj: naucz drzewo \\(\\hat{f}^b\\) o \\(d\\) regułach podziału (czyli \\(d+1\\) liściach) na zbiorze \\((X_i, r_i)\\), zaktualizuj drzewo do nowej “skurczonej” wersji \\[\\begin{equation} \\hat{f}(x)\\leftarrow \\hat{f}(x)+\\lambda\\hat{h}^b(x), \\end{equation}\\] zaktualizuj reszty \\[\\begin{equation} r_i\\leftarrow r_i-\\lambda\\hat{f}^b(x_i). \\end{equation}\\] Wyznacz boosted model \\[\\begin{equation} \\hat{f}(x) = \\sum_{b=1}^B\\lambda\\hat{f}^b(x) \\end{equation}\\] Uczenie drzew klasyfikacyjnego metoda boosting przebiega w podobny sposób. Wynik uczenia drzew metodą boosting zależy od trzech parametrów: Liczby drzew \\(B\\). W przeciwieństwie do metody bagging i lasów losowych, zbyt duże \\(B\\) może doprowadzić do przeuczenia modelu. \\(B\\) ustala się najczęściej na podstawie walidacji krzyżowej. Parametru “kurczenia” (ang. shrinkage) \\(\\lambda\\). Kontroluje on szybkość uczenia się kolejnych drzew. Typowe wartości \\(\\lambda\\) to 0.01 lub 0.001. Bardzo małe \\(\\lambda\\) może wymagać dobrania większego \\(B\\), aby zapewnić dobrą jakość predykcyjną modelu. Liczby podziałów w drzewach \\(d\\), która decyduje o złożoności drzewa. Bywa, że nawet \\(d=1\\) daje dobre rezultaty, ponieważ model wówczas uczy się powoli. Implementację metody boosting można znaleźć w pakiecie gbm (Greenwell et al. 2019) Przykład 5.3 Metodę boosting zastosujemy do zadania predykcji ceny mieszkań na przedmieściach Bostonu. Dobór parametrów modelu będzie arbitralny, więc niekoniecznie model będzie najlepiej dopasowany. library(gbm) boston.boost &lt;- gbm(medv~., data = boston.train, distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 2, shrinkage = 0.01) boston.boost ## gbm(formula = medv ~ ., distribution = &quot;gaussian&quot;, data = boston.train, ## n.trees = 5000, interaction.depth = 2, shrinkage = 0.01) ## A gradient boosted model with gaussian loss function. ## 5000 iterations were performed. ## There were 13 predictors of which 13 had non-zero influence. summary(boston.boost) ## var rel.inf ## lstat lstat 37.72235740 ## rm rm 28.25340805 ## dis dis 9.04378958 ## crim crim 6.95484787 ## nox nox 3.97210067 ## black black 3.16250916 ## ptratio ptratio 3.03202030 ## age age 2.35790500 ## chas chas 1.97366108 ## tax tax 1.67544858 ## indus indus 1.22537648 ## rad rad 0.57329299 ## zn zn 0.05328283 Predykcja na podstawie metody boosting boston.pred4 &lt;- predict(boston.boost, newdata = boston.test, n.trees = 5000) rmse(boston.pred4, boston.test$medv) ## [1] 3.06509 \\(RMSE\\) jest w tym przypadku mniejsze niż w lasach losowych ale nieco większe niż w metodzie bagging. Wszystkie metody wzmacnianych drzew dają wyniki lepsze niż pojedyncze drzewa. Bibliografia "],
["klasyfikatory-liniowe.html", "6 Klasyfikatory liniowe 6.1 Reprezentacja progowa 6.2 Reprezentacja logitowa 6.3 Wady klasyfikatorów liniowych", " 6 Klasyfikatory liniowe Obszerną rodzinę klasyfikatorów stanowią modele liniowe (ang. linear classification models). Klasyfikacji w tej rodzinie technik dokonuje się na podstawie modeli funkcji kombinacji liniowej predyktorów. Jest to ujęcie parametryczne, w którym klasyfikacji nowej wartości dokonujemy na podstawie atrybutów obserwacji i wektora parametrów. Uczenie na podstawie zestawu treningowego polega na oszacowaniu paramtrów modelu. W odróżnieniu od metod nieparametrycznych postać modelu tym razem jest znana. Każdy klasyfikator liniowy skład się z funkcji wewnętrzenj (ang. inner representation function) i funkcji zewnętrzenej (ang. outer representation function). Pierwsza jest funkcją rzeczywistą paramterów modelu i wartości atrybutów obserwacji \\[\\begin{equation} g(x) = F(\\mathbf{a}(x),\\mathbf{w})=\\sum_{i=0}^pw_ia_i(x)=\\mathbf{w}\\circ \\mathbf{a}(x), \\end{equation}\\] przyjmując, że \\(a_0(x)=1\\). Funkcja zewnętrzna przyporządkowuje binarnie klasy na podstawie wartości funkcji wewenętrznej. Istnieją dwa główne typy tych klasyfikacji: brzegowa - przyjmujemy, że funkcje wewnętrzne tworzą granice zbiorów obserwacji różnych klas, probabilistyczna - bazująca na tym, że funkcje wewnętrzne mogą pośrednio wkazywać prawdopodobieństwo przynależności do danej klasy. Pierwsza dzieli przestrzeń obserwacji za pomocą hiperpłaszczyzn na obszary jednorodne pod względem przynależności do klas. Druga jest próbą parametrycznej reprezentacji prawdopodobieństw przynależności do klas. Klasyfikacji na podstawie prawdopodobieństw można dokonać na różne sposoby, stosując: największe prawdopodobieństwo, funkcję najmniejszego kosztu błędnej klasyfikacji, krzywych ROC (ang. Receiver Operating Characteristic - o tym później). Podejście brzegowe lub probabilistyczne prowadzi najczęściej do dwóch typów reprezentacji funkcji zewnętrzenej: reprezentacji progowej (ang. threshold representation) - najczęściej przy podejściu brzegowym, reprezentacji logistycznej (ang. logit representation) - przy podejściu probabilistycznym. 6.1 Reprezentacja progowa W przypadku klasyfikacji dwustanowej, dziedzina jest dzielona na dwa regiony (pozytywny i negatywny) poprzez porównanie funkcji zewnętrzenej z wartością progową. Bez straty ogólności można sprawić, że będzie to wartość 0 \\[\\begin{equation} h(x)=H(g(x))= \\begin{cases} 1, &amp;\\text{ jeśli } g(x)\\geq 0\\\\ 0, &amp;\\text{ w przeciwnym przypadku.} \\end{cases} \\end{equation}\\] Czasami używa się parametryzacji \\(\\{-1,1\\}\\). Przez porównianie \\(g(x)\\) z 0 definiuje się hiperpłaszczyznę w \\(p\\) wymiarowej przestrzeni, która rozdziela dziedzinę na regiony pozytywne i negatywne. W tym ujęciu mówimy o liniowej separowalności obserwacji różnych klas, jeśli istnieje hiperpłaszczyzna je rozdzielająca. 6.2 Reprezentacja logitowa Najbardziej popularną reprezentacją parametryczną stosowaną w klasyfikacji jest reprezentacja logitowa \\[\\begin{equation} \\P(y=1|x)=\\frac{e^{g(x)}}{e^{g(x)}+1}. \\end{equation}\\] Wówczas \\(g(x)\\) nie reprezentuje bezpośrednio \\(\\P(y=1|x)\\) ale jego logit \\[\\begin{equation} g(x)=logit(\\P(y=1|x)), \\end{equation}\\] gdzie \\(logit(p)=\\ln\\frac{p}{1-p}\\). Dlatego właściwa postać reprezentacji jest następująca \\[\\begin{equation} \\P(y=1|x)=logit^{-1}(g(x)). \\end{equation}\\] W ten sposób reprezentacja logitowa jest równoważna reprezentacji progowej, ponieważ \\[\\begin{equation} g(x)=\\ln\\frac{\\P(y=1|x)}{1-\\P(y=1|x)}=\\ln\\frac{\\P(y=1|x)}{\\P(y=0|x)}&gt;0. \\end{equation}\\] Jednak zaletą reprezentacji logitowej, w porównaniu do progowej, jest to, że można wyznaczyć prawdopodobieństwa przynależności do obu klas. W przypadku klasyfikacji wielostanowej uczymy tyle funkcji \\(h\\) ile jest klas. 6.3 Wady klasyfikatorów liniowych tylko w przypadku prostych funkcji wewnętrznych jesteśmy w stanie ocenić wpływ poszczególnych predykorów na klasyfikację, jakość predykcji zależy od doboru funkcji wewnętrzenej (liniowa w ścisłym sensie jest najczęsciej niewystarczająca), nie jest w stanie klasyfikować poprawnie stanów (nie jest liniowo separowalna) w zagadnieniach typu XOR. "],
["regresja-logistyczna.html", "7 Regresja logistyczna 7.1 Model 7.2 Estymacja parametrów modelu 7.3 Interpretacja", " 7 Regresja logistyczna 7.1 Model Regresja logisticzna (ang. logistic regression) jest techniką z rodziny klasyfikatorów liniowych z reprezentacją logistyczną, a formalnie należy do rodziny ugogólnionych modeli liniowych (GLM). Stosowana jest wówczas, gdy zmienna wynikowa posiada dwa stany (sukces i porażka), kodowane najczęściej za pomocą 1 i 0. W tej metodzie modelowane jest warunkowe prawdopobieństwo sukcesu za pomocą kombinacji liniowej predyktorów \\(X\\). Ogólna postać modelu \\[\\begin{align} Y\\sim &amp;B(1, p)\\\\ p(X)=&amp;\\E(Y|X)=\\frac{\\exp(\\beta X)}{1+\\exp(\\beta X)}, \\end{align}\\] gdzie \\(B(1,p)\\) jest rozkładem dwumianowym o prawdopodobieństwie sukcesu \\(p\\), a \\(\\beta X\\) oznacza kombinację liniową parametrów modelu i wartości zmiennych niezależnych, przyjmując, że \\(x_0=1\\). Jako funkcji łączącej (czyli opisującej zwiazek między kombinacją liniową predyktorów i prawdopodobieństwem sukcesu) użyto logitu. Pozwala on na wygodną interpretację wyników w terminach szans. Szansą (ang. odds) nazywamy stosunek prawdopodobieństwa sukcesu do prawdopodobieństwa porażki \\[\\begin{equation} o = \\frac{p}{1-p}. \\end{equation}\\] Ponieważ będziemy przyjmowali, że \\(p\\in (0,1)\\), to \\(o\\in (0,\\infty)\\), a jej logartym należy do przedziału \\((-\\infty, \\infty)\\). Zatem logarytm szansy jest kombinacją liniową predyktorów \\[\\begin{equation} \\log\\left[\\frac{p(X)}{1-p(X)}\\right]=\\beta_0+\\beta_1x_1+\\ldots+\\beta_kx_k. \\end{equation}\\] 7.2 Estymacja parametrów modelu Estymacji parametrów modelu logistycznego dokonujemy za pomocą metody największej wiarogodności. Funkcja wiarogodności w tym przypadku przyjmuje postać \\[\\begin{equation} L(X_1,\\ldots,X_n,\\beta)=\\prod_{i=1}^{n}p(X_i)^Y_i[1-p(X_i)]^{1-Y_i}, \\end{equation}\\] gdzie wektor \\(\\beta\\) jest uwikłany w funkcji \\(p(X_i)\\). Maksymalizacji dokonujemy raczej po nałożeniu na funkcję wiarogodności logarytmu, bo to ułatwia szukanie ekstremum. \\[\\begin{equation} \\log L(X_1,\\ldots,X_n,\\beta) = \\sum_{i=1}^n(Y_i\\log p(X_i)+(1-Y_i)\\log(1-p(X_i))). \\end{equation}\\] 7.3 Interpretacja Interpretacja (lat. ceteris paribus - “inne takie samo”) poszczególnych parametrów modelu jest następująca: jeśli \\(b_i&gt;0\\) - to zmienna \\(x_i\\) ma wpływ stymulujący pojawienie się sukcesu, jeśli \\(b_i&lt;0\\) - to zmienna \\(x_i\\) ma wpływ ograniczający pojawienie się sukcesu, jeśli \\(b_i=0\\) - to zmienna \\(x_i\\) nie ma wpływu na pojawienie się sukcesu. Iloraz szans (ang. odds ratio) stosuje się w przypadku porównywania dwóch klas obserwacji. Jest on jak sama nazwa wskazuje ilorazem szans zajścia sukcesu w obu klasach \\[\\begin{equation} OR = \\frac{p_1}{1-p_1}\\frac{1-p_2}{p_2}, \\end{equation}\\] gdzie \\(p_i\\) oznacza zajście sukcesu w \\(i\\)-tej klasie. Interpretujemy go nastepująco: jeśli \\(OR&gt;1\\) - to w pierwszej grupie zajście sukcesu jest bardziej prawdopodobne, jeśli \\(OR&lt;1\\) - to w drugiej grupie zajście sukcesu jest bardziej prawdopodobne, jeśli \\(OR=1\\) - to w obu grupach zajście sukcesu jest jednakowo prawdopodobne. Przykład 7.1 Jako ilustrację działania regresji logistycznej użyjemy modelu dla danych ze zbioru Default pakietu ISLR. library(ISLR) head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 Zmienną zależną jest default, a pozostałe są predyktorami. najpierw dokonamy podziału próby na ucząca i testową, a następnie zbudujemy model. set.seed(2019) ind &lt;- sample(1:nrow(Default), size = 2/3*nrow(Default)) dt.ucz &lt;- Default[ind,] dt.test &lt;- Default[-ind,] mod.logit &lt;- glm(default~., dt.ucz, family = binomial(&quot;logit&quot;)) summary(mod.logit) ## ## Call: ## glm(formula = default ~ ., family = binomial(&quot;logit&quot;), data = dt.ucz) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4481 -0.1470 -0.0597 -0.0226 3.6966 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.085e+01 5.896e-01 -18.409 &lt;2e-16 *** ## studentYes -4.970e-01 2.851e-01 -1.744 0.0812 . ## balance 5.604e-03 2.809e-04 19.949 &lt;2e-16 *** ## income 7.933e-06 9.652e-06 0.822 0.4112 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1906.5 on 6665 degrees of freedom ## Residual deviance: 1059.8 on 6662 degrees of freedom ## AIC: 1067.8 ## ## Number of Fisher Scoring iterations: 8 Tylko income nie ma żadnego wpływu na prawdopodobieństwo stanu Yes zmiennej default. Zmienna balance wpływa stymulująco na prawdopodobieństwo pojawienia się sukcesu. Natomiast jeśli badana osoba jest studentem (studentYes), to ma wpływ ograniczający na pojawienie się sukcesu. Chcąc porównać dwie grupy obserwacji, przykładowo studentów z niestudentami, możemy wykorzystać iloraz szans. exp(cbind(OR = coef(mod.logit), confint(mod.logit))) %&gt;% kable(digits = 4) OR 2.5 % 97.5 % (Intercept) 0.0000 0.0000 0.0001 studentYes 0.6083 0.3485 1.0668 balance 1.0056 1.0051 1.0062 income 1.0000 1.0000 1.0000 Z powyższej tabeli wynika, że bycie studentem zmniejsza szanse na Yes w zmiennej default o około 40% (w stosunku do niestudentów). Natomiast wzrost zmiennej balance przy zachowaniu pozostałych zmiennych na tym samym poziomie skutkuje wzrostem szans na Yes o około 0.6%. Chcąc przeprowadzić predykcję na podstwie modelu dla ustalonych wartości cech (np. student = Yes, balance = $1000 i income = $40000) postępujemy następująco dt.new &lt;- data.frame(student = &quot;Yes&quot;, balance = 1000, income = 40000) predict(mod.logit, newdata = dt.new, type = &quot;response&quot;) ## 1 ## 0.004367692 Otrzymany wynik jest oszacowanym prawdopodobieństwem warunkowym wystąpienia sukcesu (default = Yes). Widać zatem, że poziomy badanych cech sprzyjają raczej porażce. Jeśli chcemy sprawdzić jakość klasyfikacji na zbiorze testowym, to musimy ustalić na jakim poziomie prawdopodobieństwa będziemy uznawać obserwację za sukces. W zależności od tego, na predykcji jakiego stanu zależy nam bardziej, możemy różnie dobierać ten próg (bez żadnych dodatkowych przesłanek najczęściej jest to 0.5). pred &lt;- predict(mod.logit, newdata = dt.test, type = &quot;response&quot;) pred.class &lt;- ifelse(pred &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) (tab &lt;- table(pred.class, dt.test$default)) ## ## pred.class No Yes ## No 3204 76 ## Yes 13 41 (acc &lt;- sum(diag(prop.table(tab)))) ## [1] 0.9733053 Klasyfikacja na poziomie 97% wskazuje na dobre dopasowanie modelu. "],
["LDA.html", "8 Analiza dyskryminacyjna 8.1 Liniowa analiza dyskryminacyjna Fisher’a 8.2 Liniowa analiza dyskryminacyjna - podejście probabilistyczne 8.3 Analiza dyskryminacyjna częściowych najmniejszych kwadratów 8.4 Regularyzowana analiza dyskryminacyjna 8.5 Analiza dyskryminacyjna mieszana 8.6 Elastyczna analiza dyskryminacyjna", " 8 Analiza dyskryminacyjna Analiza dyskryminacyjna (ang. discriminant analysis) jest grupą technik dyskryminacji obserwacji względem przynależności do klas. Część z nich należy do klasyfikatorów liniowych (choć nie zawsze w ścisłym sensie). Za autorów tej metody uważa się Fisher’a (1936) i Welch’a (1939). Kazdy z nich prezentował nieco inne podejscie do tematu klasyfikacji. Welch poszukiwał klasyfikacji minimalizującej prawdopodobieństwo błędnej klasyfikacji, znane jako klasyfikatory bayesowskie. Podejście Fisher’a skupiało się raczej na porównaniu zmienności miedzygrupowej do zmienności wewnątrzgrupowej. Wychodząc z założenia, że iloraz tych wariancji powinien być stosunkowo duży przy różnych klasach, jeśli do ich opisu użyjemy odpowiednich zmiennych niezależnych. W istocie chodzi o znalezienie takiego wektora, w kierunku którego wspomniany iloraz wariancji jest największy. 8.1 Liniowa analiza dyskryminacyjna Fisher’a 8.1.1 Dwie kategorie zmiennej grupującej Niech \\(\\boldsymbol D\\) będzie zbiorem zawierającym \\(n\\) punktów \\(\\{\\boldsymbol x_i, y_i\\}\\), gdzie \\(\\boldsymbol x_i\\in \\mathbb{R}^d\\), a \\(y_i\\in \\{c_1,\\ldots,c_k\\}\\). Niech \\(\\boldsymbol D_i\\) oznacza podzbiór punktów zbioru \\(\\boldsymbol D\\), które należą do klasy \\(c_i\\), czyli \\(\\boldsymbol D_i=\\{\\boldsymbol x_i|y_i=c_i\\}\\) i niech \\(|\\boldsymbol D_i|=n_i\\). Na początek załóżmy, że \\(\\boldsymbol D\\) składa się tylko z \\(\\boldsymbol D_1\\) i \\(\\boldsymbol D_2\\). Niech \\(\\boldsymbol w\\) będzie wektorem jednostkowym (\\(\\boldsymbol w&#39;\\boldsymbol w=1\\)), wówczas rzut ortogonalny punku \\(\\boldsymbol x_i\\) na wektor \\(\\boldsymbol w\\) można zapisać następująco \\[\\begin{equation} \\tilde{\\boldsymbol x}_i=\\left(\\frac{\\boldsymbol w&#39;\\boldsymbol x_i}{\\boldsymbol w&#39;\\boldsymbol w}\\right)\\boldsymbol w=(\\boldsymbol w&#39;\\boldsymbol x_i)\\boldsymbol w = a_i\\boldsymbol w, \\end{equation}\\] gdzie \\(a_i\\) jest współrzędną punktu \\(\\tilde{\\boldsymbol x}_i\\) w kierunku wektora \\(\\boldsymbol w\\), czyli \\[\\begin{equation} a_i=\\boldsymbol w&#39;\\boldsymbol x_i. \\end{equation}\\] Zatem \\((a_1,\\ldots,a_n)\\) reprezentują odwzorowanie \\(\\mathbb{R}^d\\) w \\(\\mathbb{R}\\), czyli z \\(d\\)-wymiarowej przestrzeni w przestrzeń generowaną przez \\(\\boldsymbol w\\). Rysunek 8.1: Rzut ortogonalny punktów w kierunku wektora \\(\\boldsymbol w\\) Każdy punkt należy do pewnej klasy, dlatego możemy wyliczyć \\[\\begin{align} m_1=&amp;\\frac{1}{n_1}\\sum_{ \\boldsymbol x_i\\in \\boldsymbol D_1}a_i=\\\\ =&amp;\\frac{1}{n_1}\\sum_{ \\boldsymbol x_i\\in \\boldsymbol D_1} \\boldsymbol w&#39; \\boldsymbol x_i=\\\\ =&amp; \\boldsymbol w&#39;\\left(\\frac{1}{n_1}\\sum_{ \\boldsymbol x_i\\in \\boldsymbol D_1} \\boldsymbol x_i \\right)=\\\\ =&amp; \\boldsymbol w&#39; \\boldsymbol{\\mu}_1, \\tag{8.1} \\end{align}\\] gdzie \\(\\boldsymbol \\mu_1\\) jest wektorem średnich punktów z \\(\\boldsymbol D_1\\). W podobny sposób można policzyć \\(m_2 = \\boldsymbol w&#39; \\boldsymbol \\mu_2\\). Oznacza to, że średnia projekcji jest projekcją średnich. Rozsądnym wydaje się teraz poszukać takiego wektora, aby \\(|m_1-m_2|\\) była maksymalnie duża przy zachowaniu niezbyt dużej zmienności wewnątrz grup. Dlatego kryterium Fisher’a przyjmuje postać \\[\\begin{equation} \\max_{ \\boldsymbol w}J(\\boldsymbol w)=\\frac{(m_1-M_2)^2}{ss_1^2+ss_2^2}, \\tag{8.2} \\end{equation}\\] gdzie \\(ss_j^2=\\sum_{ \\boldsymbol x_i\\in \\boldsymbol D_j}(a_i-m_j)^2=n_j\\sigma_j^2.\\) Zauważmy, że licznik w (8.2) da się zapisać jako \\[\\begin{align} (m_1-m_2)^2=&amp; ( \\boldsymbol w&#39;( \\boldsymbol \\mu_1- \\boldsymbol \\mu_2))^2=\\\\ =&amp; \\boldsymbol w&#39;((\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)&#39;) \\boldsymbol w=\\\\ =&amp; \\boldsymbol w&#39; \\boldsymbol B \\boldsymbol w \\end{align}\\] gdzie \\(\\boldsymbol B=(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)&#39;\\) jest macierzą \\(d\\times d\\). Ponadto \\[\\begin{align} ss_j^2=&amp;\\sum_{ \\boldsymbol x_i\\in \\boldsymbol D_j}(a_i-m_j)^2=\\\\ =&amp;\\sum_{ \\boldsymbol x_i\\in \\boldsymbol D_j}( \\boldsymbol w&#39; \\boldsymbol x_i- \\boldsymbol w&#39; \\boldsymbol\\mu_j)^2=\\\\ =&amp; \\sum_{ \\boldsymbol x_i\\in \\boldsymbol D_j}( \\boldsymbol{w}&#39;( \\boldsymbol{x}_i- \\boldsymbol{\\mu}_j))^2=\\\\ =&amp; \\boldsymbol{w}&#39;\\left(\\sum_{ \\boldsymbol x_i\\in \\boldsymbol D_j}(\\boldsymbol{x}_i-\\boldsymbol \\mu_j)(\\boldsymbol x_i- \\boldsymbol \\mu_j)&#39;\\right) \\boldsymbol{w}=\\\\ =&amp; \\boldsymbol{w}&#39; \\boldsymbol{S}_j \\boldsymbol{w}, \\tag{8.3} \\end{align}\\] gdzie \\(\\boldsymbol{S}_j=n_j \\boldsymbol{\\Sigma}_j\\). Zatem mianowinik (8.2) możemy zapisać jako \\[\\begin{equation} ss_1^2+ss_2^2= \\boldsymbol{w}&#39;(\\boldsymbol{S}_1+ \\boldsymbol{S}_2) \\boldsymbol{w}= \\boldsymbol{w}&#39; \\boldsymbol{S} \\boldsymbol{w}, \\end{equation}\\] gdzie \\(\\boldsymbol{S}=\\boldsymbol{S}_1+\\boldsymbol{S}_2\\). Ostatecznie warunek Fisher’a przyjmuje postać \\[\\begin{equation} \\max_{ \\boldsymbol{w}}J( \\boldsymbol{w})=\\frac{ \\boldsymbol{w}&#39; \\boldsymbol{B} \\boldsymbol{w}}{ \\boldsymbol{w}&#39; \\boldsymbol{S} \\boldsymbol{w}}. \\tag{8.4} \\end{equation}\\] Różniczkując (8.4) po \\(\\boldsymbol{w}\\) otrzymamy warunek \\[\\begin{equation} \\boldsymbol{B} \\boldsymbol{w} = \\lambda \\boldsymbol{S} \\boldsymbol{w}, \\tag{8.5} \\end{equation}\\] gdzie \\(\\lambda=J(\\boldsymbol{w})\\). Maksimum (8.5) jest osiągane dla wektora \\(\\boldsymbol{w}\\) równego wektrowi własnemu odpowiadającemu największej wartości własnej równania charkterystycznego \\(|\\boldsymbol{B}-\\lambda\\boldsymbol{S}|=0\\). Jeśli \\(\\boldsymbol{S}\\) nie jest osobliwa, to rozwiązanie (8.5) otrzymujemy przez znalezienie największej wartości własnej macierzy \\(\\boldsymbol{B}\\boldsymbol{S}^{-1}\\) lub bez wykorzystania wartości i wektorów własnych. Ponieważ \\(\\boldsymbol{B}=\\left((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\right)\\boldsymbol{w}\\) jest macierzą \\(d \\times d\\) rzędu 1, to \\(\\boldsymbol{B}\\boldsymbol{w}\\) jest punktem na kierunku wyznaczonym przez wektor \\(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2\\), bo \\[\\begin{align} \\boldsymbol{B}\\boldsymbol{w}=&amp; \\left((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\right)\\boldsymbol{w}=\\\\ =&amp;(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)\\left((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\right)\\boldsymbol{w}=\\\\ =&amp; b(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2), \\end{align}\\] gdzie \\(b = (\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\boldsymbol{w}\\) jest skalarem. Wówczas (8.5) zapiszemy jako \\[\\begin{gather} b(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2) = \\lambda\\boldsymbol{S}\\boldsymbol{w}\\\\ \\boldsymbol{w}= \\frac{b}{\\lambda}\\boldsymbol{S}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2) \\end{gather}\\] A ponieważ \\(b/\\lambda\\) jest liczbą, to kierunek najlepszej dyskryminacji grup wyznacza wektor \\[\\begin{equation} \\boldsymbol{w}=\\boldsymbol{S}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2). \\end{equation}\\] Rysunek 8.2: Rzut ortogonalny w kierunku wektora \\(\\boldsymbol{w}\\), będącego najlepiej dyskryminującym obie grupy obserwacji 8.1.2 \\(k\\)-kategorii zmiennej grupującej Uogólnieniem tej teorii na przypadek \\(k\\) klas otrzymujemy przez uwzględnienie \\(k-1\\) funkcji dyskryminacyjnych. Zmienność wewnątrzgrupowa przyjmuje wówczas postać \\[\\begin{equation} \\boldsymbol{S}_W=\\sum_{i=1}^k\\boldsymbol{S}_i, \\end{equation}\\] gdzie \\(\\boldsymbol{S}_i\\) jest zdefiniowane jak w (8.3). Niech średnia i rozrzut globalny będą dane wzorami \\[\\begin{equation} \\boldsymbol{m}=\\frac{1}{n}\\sum_{i=1}^kn_i\\boldsymbol{m}_i, \\end{equation}\\] \\[\\begin{equation} \\boldsymbol{S}_T=\\sum_{j=1}^k\\sum_{\\boldsymbol{x}\\in D_j}(\\boldsymbol{x}-\\boldsymbol{m})(\\boldsymbol{x}-\\boldsymbol{m})&#39; \\end{equation}\\] gdzie \\(\\boldsymbol{m}_i\\) jest określone jak w (8.1). Wtedy zmienność międzygrupową możemy wyrazić jako \\[\\begin{equation} \\boldsymbol{S}_B=\\sum_{i=1}^kn_i(\\boldsymbol{m}_i-\\boldsymbol{m})(\\boldsymbol{m}_i-\\boldsymbol{m})&#39;, \\end{equation}\\] bo \\(\\boldsymbol{S}_T=\\boldsymbol{S}_W+\\boldsymbol{S}_B.\\) Określamy projekcję \\(d\\)-wymiarowej przestrzeni na \\(k-1\\)-wymiarową przestrzeń za pomocą \\(k-1\\) funkcji dyskryminacyjnych postaci \\[\\begin{equation} \\boldsymbol{a}_j=\\boldsymbol{w}_j&#39;\\boldsymbol{x}, \\quad j=1,\\ldots,k-1. \\end{equation}\\] Połączone wszystkie \\(k-1\\) rzutów możemy zapisać jako \\[\\begin{equation} \\boldsymbol{a}=\\boldsymbol{W}&#39;\\boldsymbol{x}. \\end{equation}\\] W nowej przestrzeni \\(k-1\\)-wymiarowej możemy zdefiniować \\[\\begin{equation} \\tilde{\\boldsymbol{m}}=\\frac{1}{n}\\sum_{i=1}^kn_i\\tilde{\\boldsymbol{m}}_i, \\end{equation}\\] gdzie \\(\\tilde{\\boldsymbol{m}}_i= \\frac{1}{n_i}\\sum_{\\boldsymbol{a}\\in A_i}\\boldsymbol{a}\\), a \\(A_i\\) jest projekcją obiektów z \\(i\\)-tej klasy w kierunku wektora \\(\\boldsymbol{W}\\). Dalej możemy zdefiniować zmienności miedzy- i wewnątrzgrupowe dla obiektów przekształconych przez \\(\\boldsymbol{W}\\) \\[\\begin{align} \\tilde{\\boldsymbol{S}}_W=&amp;\\sum_{i=1}^k\\sum_{\\boldsymbol{a}\\in A_i}(\\boldsymbol{a}-\\tilde{\\boldsymbol{m}})(\\boldsymbol{a}-\\tilde{\\boldsymbol{m}})&#39;\\\\ \\tilde{\\boldsymbol{S}}_B=&amp;\\sum_{i=1}^kn_i(\\tilde{\\boldsymbol{m}}_i-\\tilde{\\boldsymbol{m}})(\\tilde{\\boldsymbol{m}}_i-\\tilde{\\boldsymbol{m}})&#39;. \\end{align}\\] Łatwo można zatem pokazać, że \\[\\begin{align} \\tilde{\\boldsymbol{S}}_W = &amp; \\boldsymbol{W}&#39;\\boldsymbol{S}_W\\boldsymbol{W}\\\\ \\tilde{\\boldsymbol{S}}_B = &amp; \\boldsymbol{W}&#39;\\boldsymbol{S}_B\\boldsymbol{W}. \\end{align}\\] Ostatecznie warunek (8.2) w \\(k\\)-wymiarowym ujęciu można przedstawić jako \\[\\begin{equation} \\max_{\\boldsymbol{W}}J(\\boldsymbol{W})=\\frac{\\tilde{\\boldsymbol{S}}_W}{\\tilde{\\boldsymbol{S}}_B}=\\frac{\\boldsymbol{W}&#39;\\boldsymbol{S}_W\\boldsymbol{W}}{\\boldsymbol{W}&#39;\\boldsymbol{S}_B\\boldsymbol{W}}. \\end{equation}\\] Maksimum można znaleźć poprzez rozwiązanie równania charakterystycznego \\[\\begin{equation} |\\boldsymbol{S}_B-\\lambda_i\\boldsymbol{S}_W|=0 \\end{equation}\\] dla każdego \\(i\\). Przykład 8.1 Dla danych ze zbioru iris przeprowadzimy analizę dyskryminacji. Implementację metody LDA znajdziemy w pakiecie MASS w postaci funkcji lda. Zaczynamy od standaryzacji zmiennych i podziału próby na uczącą i testową. library(MASS) library(tidyverse) iris.std &lt;- iris %&gt;% mutate_if(is.numeric, scale) set.seed(2019) ind &lt;- sample(nrow(iris.std), size = 100) dt.ucz &lt;- iris.std[ind,] dt.test &lt;- iris.std[-ind,] Budowa modelu mod.lda &lt;- lda(Species~., data = dt.ucz) mod.lda$prior ## setosa versicolor virginica ## 0.36 0.31 0.33 Prawdopodobieństwa a priori przynależności do klas przyjęto na podstawie próby uczącej. mod.lda$means ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa -1.0251463 0.8690229 -1.294839 -1.2527443 ## versicolor 0.1619267 -0.5015842 0.316167 0.1786195 ## virginica 0.9174351 -0.2636338 1.046883 1.0504160 W części means wyświetlone są średnie poszczególnych zmiennych niezależnych w podziale na grupy. Dzięki temu można określić położenia środków ciężkości poszczególnych klas w oryginalnej przestrzeni. mod.lda$scaling ## LD1 LD2 ## Sepal.Length 1.0073378 0.211252 ## Sepal.Width 0.4701094 -1.053135 ## Petal.Length -4.0746585 1.488372 ## Petal.Width -2.5146178 -2.312201 Powyższa tabela zawiera współrzędne wektrów wyznaczających funkcje dyskryminacyjne. Na ich podstawie możemy określić, która z nich wpływa najmocniej na tworzenie się nowej przestrzeni. Obiekt svd przechowuje pierwiastki z \\(\\lambda_i\\), dlatego podnosząc je do kwadratu i dzieląc przez ich sumę otrzymamy udział poszczególnych zmiennych w dyskryminacji przypadków. Jak widać pierwsza funkcja dyskryminacyjna w zupełności by wystarczyła. mod.lda$svd^2/sum(mod.lda$svd^2) ## [1] 0.994875091 0.005124909 Klasyfikacja na podstawie modelu pred.lda &lt;- predict(mod.lda, dt.test) Wynik predykcji przechowuje trzy rodzaje obiektów: klasy, które przypisał obiektom model (class); prawdopodobieństwa a posteriori przynależności do klas na podstawie modelu (posterior); współrzędne w nowej przestrzeni LD1, LD2 (x). Sprawdzenie jakości klasyfikacji tab &lt;- table(pred = pred.lda$class, obs = dt.test$Species) tab ## obs ## pred setosa versicolor virginica ## setosa 14 0 0 ## versicolor 0 18 0 ## virginica 0 1 17 sum(diag(prop.table(tab))) ## [1] 0.98 Jak widać z powyższej tabeli model dobrze sobie radzi z klasyfikacją obiektów. Odsetek poprawnych klasyfikacji wynosi 98%. cbind.data.frame(obs = dt.test$Species, pred.lda$x, pred = pred.lda$class) %&gt;% ggplot(aes(x = LD1, y = LD2))+ geom_point(aes(color = pred, shape = obs)) Rysunek 8.3: Klasyfikacja w przestrzeni LD1, LD2 na podstawie modelu mod.lda 8.2 Liniowa analiza dyskryminacyjna - podejście probabilistyczne Jak wspomniano na wstępie (patrz rozdział 8), podejście prezentowane przez Welcha polegało na minimalizacji prawdopodobieństwa popełnienia błędu przy klasyfikacji. Cała rodzina klasyfikatorów Bayesa (patrz rozdział 9) polega na wyznaczeniu prawdopodobieństw a posteriori, na podstawie których dokonuje się decyzji o klasyfikacji obiektów. Tym razem dodajemy również założenie, że zmienne niezależne \\(\\boldsymbol{x}=(\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_d)\\) charakteryzują się wielowymiarowym rozkładem normalnym \\[\\begin{equation} f(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})&#39;\\boldsymbol{\\Sigma}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right], \\tag{8.6} \\end{equation}\\] gdzie \\(\\boldsymbol{\\mu}\\) jest wektorem średnich \\(\\boldsymbol{x}\\), a \\(\\boldsymbol{\\Sigma}\\) jest macierzą kowaniancji \\(\\boldsymbol{x}\\). Uwaga. Liniowa kombinacja zmiennych losowych o normalnym rozkładzie łącznym ma również rozkład łączny normalny. W szczególności, jeśli \\(A\\) jest macierzą wymiaru \\(d\\times k\\) i \\(\\boldsymbol{y} = A&#39;\\boldsymbol{x}\\), to \\(f(\\boldsymbol{y})\\sim N(A&#39;\\boldsymbol{\\mu}, A&#39;\\boldsymbol{\\Sigma}A)\\). Odpowiednia forma macierzy przekształcenia \\(A_w\\), sprawia, że zmienne po transformacji charakteryzują się rozkładem normalnym łącznym o wariancji określonej przez \\(I\\). Jeśli \\(\\boldsymbol{\\Phi}\\) jest macierzą, której kolumny są ortonormalnymi wektorami własnymi macierzy \\(\\boldsymbol{\\Sigma}\\), a \\(\\boldsymbol{\\Lambda}\\) macierzą diagonalną wartości własnych, to transformacja \\(A_w=\\boldsymbol{\\Phi}\\boldsymbol{\\Lambda}^{-1}\\) przekształca \\(\\boldsymbol{x}\\) w \\(\\boldsymbol{y}\\sim N(A_w&#39;\\boldsymbol{\\mu}, I)\\). Rysunek 8.4: Transformacje rozkładu normalnego łącznego. Źródło: Duda, Hart, and Stork (2001) Definicja 8.1 Niech \\(g_i(\\boldsymbol{x}),\\ i=1,\\ldots,k\\) będzie pewną funkcją dyskryminacyjną, wówczas obiekt \\(\\boldsymbol{x}\\) nalezy zaklasyfikować do grupy \\(c_i\\) jeśli spełniony jest warunek \\[\\begin{equation} g_i(\\boldsymbol{x})&gt;g_j(\\boldsymbol{x}), \\quad j\\neq i. \\end{equation}\\] W podejściu polegającym na minimalizacji prawdopodobieństwa błędnej klasyfikacji, przyjmuje się najczęściej, że \\[\\begin{equation} g_i(\\boldsymbol{x})=\\P(c_i|\\boldsymbol{x}), \\end{equation}\\] czyli jako prawdopodobieństwo a posteriori. Wszystkie trzy poniższe postaci funkcji dyskrymincyjnych są dopuszczalne i równoważne ze względu na rezutlat grupowania \\[\\begin{align} g_i(\\boldsymbol{x})=&amp;\\P(c_i|\\boldsymbol{x})=\\frac{\\P(\\boldsymbol{x}|c_i)\\P(c_i)}{\\sum_{i=1}^k\\P(\\boldsymbol{x}|c_i)\\P(c_i)},\\\\ g_i(\\boldsymbol{x})=&amp;\\P(\\boldsymbol{x}|c_i)\\P(c_i),\\\\ g_i(\\boldsymbol{x})=&amp;\\log\\P(\\boldsymbol{x}|c_i)+\\log\\P(c_i) \\tag{8.7} \\end{align}\\] W przypadku gdy \\(\\boldsymbol{x}|c_i\\sim N(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)\\), to na podstawie (8.6) \\(g_i\\) danej jako (8.7) przyjmuje postać \\[\\begin{equation} g_i(\\boldsymbol{x})=-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_i)&#39;\\boldsymbol{\\Sigma}_i^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_i)-\\frac{d}{2}\\log(2\\pi)-\\frac{1}{2}\\log|\\boldsymbol{\\Sigma}_i|+\\log\\P(c_i). \\end{equation}\\] W kolejnych podrozdziałach preanalizujemy trzy możliwe formy macierzy kowariancji. 8.2.1 Przypadek gdy \\(\\boldsymbol{\\Sigma}_i=I\\) To najprostszy przypadek, zakładający niezależność zmiennych wchodzących w skład \\(\\boldsymbol x\\), których wariancje są stałe \\(\\sigma^2\\). Wówczas \\(g_i\\) przyjmuje postać \\[\\begin{equation} g_i(\\boldsymbol x)=-\\frac{||\\boldsymbol x-\\boldsymbol \\mu_i||^2}{2\\sigma^2}+\\log\\P(c_i), \\tag{8.8} \\end{equation}\\] gdzie \\(||\\cdot ||\\) jest normą euklidesową. Rozpisując licznik równania (8.8) mamy \\[\\begin{equation} ||\\boldsymbol x-\\boldsymbol \\mu_i||^2=(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;(\\boldsymbol x-\\boldsymbol \\mu_i). \\end{equation}\\] Zatem \\[\\begin{equation} g_i(\\boldsymbol x)=-\\frac{1}{2\\sigma^2}[\\boldsymbol x&#39;\\boldsymbol x-2\\boldsymbol \\mu_i&#39;\\boldsymbol x+\\boldsymbol \\mu_i&#39;\\boldsymbol \\mu_i]+\\log\\P(c_i). \\end{equation}\\] A ponieważ \\(\\boldsymbol x&#39;\\boldsymbol x\\) nie zależy do \\(i\\), to funkcję dyskryminacyjną możemy zapisać jako \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0}, \\end{equation}\\] gdzie \\(\\boldsymbol w_i=\\frac{1}{\\sigma^2}\\boldsymbol \\mu_i\\), a \\(w_{i0}=\\frac{-1}{2\\sigma^2}\\boldsymbol \\mu_i&#39;\\boldsymbol \\mu_i+\\log\\P(c_i).\\) Na podstawie funkcji dyskryminacyjnych wyznaczamy hiperpłaszczyzny decyzyjne jako zbiory punktów dla których \\(g_i(\\boldsymbol x)=g_j(\\boldsymbol x)\\), gdzie \\(g_i, g_j\\) są największe. Możemy to zapisać w następujący sposób \\[\\begin{equation} \\boldsymbol w&#39;(\\boldsymbol x-\\boldsymbol x_0)=0, \\tag{8.9} \\end{equation}\\] gdzie \\[\\begin{equation} \\boldsymbol w = \\boldsymbol \\mu_i-\\boldsymbol \\mu_j, \\end{equation}\\] oraz \\[\\begin{equation} \\boldsymbol x_0 = \\frac112(\\boldsymbol \\mu_i+\\mu_j)-\\frac{\\sigma^2}{||\\boldsymbol \\mu_i-\\boldsymbol \\mu_j||^2}\\log\\frac{\\P(c_i)}{\\P(c_j)}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j). \\end{equation}\\] Równanie (8.9) określa hiperpłaszczyznę przechodzącą przez \\(\\boldsymbol x_0\\) i prostopadłą do \\(\\boldsymbol w\\). Rysunek 8.5: Dyskrymiancja hiperpłaszczyznami w sygucaji dwóch klas. Wykres po lewej, to ujęcie jednowymiarowe, wykresy po środu - ujęcie 2-wymiarowe i wykresy po prawej, to ujęcie 3-wymiarowe. Źródło: Duda, Hart, and Stork (2001) 8.2.2 Przypadek gdy \\(\\boldsymbol \\Sigma_i=\\boldsymbol \\Sigma\\) Przypadek ten opisuje sytuację, gdy rozkłady \\(\\boldsymbol x\\) są identyczne we wszystkich grupach ale zmienne w ich skład wchodzące nie są niezależne. W tym przypadku funkcje dyskryminacyjne przyjmują postać \\[\\begin{equation} g_i(\\boldsymbol x)=\\frac12(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;\\boldsymbol\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu_i)+\\log\\P(c_i). \\tag{8.10} \\end{equation}\\] Jeśli \\(\\P(c_i)\\) są identyczne dla wszystkich klas, to można je pominąć we wzorze (8.10). Metryka euklidesowa ze wzoru (8.8) została zastąpiona przez odległość Mahalonobis’a. Podobnie ja w przypadku gdy \\(\\boldsymbol \\Sigma_i=I\\), tak i teraz można uprościć (8.10) przez rozpisanie formy kwadratowej, aby otrzymać, że \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0}, \\end{equation}\\] gdzie \\(\\boldsymbol w_i=\\boldsymbol\\Sigma^{-1}\\boldsymbol \\mu_i\\), a \\(w_{i0}=-\\frac{1}{2}\\boldsymbol \\mu_i&#39;\\boldsymbol\\Sigma^{-1}\\boldsymbol \\mu_i+\\log\\P(c_i).\\) Ponieważ funkcje dyskryminacyjne są liniowe, to hiperpłaszczyzny są ograniczeniami obszarów obserwacji każdej z klas \\[\\begin{equation} \\boldsymbol w&#39;(\\boldsymbol x-\\boldsymbol x_0)=0, \\tag{8.11} \\end{equation}\\] gdzie \\[\\begin{equation} \\boldsymbol w = \\boldsymbol\\Sigma^{-1} (\\boldsymbol \\mu_i-\\boldsymbol \\mu_j), \\end{equation}\\] oraz \\[\\begin{equation} \\boldsymbol x_0 = \\frac12(\\boldsymbol \\mu_i+\\mu_j)-\\frac{\\log[ \\P(c_i)/\\P(c_j)]}{(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;\\boldsymbol\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu_i)}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j). \\end{equation}\\] Tym razem \\(\\boldsymbol{w}=\\Sigma^{-1}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j)\\) nie jest wektorem w kierunku \\(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j\\), więc hiperpłaszczyzna rozdzielająca obiekty różnych klas nie jest prostopadła do wektora \\(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j\\) ale przecina go w połowie (w punkcie \\(\\boldsymbol x_0\\)). Rysunek 8.6: Hiperpłaszczyzna rozdzielająca obszary innych klas może być przesunięta w kierunku bardziej prawdopodobnej klasy, jeśli prawdopodobieństwa a priori są różne. Źródło: Duda, Hart, and Stork (2001) 8.2.3 Przypadek gdy \\(\\boldsymbol \\Sigma_i\\) jest dowolnej postaci Jest to najbardziej ogólny przypadek, kiedy nie nakłada się żadnych ograniczeń na macierze kowariancji grupowych. Postać funkcji dyskryminacyjnych jest następująca \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol x&#39;\\boldsymbol W_i\\boldsymbol x+\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0} \\tag{8.12} \\end{equation}\\] gdzie \\[\\begin{align} \\boldsymbol W_i = &amp;-\\frac12 \\boldsymbol\\Sigma_i^{-1},\\\\ \\boldsymbol w_i=&amp; \\boldsymbol\\Sigma_i^{-1}\\boldsymbol\\mu_i,\\\\ w_{i0} = &amp;-\\frac12\\boldsymbol\\mu_i&#39;\\boldsymbol\\Sigma_i^{-1}\\boldsymbol\\mu_i-\\frac12\\log|\\boldsymbol\\Sigma_i|+\\log\\P(c_i). \\end{align}\\] Ograniczenia w ten sposób budowane są hiperpowierzchniami (nie koniecznie hiperpłaszczyznami). W literaturze ta metoda znana jest pod nazwą kwadratowej analizy dyskryminacyjnej (ang. Quadratic Discriminant Analysis). Rysunek 8.7: Przykład zastosowania kwadratowej analizy dyskryminacyjnej. Pokazane są dopuszczalne postaci zbiorów ograniczających. Źródło: Duda, Hart, and Stork (2001) Przykład 8.2 Przeprowadzimy klasyfikację na podstawie zbioru Smarket pakietu ILSR. Dane zawierają kursy indeksu giełdowego S&amp;P500 w latach 2001-2005. Na podstawie wartości waloru z poprzednich 2 dni będziemy chcieli przewidzieć czy ruch w kolejnym okresie czasu będzie w górę czy w dół. library(ISLR) head(Smarket) ## Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction ## 1 2001 0.381 -0.192 -2.624 -1.055 5.010 1.1913 0.959 Up ## 2 2001 0.959 0.381 -0.192 -2.624 -1.055 1.2965 1.032 Up ## 3 2001 1.032 0.959 0.381 -0.192 -2.624 1.4112 -0.623 Down ## 4 2001 -0.623 1.032 0.959 0.381 -0.192 1.2760 0.614 Up ## 5 2001 0.614 -0.623 1.032 0.959 0.381 1.2057 0.213 Up ## 6 2001 0.213 0.614 -0.623 1.032 0.959 1.3491 1.392 Up set.seed(2019) dt.ucz &lt;- Smarket %&gt;% mutate_if(is.numeric, scale) %&gt;% sample_frac(size = 2/3) dt.test &lt;- Smarket[-as.numeric(rownames(dt.ucz)),] mod.qda &lt;- qda(Direction~Lag1+Lag2, data = Smarket) mod.qda ## Call: ## qda(Direction ~ Lag1 + Lag2, data = Smarket) ## ## Prior probabilities of groups: ## Down Up ## 0.4816 0.5184 ## ## Group means: ## Lag1 Lag2 ## Down 0.05068605 0.03229734 ## Up -0.03969136 -0.02244444 Ponieważ funkcje dyskryminacyjne mogą być nieliniowe, to podsumowanie modelu nie zawiera współczynników funkcji. Podsumowanie zawiera tylko prawdopodobieństwa a priori i średnie poszczególnych zmiennych niezależnych w klasach. pred.qda &lt;- predict(mod.qda, dt.test) tab &lt;- table(pred = pred.qda$class, dt.test$Direction) tab ## ## pred Down Up ## Down 10 12 ## Up 171 224 sum(diag(prop.table(tab))) ## [1] 0.5611511 library(klaR) partimat(Direction ~ Lag1+Lag2, data = dt.ucz, method = &quot;qda&quot;, col.correct=&#39;blue&#39;, col.wrong=&#39;red&#39;) Rysunek 8.8: Wykres klasyfikacji na podstawie QDA. Obserwacje zaznczone kolorem niebieskim są prawidłowo zaklasyfikowane, a czerwonym źle 8.3 Analiza dyskryminacyjna częściowych najmniejszych kwadratów Analiza dyskryminacyjna częściowych najmniejszych kwadratów (ang. Partial Least Squares Discriminant Analysis) jest wykorzystywana szczególnie w sytuacjach gdy zestaw predyktorów zwiera zmienne silnie ze sobą skorelowane. Jak wiadomo z wcześniejszych rozważań, metody dyskryminacji obserwacji są mało odporne na nadmiarowość zmiennych niezależnych. Stąd powstał pomysł zastosowania połączenia LDA z PLS (Partial Least Squares), której celem jest redukcja wymiaru przestrzeni jednocześnie maksymalizując korelację zmiennych niezależnych ze zmienną wynikową. Parametrem, który jest kontrolowany podczas budowy modelu jest liczba ukrytych zmiennych. Metoda PLSDA ma kilka implementacji w R, ale najbradziej znana jest funkcja plsda z pakietu caret (Jed Wing et al. 2018). Przykład 8.3 Kontynując poprzedni przykład przeprowadzimy klasyfikacje ruchu waloru korzystając z metody PLSDA. W przeciwieństwie do poprzednich funkcji plsda potrzebuje przekazania zbioru predyktorów i wektora zmiennej wynikowej oddzielnie, a nie za pomocą formuły. Doboru liczby zmiennych latentnych dokonamy arbitralnie. library(caret) mod.plsda &lt;- plsda(dt.ucz[,-c(1,7:9)], as.factor(dt.ucz$Direction), ncomp = 2) mod.plsda$loadings ## ## Loadings: ## Comp 1 Comp 2 ## Lag1 -0.712 0.450 ## Lag2 0.234 0.237 ## Lag3 0.647 ## Lag4 0.158 0.519 ## Lag5 0.681 ## ## Comp 1 Comp 2 ## SS loadings 1.008 1.001 ## Proportion Var 0.202 0.200 ## Cumulative Var 0.202 0.402 Dwie ukryte zmienne użyte do redukcji wymiaru przestrzeni wyjaśniają około 40% zmienności pierwotnych zmiennych. Ładunki (Loadings) pokazują kontrybucje poszczególnych zmiennych w tworzenie się zmiennych ukrytych. pred.plsda &lt;- predict(mod.plsda, dt.test[,-c(1,7:9)]) tab &lt;- table(pred.plsda, dt.test$Direction) tab ## ## pred.plsda Down Up ## Down 37 50 ## Up 144 186 sum(diag(prop.table(tab))) ## [1] 0.5347722 Ponieważ korelacje pomiędzy predyktorami w naszym przypadku nie były duże, to zastosowanie PLSDA nie poprawiło znacząco klasyfikacji w stosunku do metody QDA. cor(dt.ucz[,2:6]) ## Lag1 Lag2 Lag3 Lag4 Lag5 ## Lag1 1.000000000 -0.001713222 0.003820374 0.01583203 0.02504524 ## Lag2 -0.001713222 1.000000000 -0.046611448 -0.02069792 -0.04105822 ## Lag3 0.003820374 -0.046611448 1.000000000 -0.06142632 -0.03424691 ## Lag4 0.015832026 -0.020697920 -0.061426325 1.00000000 -0.07102928 ## Lag5 0.025045238 -0.041058218 -0.034246907 -0.07102928 1.00000000 8.4 Regularyzowana analiza dyskryminacyjna Regularyzowana analiza dyskrymiancyjna (ang. Regularized Discriminant Analysis) powstała jako technika równoważąca zalety i waday LDA i QDA. Ze względu na zdolności generalizacyjne model LDA jest lepszy od QDA (mniejsza wariancja modelu), ale jednocześnie QDA ma bardziej elastyczną postać hiperpowierzchni brzegowych rozdzielających obiekty różnych klas. Dlatego Friedman (1989) wprowadził technikę będącą kompromisem pomiędzy LDA i QDA poprzez odpowiednie określenie macierzy kowariancji \\[\\begin{equation} \\tilde{\\boldsymbol \\Sigma}_i(\\lambda) = \\lambda\\boldsymbol\\Sigma_i + (1-\\lambda)\\boldsymbol\\Sigma, \\end{equation}\\] gdzie \\(\\boldsymbol \\Sigma_i\\) jest macierzą kowariancji dla \\(i\\)-tej klasy, a \\(\\boldsymbol \\Sigma\\) jest uśrednioną macierzą kowariancji wszystkich klas. Zatem odpowiedni dobór parametru \\(\\lambda\\) decyduje czy poszukujemy modelu prostrzego (\\(\\lambda = 0\\) odpowiada LDA), czy bardziej elastycznego (\\(\\lambda=1\\) oznacza QDA). Dodatkowo metoda RDA pozwala na elastyczny wybór pomiędzy postaciami macierzy kowariancji wspólnej dla wszystkich klas \\(\\boldsymbol\\Sigma\\). Może ona być macierzą jednostkową, jak w przypadku 8.2.1, co oznacza niezależność predyktorów modelu, może też być jak w przypadku 8.2.2, gdzie dopuszcza się korelacje między predyktorami. Dokonuje się tego przez odpowiedni dobór parametru \\(\\gamma\\) \\[\\begin{equation} \\boldsymbol \\Sigma(\\gamma) = \\gamma\\boldsymbol \\Sigma+(1-\\gamma)\\sigma^2I. \\end{equation}\\] Przykład 8.4 Funkcja rda pakietu klaR jest implementacją powyższej metody. Ilustrają jej działania będzie klasyfikacja stanów z poprzedniego przykładu. library(klaR) mod.rda &lt;- rda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, dt.ucz) mod.rda ## Call: ## rda(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = dt.ucz) ## ## Regularization parameters: ## gamma lambda ## 0.33416870 0.03931045 ## ## Prior probabilities of groups: ## Down Up ## 0.4789916 0.5210084 ## ## Misclassification rate: ## apparent: 43.337 % ## cross-validated: 45.524 % Model zostal oszacowany z parametrami wyznaczonymi na podstawie sparawdzianu krzyżowego zastosowanego w funkcji rda. pred.rda &lt;- predict(mod.rda, dt.test) (tab &lt;- table(pred = pred.rda$class, dt.test$Direction)) ## ## pred Down Up ## Down 19 30 ## Up 162 206 sum(diag(prop.table(tab))) ## [1] 0.5395683 Jakość klasyfiakcji jest na zbliżonym poziomie jak przy poprzednich metodach. 8.5 Analiza dyskryminacyjna mieszana Liniowa analiza dyskryminacyjna zakładała, że średnie (centroidy) w klasach są różne ale macierz kowariancji wszystkich klas jest jednakowa. Analiza dykryminacyjna mieszana (ang. Mixture Discriminant Analysis) prezentuje jeszcze inne podejście ponieważ zakłada, że każda klasa może być charakteryzowana przez wiele wielowymiarowych rozkładów normalnych, których centroidy mogą się róznić, ale macierze kowariancji nie. Wówczas rozkład dla danej klasy jest mieszaniną rozkładów składowych, a funkcja dyskryminacyjna dla \\(i\\)-tej klasy przyjmuje postać \\[\\begin{equation} g_i(\\boldsymbol x)\\propto \\sum_{k=1}^{L_i}\\phi_{ik}g_{ik}(\\boldsymbol x), \\end{equation}\\] gdzie \\(L_i\\) jest liczbą rozkładów składających się na \\(i\\)-tą klasę, a \\(\\phi_{ik}\\) jest współczynnikiem proporcji estymowanych w czasie uczenia modelu. Przykład 8.5 Funkcja mda pakietu mda (Trevor Hastie &amp; Robert Tibshirani. Original R port by Friedrich Leisch, Hornik, and Ripley. 2017) jest implementacją tej techniki w R. Jej zastosowanie pokażemy na przykładzie danych giełdowych z poprzedniego przykładu. Użyjemy domyślnych ustawień funkcji (trzy rozkłady dla każdej klasy). library(mda) mod.mda &lt;- mda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, dt.ucz) mod.mda ## Call: ## mda(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = dt.ucz) ## ## Dimension: 5 ## ## Percent Between-Group Variance Explained: ## v1 v2 v3 v4 v5 ## 48.45 88.33 94.80 99.68 100.00 ## ## Degrees of Freedom (per dimension): 6 ## ## Training Misclassification Error: 0.42737 ( N = 833 ) ## ## Deviance: 1134.453 pred.mda &lt;- predict(mod.mda, dt.test) (tab &lt;- table(pred = pred.mda, dt.test$Direction)) ## ## pred Down Up ## Down 23 38 ## Up 158 198 sum(diag(prop.table(tab))) ## [1] 0.529976 Kolejny raz model dyskryminacyjny charakteryzuje się podobną jakością klasyfikacji. 8.6 Elastyczna analiza dyskryminacyjna Zupełnie inne podejście w stosunku do wcześniejszych rozwiązań, przezentuje elastyczna analiza dyskryminacyjna (ang. Flexible Discriminant Analysis) . Kodując klasy wynikowe jako zmienne dychotomiczne (dla każdej klasy jest odrębna zmienna wynikowa) dla każdej z nich budowanych jest \\(k\\) modeli regresji. Mogą to być modele regresji penalizowanej, jak regresja grzebietowa lub LASSO, modele regresji wielomianowej albo modele regresji sklejanej (MARS), o których będzie mowa w dalszej części tego opracowania. Przykładowo, jeśli modelem bazowym jest MARS, to funkcja dyskryminacyjna \\(i\\)-tej klasy może być postaci \\[\\begin{equation} g_i(\\boldsymbol x)=\\beta_0+\\beta_1h(1-x_1)+\\beta_2h(x_2-1)+\\beta_3h(1-x_3)+\\beta_4h(x_1-1), \\end{equation}\\] gdzie \\(h\\) są tzw. funkcjami bazowymi postaci \\[\\begin{equation} h(x)= \\begin{cases} x, &amp; x&gt; 0\\\\ 0, &amp; x\\leq 0. \\end{cases} \\end{equation}\\] Klasyfikacji dokonujemy sprawdzając znak funkcji dyskryminacyjnej \\(g_i\\), jeśli jest dodatni, to funkcja przypisuje obiekt do klasy \\(i\\)-tej. W przeciwnym przypadku nie należy do tej klasy. Rysunek 8.9: Przykład klasyfikacji dwustanowej za pomocą metody FDA Przykład 8.6 Funkcja fda pakietu mda jest implementacją techniki FDA w R. Na postawie danych z poprzedniego przykładu zostanie przedstawiona zasada dziełania. Przyjmiemy domyślne ustawienia funkcji, z wyjątkiem metody estymacji modelu, jako którą przyjmiemy MARS. mod.fda &lt;- fda(Direction~Lag1+Lag2, dt.ucz, method = mars) mod.fda ## Call: ## fda(formula = Direction ~ Lag1 + Lag2, data = dt.ucz, method = mars) ## ## Dimension: 1 ## ## Percent Between-Group Variance Explained: ## v1 ## 100 ## ## Training Misclassification Error: 0.43938 ( N = 833 ) Ponieważ, zmienna wynikowa jest dwustanowa, to powstała tylko jedna funkcja dyskryminacyjna. Parametry modelu są następujące mod.fda$fit$coefficients ## [,1] ## [1,] 0.1129623 ## [2,] -0.5202437 ## [3,] 0.5462219 pred.fda &lt;- predict(mod.fda, dt.test) (tab &lt;- table(pred = pred.fda, dt.test$Direction)) ## ## pred Down Up ## Down 108 118 ## Up 73 118 sum(diag(prop.table(tab))) ## [1] 0.5419664 Jakość klasyfikacji jest tylko nieco lepsza niż w przypadku poprzednich metod. Bibliografia "],
["bayes.html", "9 Klasyfikatory bayesowskie 9.1 Klasyfikator maximum a posteriori (MAP) 9.2 Klasyfikator największej warogodności (ML) 9.3 Naiwny klasyfikator Bayesa (NB)18 9.4 Zalety i wady", " 9 Klasyfikatory bayesowskie Całą gamę klasyfikatorów opartych na twierdzeniu Bayesa nazywać będziemy bayesowskimi. \\[\\begin{equation}\\label{bayes} P(A|B)=\\frac{P(A)P(B|A)}{P(B)}, \\end{equation}\\] gdzie \\(P(B)&gt;0\\). Bayesowskie reguły podejmowania decyzji dały podstawy takich metod jak: liniowa analiza dyskryminacyjna; kwadratowa analiza dyskryminacyjna; W ustaleniu klasyfikatora bayesowskiego będzie nam przyświecała cały czas ta sama reguła: jeśli znam wartości cech charakteryzujących badane obiekty oraz klasy do których należą (w próbie uczącej), to na ich podstawie mogę wyznaczyć miary prawdopodobieństw a posteriori, które pomogą mi w ustaleniu klasy do której należy nowy testowy element. W dalszej części będziemy przyjmowali następujące oznaczenia: \\(T\\) - zbiór danych uczących (treningowych), \\(T^j\\) - zbiór danych uczących dla których przyjęliśmy decyzję o przynależności do \\(j\\)-tej klasy, \\(T^j_{a_i=v}\\) - zbiór danych uczących o wartości atrybutu \\(a_i\\) równej \\(v\\) i klasy \\(j\\)-tej, \\(\\mathbb{H}\\) - przestrzeń hipotez, \\(P(h|a_1=v_1, a_2=v_2,\\ldots,a_p=v_p)\\) - prawdopodobieństwo a posteriori, że prawdziwa jest hipoteza \\(h\\in \\mathbb{H}\\), jeśli znamy atrybuty obiektu, \\(P(h)\\) - prawdopodobieństwo a priori zajścia hipotezy \\(h\\in \\mathbb{H}\\), \\(c\\) - prawdziwy stan obiektu. 9.1 Klasyfikator maximum a posteriori (MAP) Na podstawie wiedzy o atrybutach obiektu \\(x\\) podejmujemy decyzję o klasyfikacji tego obiektu zgodnie z hipotezą \\(h_{MAP}\\in \\mathbb{H}\\), która przyjmuje postać \\[\\begin{align}\\label{MAP} h_{MAP}=&amp;\\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(h|a_1=v_1, a_2=v_2,\\ldots,a_p=v_p)\\\\ =&amp; \\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h)\\cdot P(h), \\end{align}\\] gdzie ostatnia równość wynika z twierdzenia Bayesa oraz faktu, że dla konkretnego obiektu \\(x\\) wielkości atrybutów nie zależą od postawionej hipotezy. 9.2 Klasyfikator największej warogodności (ML) Na podstawie wiedzy o atrybutach obiektu \\(x\\) podejmujemy decyzję o klasyfikacji tego obiektu zgodnie z hipotezą \\(h_{ML}\\in \\mathbb{H}\\), która przyjmuje postać \\[\\begin{equation}\\label{ML} h_{ML}=\\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h). \\end{equation}\\] Uwaga. Obie wspomniane metody wymagają znajomości prawdopodobieństwa \\(P(a_1=v_1,a_2=v_2,\\ldots,a_p=v_p|h)\\), ale różnią się podejściem do wiedzy o prawdopodobieństwach a priori. W metodzie MAP brana pod uwagę jest wiedza o prawdopodobieństwie przynależności do poszczególnych klas, a w ML nie. Dla klasyfikacji, w których prawdopodobieństwa przynależności do klas są takie same, klasyfikatory MAP i ML są równoważne. 9.3 Naiwny klasyfikator Bayesa (NB)18 Największy problem w wyznaczeniu klasyfikatorów MAP i ML stanowi wyznaczenie rozkładu łącznego \\(P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h)\\). W naiwnym klasyfikatorze Bayesa zakłada się niezależność warunkową poszczególnych atrybutów względem klasy do której ma należeń wg hipotezy obiekt. Założenie to często nie jest spełnione i stąd nazwa przymiotnik “naiwny”. Definicja naiwnego klasyfikatora bayesowskiego różni się od klasyfikatora MAP tylko podejściem do prawdopodobieństwa a posteriori. \\[\\begin{equation}\\label{naiwny_bayes} h_{NB}=\\operatorname{arg}\\max_{h_j\\in \\mathbb{H}}P(h_j)\\prod_{i=1}^{p}P(a_i=v_i|h_j), \\end{equation}\\] gdzie \\(h_j\\) oznacza hipotezę (decyzję), że badany obiekt należy do \\(j\\)-tej klasy. Oczywiście zarówno prawdopodobieństwo a priori jak i a posteriori są wyznaczane na podstawie próby, i tak prawdopodobieństwo a priori wynosi \\[\\begin{equation}\\label{apriori} P(h_j)=P_T(h_j)=\\frac{|T^j|}{|T|}, \\end{equation}\\] gdzie \\(|A|\\) oznacza moc zbioru \\(A\\). Natomiast prawdopodobieństwo a posteriori dla \\(i\\)-tego atrybutu wynosi \\[\\begin{equation}\\label{aposteriori} P(a_i=v_i|h_j)=P_{T^j}(a_i=v_i)=\\frac{|T^j_{a_i=v_i}|}{|T^j|}. \\end{equation}\\] Na mocy powyższego możemy zauważyć, że jeżeli założenie o warunkowej niezależności jest spełnione, to klasyfikatory NB i MAP są równoważne. Chcąc przypisać klasę nowemu obiektowi powstaje problem praktyczny, polegający na tym, że dla pewnych konfiguracji atrybutów nie ma odpowiedników w nauczonym modelu. Powodem takiego stanu rzeczy jest fakt, że takie kombinacje nie wystąpiły w próbie uczącej. Istnieją dwa sposoby predykcji w takiej sytuacji: \\[\\begin{equation}\\label{pred1} P(a_i=v_i|h_j)= \\begin{cases} \\frac{|T^j_{a_i=v_i}|}{|T^j|}, &amp; T^j_{a_i=v_i}\\neq \\emptyset\\\\ \\epsilon, &amp; \\text{w przeciwnym przypadku.} \\end{cases} \\end{equation}\\] W tym przypadku przyjmuje się, że \\(\\epsilon \\ll 1/|T_j|\\). Drugi sposób wykorzystuje estymację z poprawką \\[\\begin{equation}\\label{pred2} P(a_i=v_i|h_j)=\\frac{|T^j_{a_i=v_i}|+mp}{|T^j|+mp}, \\end{equation}\\] gdzie \\(p\\) oznacza prawdopodobieństwo a priori przyjęcia przez atrybut \\(a\\) wartości \\(v\\) (najczęściej \\(p=1/|A|\\), \\(A\\) - zbiór wszystkich możliwych wartości atrybutu \\(a\\)), \\(m\\) - waga (najczęściej \\(m=|A|\\)). W przypadku gdy atrybuty są mierzone na skali ciągłej najczęściej stosuje się dyskretyzację ich do zmiennych ze skali przedziałowej. Inna metoda stosowana w przypadku ciągłych atrybutów, to użycie gęstości \\(g_i^j\\) o rozkładzie normalnym w miejsce \\(P(a_i=v_i|h_j)\\). Przy czym do obliczenia parametrów rozkładu stosujemy wzory \\[\\begin{equation}\\label{sred} m_i^j=\\frac{1}{|T^j|}\\sum_{x\\in T^j}a_i(x), \\end{equation}\\] oraz \\[\\begin{equation}\\label{odch} (s_i^j)^2=\\frac{1}{|T^j|-1}\\sum_{x\\in T^j}(a_i(x)-m_i^j)^2. \\end{equation}\\] Obsługa braków danych przez naiwny klasyfikator Bayesa jest dość prosta i opiera się na liczeniu prawdopodobieństw a posteriori wyłącznie dla obiektów, których wartości atrybutów są znane. Dlatego prawdopodobieństwa warunkowe liczy się wg wzoru \\[\\begin{equation}\\label{pr_war} P(a_i=v_i|h_j)=\\frac{|T^j_{a_i=v_i}|}{|T^j|-|T^j_{a_i=NA}|}. \\end{equation}\\] Jeśli brakujące dane nie niosą w sobie istotnych informacji dotyczących klasyfikacji obiektów, to naiwny klasyfikator Bayesa będzie działał poprawnie. Naiwny klasyfikator Bayesa jest implementowany w pakietach e1071 (Meyer et al. 2019) i klaR (Weihs et al. 2005). Przykład 9.1 Przeprowadzimy klasyfikację dla zbioru Titanic. W przypadku funkcji z pakietu e1071 nie potrzeba zamieniać tabeli na przypadki. W pakiecie klaR istnieje inna funkcja budująca klasyfikator Bayesa NaiveBayes, ale w tym przypadku jeśli zbiór jest w formie tabeli, to należy go zamienić na ramkę danych z oddzielnymi przypadkami. library(e1071) Titanic ## , , Age = Child, Survived = No ## ## Sex ## Class Male Female ## 1st 0 0 ## 2nd 0 0 ## 3rd 35 17 ## Crew 0 0 ## ## , , Age = Adult, Survived = No ## ## Sex ## Class Male Female ## 1st 118 4 ## 2nd 154 13 ## 3rd 387 89 ## Crew 670 3 ## ## , , Age = Child, Survived = Yes ## ## Sex ## Class Male Female ## 1st 5 1 ## 2nd 11 13 ## 3rd 13 14 ## Crew 0 0 ## ## , , Age = Adult, Survived = Yes ## ## Sex ## Class Male Female ## 1st 57 140 ## 2nd 14 80 ## 3rd 75 76 ## Crew 192 20 nb &lt;- naiveBayes(Survived ~ ., data = Titanic) nb$apriori ## Survived ## No Yes ## 1490 711 Poniższe tabele zawierają warunkowe prawdopodobieństwa przynależności do poszczólnych klas. nb$tables ## $Class ## Class ## Survived 1st 2nd 3rd Crew ## No 0.08187919 0.11208054 0.35436242 0.45167785 ## Yes 0.28551336 0.16596343 0.25035162 0.29817159 ## ## $Sex ## Sex ## Survived Male Female ## No 0.91543624 0.08456376 ## Yes 0.51617440 0.48382560 ## ## $Age ## Age ## Survived Child Adult ## No 0.03489933 0.96510067 ## Yes 0.08016878 0.91983122 dane &lt;- as.data.frame(Titanic) pred &lt;- predict(nb, dane) pred ## [1] Yes No No No Yes Yes Yes Yes No No No No Yes Yes Yes Yes Yes ## [18] No No No Yes Yes Yes Yes No No No No Yes Yes Yes Yes ## Levels: No Yes tab &lt;- table(pred, dane$Survived) tab ## ## pred No Yes ## No 7 7 ## Yes 9 9 sum(diag(prop.table(tab))) ## [1] 0.5 Naiwny klasyfikator spisał się bardzo słabo, ponieważ klasyfikacja na poziomie 0.5 jest taka jak przy rzucie monetą. Przykład 9.2 Przeprowadzimy klasyfikację gatunków irysów na podstawie szerokości i długości kielicha i płatka. library(klaR) set.seed(2019) uczaca &lt;- sample(1:nrow(iris), 2*nrow(iris)/3) pr.ucz &lt;- iris[uczaca,] pr.test &lt;- iris[-uczaca,] nb2 &lt;- NaiveBayes(Species~., data = pr.ucz) nb2$apriori ## grouping ## setosa versicolor virginica ## 0.36 0.31 0.33 Prawdopodobieństwa a priori zostały oszacowane na podstawie próby uczącej. Poniższe tabele zawierają średnie i odchylenia standardowe zmiennych w poszczególnych klasach. nb2$tables ## $Sepal.Length ## [,1] [,2] ## setosa 4.994444 0.3438807 ## versicolor 5.977419 0.5613731 ## virginica 6.603030 0.7359029 ## ## $Sepal.Width ## [,1] [,2] ## setosa 3.436111 0.3586903 ## versicolor 2.838710 0.2996414 ## virginica 2.942424 0.3211603 ## ## $Petal.Length ## [,1] [,2] ## setosa 1.472222 0.1782632 ## versicolor 4.316129 0.4576001 ## virginica 5.606061 0.6269666 ## ## $Petal.Width ## [,1] [,2] ## setosa 0.2444444 0.09394358 ## versicolor 1.3354839 0.18537959 ## virginica 2.0000000 0.25860201 pred &lt;- predict(nb2, newdata = pr.test) tab &lt;- table(pred$class, pr.test$Species) tab ## ## setosa versicolor virginica ## setosa 14 0 0 ## versicolor 0 18 1 ## virginica 0 1 16 sum(diag(prop.table(tab))) ## [1] 0.96 Klasyfikacja na podstawie modelu jest badzo do dobra (96%). 9.4 Zalety i wady Zalety: prostota konstrukcji i prosty algorytm; jeśli jest spełnione założenie warunkowej niezależności, to ten klasyfikator działa szybciej i czasem lepiej niż inne metody klasyfikacji; nie potrzebuje dużych zbiorów danych do estymacji parametrów; Wady: często nie spełnione założenie o warunkowej niezależności powoduje obciążenie wyników; brak możliwości wprowadzania interakcji efektów kilku zmiennych; potrzebuje założenia normalności warunkowych gęstości w przypadku ciągłych atrybutów; często istnieją lepsze klasyfikatory. Bibliografia "],
["bibliografia.html", "Bibliografia", " Bibliografia "]
]
