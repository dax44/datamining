[
["index.html", "Eksploracja danych WstÄ™p O ksiÄ…Å¼ce Zakres przedmiotu Zakres technik stosowanych w data mining Etapy eksploracji danych", " Eksploracja danych Dariusz Majerek Katedra Matematyki Stosowanej WydziaÅ‚ Podstaw Techniki Politechnika Lubelskad.majerek@pollub.pl 2019-05-06 WstÄ™p O ksiÄ…Å¼ce Niniejsza ksiÄ…Å¼ka powstaÅ‚a na bazie doÅ›wiadczeÅ„ autora, a gÅ‚Ã³wnym jej celem jest przybliÅ¼enie czytelnikowi podstaw z dziedziny Data mining studentom kierunku Matematyka Politechniki Lubelskiej. BÄ™dzie Å‚Ä…czyÄ‡ w sobie zarÃ³wno treÅ›ci teoretyczne zwiÄ…zane z przedstawianymi etapami eksploracji danych i budowÄ… modeli, jak i praktyczne wskazÃ³wki dotczÄ…ce budowy modeli w Å›rodowisku R (R Core Team 2018). Podane zostanÄ… rÃ³wnieÅ¼ wskazÃ³wki, jak raportowaÄ‡ wyniki analiz i jak dokonaÄ‡ wÅ‚aÅ›ciwych ilustracji wynikÃ³w. Bardzo uÅ¼yteczny w napisaniu ksiÄ…Å¼ki byÅ‚y pakiety programu R: bookdown (Xie 2018a), knitr (Xie 2018b) oraz pakiet rmarkdown (Allaire et al. 2018). Zakres przedmiotu Przedmiot Eksploracja danych bÄ™dzie obejmowaÅ‚ swoim zakresem eksploracjÄ™ i wizualizacjÄ™ danych oraz uczenie maszynowe. Eksploracja danych ma na celu pozyskiwanie i systematyzacjÄ™ wiedzy pochodzÄ…cej z danych. Odbywa siÄ™ ona gÅ‚Ã³wnie przy uÅ¼yciu technik statystycznych, rachunku prawdopodobieÅ„stwa i metod z zakresu baz danych. Natomiast uczenie maszynowe, to gaÅ‚Ä…Åº nauki (obejmuje nie tylko statystykÄ™, choÄ‡ to na niej siÄ™ gÅ‚Ã³wnie opiera) dotyczÄ…cej budowy modeli zdolnych do rozpoznawania wzorcÃ³w, przewidywania wartoÅ›ci i klasyfikacji obiektÃ³w. Data mining to szybko rosnaca grupa metod analizy danych rozwijana nie tylko przez statystykÃ³w ale rÃ³wnieÅ¼ przez biologÃ³w, genetykÃ³w, cybernetykÃ³w, informatykÃ³w, ekonomistÃ³w, osoby pracujace nad rozpoznawaniem obrazÃ³w i wiele innych grup zawodowych. W dzisiejszych czasch trudno sobie wyobraziÄ‡ Å¼ycie bez sztucznej inteligencji. Towarzyszy ona nam w codziennym, Å¼yciu kiedy korzystamy z telefonÃ³w komÃ³rkowych, wyszukiwarek internetowych, robotÃ³w sprzÄ…tajÄ…cych, automatycznych samochodÃ³w, nawigacji czy gier komputerowych. Lista ta jest niepeÅ‚na i stale siÄ™ wydÅ‚uÅ¼a. href=â€œhttps://twitter.com/i/status/1091069356367200256â€&gt;January 31, 2019 Zakres technik stosowanych w data mining statystyka opisowa wielowymiarowa analiza danych analiza szeregÃ³w czasowych analiza danych przestrzennych reguÅ‚y asocjacji uczenie maszynowe1, w tym: klasyfikacja predykcja analiza skupieÅ„ text mining i wiele innych Rysunek .: PrzykÅ‚ad nienadzorowanego uczenia maszynowego. Å¹rÃ³dÅ‚o:https://analyticstraining.com/cluster-analysis-for-business/ href=â€œhttps://twitter.com/i/status/1097199751072690176â€&gt;Ferbruary 17, 2019 Etapy eksploracji danych Rysunek .: Etapy eksploracji danych (Kavakiotis et al. 2017) Czyszczenie danych - polega na usuwaniu brakÃ³w danych, usuwaniu staÅ‚ych zmiennych, imputacji brakÃ³w danych oraz przygotowaniu danych do dalszych analiz. Integracja danych - Å‚Ä…czenie danych pochodzÄ…cych z rÃ³Å¼nych ÅºrÃ³deÅ‚. Selekcja danych - wybÃ³r z bazy tych danych, ktÃ³re sÄ… potrzebne do dalszych analiz. Transformacja danych - przeksztaÅ‚cenie i konsolidacja danych do postaci przydatnej do eksploracji. Eksploracja danych - zastosowanie technik wymienionych wczeÅ›niej w celu odnalezienia wzorcÃ³w2 i zaleÅ¼noÅ›ci. Ewaluacja modeli - ocena poprawnoÅ›ci modeli oraz wzorcÃ³w z nich uzyskanych. Wizualizacja wynikÃ³w - graficzne przedstawienie odkrytych wzorcÃ³w. WdraÅ¼anie modeli - zastosowanie wyznaczonych wzorcÃ³w. Bibliografia "],
["roz1.html", "1 Import danych", " 1 Import danych Åšrodowisko R pozwala na import i export plikÃ³w o rÃ³Å¼nych rozszerzeniach (txt, csv, xls, xlsx, sav, xpt, dta, itd.)3. W tym celu czasami trzeba zainstalowaÄ‡ pakiety rozszerzajÄ…ce podstawowe moÅ¼liwoÅ›ci R-a. Najnowsza4 wersja programu RStudio (v. 1.1.463)5 pozwala na wczytanie danych z popularnych ÅºrÃ³deÅ‚ za pomocÄ… GUI. Rysunek 1.1: NarzÄ™dzie do importu plikÃ³w programu RStudio JeÅ›li dane sÄ… zapisane w trybie tekstowym (np. txt, csv), to wczytujemy je w nastÄ™pujÄ…cy sposÃ³b dane1 &lt;- read.table(&quot;data/dane1.txt&quot;, header = T) head(dane1) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane2 &lt;- read.csv2(&quot;data/dane1.csv&quot;, header = T) head(dane2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # funkcja pakietu readr wczytuje plik jako ramkÄ™ danych w formacie tibble # pakiet readr jest czÄ™siÄ… wiÄ™kszego pakietu tidyverse, # ktÃ³ry zostaÅ‚ wczytany wczsniej dane3 &lt;- read_csv2(&quot;data/dane1.csv&quot;) dane3 ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows JeÅ›li dane sÄ… przechowywane w pliku Excel (np. xlsx), to importujemy je za pomocÄ… funkcji read_excel pakietu readxl. DomyÅ›lnie jest wczytywany arkusz pierwszy ale jeÅ›li zachodzi taka potrzeba, to moÅ¼na ustaliÄ‡, ktÃ³ry arkusz pliku Excel ma byÄ‡ wczytany za pomocÄ… paramteru sheet, np. sheet=3, co oznacza, Å¼e zostanie wczytany trzeci arkusz pliku. Rysunek 1.2: Fragment pliku Excel PoniewaÅ¼ w pliku dane1.xlsx braki danych zostaÅ‚y zakodowane znakami BD oraz -, to naleÅ¼y ten fakt przekazaÄ‡ funkcji, aby poprawnie wczytaÄ‡ braki danych. W przeciwnym przypadku zmienne zawierajÄ…ce braki tak kodowane, bÄ™dÄ… wczytane jako zmienne znakowe. library(readxl) dane4 &lt;- read_excel(&quot;data/dane1.xlsx&quot;, na = c(&quot;BD&quot;, &quot;-&quot;)) dane4 ## # A tibble: 150 x 5 ## `DÅ‚ugoÅ›Ä‡ kielic~ `SzerokoÅ›Ä‡ kiel~ `DÅ‚ugoÅ›Ä‡ pÅ‚atka` `SzerokoÅ›Ä‡ pÅ‚at~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; Istniej oczywiÅ›cie jeszcze wiele innych fomatÃ³w danych, charakterystycznych dla programÃ³w, w ktÃ³rych sÄ… traktowane jako domyÅ›lne.6 W szczegÃ³lny sposÃ³b naleÅ¼y zwrÃ³ciÄ‡ uwagÄ™ na pliki o rozszerzeniu RData lub rda7 oraz pliki rds. Pliki rda sÅ‚uÅ¼Ä… do przechowywania obiektÃ³w programu R. MogÄ… to byÄ‡ pliki danych ale rÃ³wnieÅ¼ obiekty graficzne (typu wyniki funkcji ggplot), modele (np. wynik funkcji lm()), zdefiniowane funkcje i wszystkie inne obiekty, ktÃ³re da siÄ™ zapisaÄ‡ w Å›rodowisku R. Ponadto pliki rda pozawalajÄ… na zapisanie wielu obiektÃ³w w jednym pliku. Pliki o rozszerzeniu rds majÄ… podobnÄ… funkcjÄ™ z tym, Å¼e pozwalajÄ… na przechowywanie tylko jednego obiektu. # wszystkie wczytane wczeÅ›niej pliki zapisuje w jednym pliku save(dane1, dane2, dane3, dane4, file = &quot;data/dane.rda&quot;) # plik rda zostaÅ‚ zapisany list.files(path = &quot;data/&quot;) ## [1] &quot;algae.csv&quot; &quot;Analysis.txt&quot; &quot;dane.rda&quot; &quot;dane1.csv&quot; ## [5] &quot;dane1.txt&quot; &quot;dane1.xlsx&quot; &quot;dane4.rds&quot; &quot;dane4.sav&quot; # usuwam dane ze Å›rodowiska R rm(dane1, dane2, dane3, dane4) # sprawdzam co jest wczytane do R ls() ## character(0) # wczytujÄ™ plik rda load(&quot;data/dane.rda&quot;) # jeszcze raz sprawdzam co jest wczytane do R ls() ## [1] &quot;dane1&quot; &quot;dane2&quot; &quot;dane3&quot; &quot;dane4&quot; ZapisujÄ…c obiekty jako oddzielne pliki, moÅ¼na przy wczytywaniu nadawaÄ‡ im nazwy. rm(dane1, dane2, dane3) ls() ## [1] &quot;dane4&quot; saveRDS(dane4, file = &quot;data/dane4.rds&quot;) nowe_dane &lt;- readRDS(&quot;data/dane4.rds&quot;) nowe_dane ## # A tibble: 150 x 5 ## `DÅ‚ugoÅ›Ä‡ kielic~ `SzerokoÅ›Ä‡ kiel~ `DÅ‚ugoÅ›Ä‡ pÅ‚atka` `SzerokoÅ›Ä‡ pÅ‚at~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; OprÃ³cz wielu zalet takiego sposobu importu i eksportu danych jest jedna powaÅ¼na wada, pliki te moÅ¼na odczytaÄ‡ jedynie za pomocÄ… R. OsobiÅ›cie polecam stosowaÄ‡ do importu i eksportu danych plikÃ³w w takich formatach, ktÃ³re mogÄ… przeczytaÄ‡ wszyscy. Jak dotÄ…d widaÄ‡ do importu rÃ³Å¼nych formatÃ³w danych potrzebujemy rÃ³Å¼nych funkcji, czasami nawet z rÃ³Å¼nych pakietÃ³w. Istnieje rozwiÄ…zanie tej niedogodnoÅ›ci ğŸ™‹ library(rio) dane1 &lt;- import(&quot;data/dane1.txt&quot;) head(dane1) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane2 &lt;- import(&quot;data/dane1.csv&quot;, dec = &quot;,&quot;) # dane1.csv miaÅ‚y , jako znak rozdzielajÄ…cy cechÄ™ i mantysÄ™ liczb # dlatego wÅ‚Ä…czamy parametr dec head(dane2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dane3 &lt;- import(&quot;data/dane1.xlsx&quot;, na=c(&quot;BD&quot;,&quot;-&quot;)) head(dane3) ## DÅ‚ugoÅ›Ä‡ kielicha SzerokoÅ›Ä‡ kielicha DÅ‚ugoÅ›Ä‡ pÅ‚atka SzerokoÅ›Ä‡ pÅ‚atka ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## Gatunki ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa dane4 &lt;- import(&quot;data/dane4.rds&quot;) dane4 ## # A tibble: 150 x 5 ## `DÅ‚ugoÅ›Ä‡ kielic~ `SzerokoÅ›Ä‡ kiel~ `DÅ‚ugoÅ›Ä‡ pÅ‚atka` `SzerokoÅ›Ä‡ pÅ‚at~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 NA NA 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows, and 1 more variable: Gatunki &lt;chr&gt; Lista moÅ¼liwoÅ›ci jakÄ… daje nam pakiet rio (Chan and Leeper 2018) jest niemal nieograniczona:8 Comma-separated data (.csv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE Pipe-separated data (.psv), using fread or, if fread = FALSE, read.table with sep = â€˜|â€™, row.names = FALSE and stringsAsFactors = FALSE Tab-separated data (.tsv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE SAS (.sas7bdat), using read_sas. SAS XPORT (.xpt), using read_xpt or, if haven = FALSE, read.xport. SPSS (.sav), using read_sav. If haven = FALSE, read.spss can be used. Stata (.dta), using read_dta. If haven = FALSE, read.dta can be used. SAS XPORT (.xpt), using read.xport. SPSS Portable Files (.por), using read_por. Excel (.xls and .xlsx), using read_excel. Use which to specify a sheet number. For .xlsx files, it is possible to set readxl = FALSE, so that read.xlsx can be used instead of readxl (the default). R syntax object (.R), using dget Saved R objects (.RData,.rda), using load for single-object .Rdata files. Use which to specify an object name for multi-object .Rdata files. This can be any R object (not just a data frame). Serialized R objects (.rds), using readRDS. This can be any R object (not just a data frame). Epiinfo (.rec), using read.epiinfo Minitab (.mtp), using read.mtp Systat (.syd), using read.systat â€œXBASEâ€ database files (.dbf), using read.dbf Weka Attribute-Relation File Format (.arff), using read.arff Data Interchange Format (.dif), using read.DIF Fortran data (no recognized extension), using read.fortran Fixed-width format data (.fwf), using a faster version of read.fwf that requires a widths argument and by default in rio has stringsAsFactors = FALSE. If readr = TRUE, import will be performed using read_fwf, where widths should be: NULL, a vector of column widths, or the output of fwf_empty, fwf_widths, or fwf_positions. gzip comma-separated data (.csv.gz), using read.table with row.names = FALSE and stringsAsFactors = FALSE CSVY (CSV with a YAML metadata header) using read_csvy. Feather R/Python interchange format (.feather), using read_feather Fast storage (.fst), using read.fst JSON (.json), using fromJSON Matlab (.mat), using read.mat EViews (.wf1), using readEViews OpenDocument Spreadsheet (.ods), using read_ods. Use which to specify a sheet number. Single-table HTML documents (.html), using read_html. The data structure will only be read correctly if the HTML file can be converted to a list via as_list. Shallow XML documents (.xml), using read_xml. The data structure will only be read correctly if the XML file can be converted to a list via as_list. YAML (.yml), using yaml.load Clipboard import (on Windows and Mac OS), using read.table with row.names = FALSE Google Sheets, as Comma-separated data (.csv) PrzykÅ‚ad 1.1 PoniÅ¼sza ilustracja przedstawia fragment pliku danych Analysis.txt zawierajÄ…cego pewne bÅ‚Ä™dy, ktÃ³re naleÅ¼y naprawiÄ‡ na etapie importu danych. Po pierwsze brakuje w nim nazw zmiennych (choÄ‡ nie widaÄ‡ tego na rysunku). PoszczegÃ³lne kolumny nazywajÄ… siÄ™ nastÄ™pujÄ…co: season, size, speed, mxPH, mnO2, Cl, NO3, NH4, oPO4, PO4, Chla, a1, a2, a3, a4, a5, a6, a7. Naszym zadaniem jest import tego pliku z jednoczesnÄ… obsÅ‚ugÄ… brakÃ³w (braki danych sÄ… zakodowane przez XXXXXXX) oraz nadaniem nagÅ‚Ã³wkÃ³w kolumn. Plik Analisis.txt jest umieszczony w kagalogu data/. Z racji, Å¼e plik dotyczy glonÃ³w, to dane zapiszemy pod nazwÄ… algae. Rysunek 1.3: Fragment pliku danych Analisis.txt algae &lt;- import(&#39;data/Analysis.txt&#39;, header=F, dec=&#39;.&#39;, col.names=c(&#39;season&#39;,&#39;size&#39;,&#39;speed&#39;,&#39;mxPH&#39;,&#39;mnO2&#39;,&#39;Cl&#39;, &#39;NO3&#39;,&#39;NH4&#39;,&#39;oPO4&#39;,&#39;PO4&#39;,&#39;Chla&#39;,&#39;a1&#39;,&#39;a2&#39;, &#39;a3&#39;,&#39;a4&#39;,&#39;a5&#39;,&#39;a6&#39;,&#39;a7&#39;), na.strings=c(&#39;XXXXXXX&#39;)) head(algae) ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla ## 1 winter small medium 8.00 9.8 60.800 6.238 578.000 105.000 170.000 50.0 ## 2 spring small medium 8.35 8.0 57.750 1.288 370.000 428.750 558.750 1.3 ## 3 autumn small medium 8.10 11.4 40.020 5.330 346.667 125.667 187.057 15.6 ## 4 spring small medium 8.07 4.8 77.364 2.302 98.182 61.182 138.700 1.4 ## 5 autumn small medium 8.06 9.0 55.350 10.416 233.700 58.222 97.580 10.5 ## 6 winter small high 8.25 13.1 65.750 9.248 430.000 18.250 56.667 28.4 ## a1 a2 a3 a4 a5 a6 a7 ## 1 0.0 0.0 0.0 0.0 34.2 8.3 0.0 ## 2 1.4 7.6 4.8 1.9 6.7 0.0 2.1 ## 3 3.3 53.6 1.9 0.0 0.0 0.0 9.7 ## 4 3.1 41.0 18.9 0.0 1.4 0.0 1.4 ## 5 9.2 2.9 7.5 0.0 7.5 4.1 1.0 ## 6 15.1 14.6 1.4 0.0 22.5 12.6 2.9 summary(algae) ## season size speed mxPH ## Length:200 Length:200 Length:200 Min. :5.600 ## Class :character Class :character Class :character 1st Qu.:7.700 ## Mode :character Mode :character Mode :character Median :8.060 ## Mean :8.012 ## 3rd Qu.:8.400 ## Max. :9.700 ## NA&#39;s :1 ## mnO2 Cl NO3 NH4 ## Min. : 1.500 Min. : 0.222 Min. : 0.050 Min. : 5.00 ## 1st Qu.: 7.725 1st Qu.: 10.981 1st Qu.: 1.296 1st Qu.: 38.33 ## Median : 9.800 Median : 32.730 Median : 2.675 Median : 103.17 ## Mean : 9.118 Mean : 43.636 Mean : 3.282 Mean : 501.30 ## 3rd Qu.:10.800 3rd Qu.: 57.824 3rd Qu.: 4.446 3rd Qu.: 226.95 ## Max. :13.400 Max. :391.500 Max. :45.650 Max. :24064.00 ## NA&#39;s :2 NA&#39;s :10 NA&#39;s :2 NA&#39;s :2 ## oPO4 PO4 Chla a1 ## Min. : 1.00 Min. : 1.00 Min. : 0.200 Min. : 0.00 ## 1st Qu.: 15.70 1st Qu.: 41.38 1st Qu.: 2.000 1st Qu.: 1.50 ## Median : 40.15 Median :103.29 Median : 5.475 Median : 6.95 ## Mean : 73.59 Mean :137.88 Mean : 13.971 Mean :16.92 ## 3rd Qu.: 99.33 3rd Qu.:213.75 3rd Qu.: 18.308 3rd Qu.:24.80 ## Max. :564.60 Max. :771.60 Max. :110.456 Max. :89.80 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :12 ## a2 a3 a4 a5 ## Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 3.000 Median : 1.550 Median : 0.000 Median : 1.900 ## Mean : 7.458 Mean : 4.309 Mean : 1.992 Mean : 5.064 ## 3rd Qu.:11.375 3rd Qu.: 4.925 3rd Qu.: 2.400 3rd Qu.: 7.500 ## Max. :72.600 Max. :42.800 Max. :44.600 Max. :44.400 ## ## a6 a7 ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 0.000 Median : 1.000 ## Mean : 5.964 Mean : 2.495 ## 3rd Qu.: 6.925 3rd Qu.: 2.400 ## Max. :77.600 Max. :31.600 ## export(algae, file = &quot;data/algae.csv&quot;) Bibliografia "],
["przygotowanie-danych.html", "2 Przygotowanie danych 2.1 Identyfikacja brakÃ³w danych 2.2 ZastÄ™powanie brakÃ³w danych", " 2 Przygotowanie danych Dane, ktÃ³re importujemy z zewnÄ™trznego ÅºrÃ³dÅ‚a najczÄ™Å›ciej nie speÅ‚niajÄ… formatÃ³w obowiÄ…zujÄ…cych w R. CzÄ™sto zmienne zawierajÄ… niedopuszczalne znaki szczegÃ³lne, odstÄ™py w nazwach, powtÃ³rzone nazwy kolumn, nazwy zmiennych zaczynajÄ…ce siÄ™ od liczby, czy puste wiersze lub kolumny. Przed przystÄ…pieniem do analizy zbioru naleÅ¼y rozwaÅ¼yÄ‡ ewentualne poprawki nazw zmiennych, czy usuniÄ™cie pustych kolumn i wierszy. NiektÃ³rych czynnoÅ›ci moÅ¼na dokonaÄ‡ juÅ¼ na etapie importu danych, stosujÄ…c pewne pakiety oraz nowe funkcjonalnoÅ›ci Å›rodowiska RStudio. W wiÄ™kszoÅ›ci przypadkÃ³w uchroni nas to od Å¼mudnego przeksztaÅ‚cania typÃ³w zmiennych. OczywiÅ›cie wszystkie te czynnoÅ›ci czyszczenia danych moÅ¼na rÃ³wnieÅ¼ dokonaÄ‡ juÅ¼ po imporcie danych, za pomocÄ… odpowiednich komend R. ## przykÅ‚adowe niepoÅ¼Ä…dane nazwy zmiennych test_df &lt;- as.data.frame(matrix(rnorm(18),ncol = 6)) names(test_df) &lt;- c(&quot;hIgHlo&quot;, &quot;REPEAT VALUE&quot;, &quot;REPEAT VALUE&quot;, &quot;% successful (2009)&quot;, &quot;abc@!*&quot;, &quot;&quot;) test_df ## hIgHlo REPEAT VALUE REPEAT VALUE % successful (2009) abc@!* ## 1 1.0263219 -0.4260190 -1.2152640 0.1009963 0.6798836 ## 2 1.3175625 0.7178012 -0.4132225 1.6192768 0.4576477 ## 3 -0.2323977 -0.7498732 1.6017626 0.6285578 -0.4910620 ## ## 1 -0.4880386 ## 2 -0.4096234 ## 3 0.8594420 ## do poprawy nazw zmiennych uÅ¼yjemy funkcji make.names names(test_df) &lt;- make.names(names(test_df)) test_df ## hIgHlo REPEAT.VALUE REPEAT.VALUE X..successful..2009. abc... ## 1 1.0263219 -0.4260190 -1.2152640 0.1009963 0.6798836 ## 2 1.3175625 0.7178012 -0.4132225 1.6192768 0.4576477 ## 3 -0.2323977 -0.7498732 1.6017626 0.6285578 -0.4910620 ## X ## 1 -0.4880386 ## 2 -0.4096234 ## 3 0.8594420 Efekt koÅ„cowy choÄ‡ skuteczny, to nie jest zadowalajÄ…cy. Czyszczenia nazw zmiennych moÅ¼na teÅ¼ dokonaÄ‡ stosujÄ…c funkcjÄ™ clean_names pakietu janitor (Firke 2018). Pozwala on rÃ³wnieÅ¼ na usuwanie pustych wierszy i kolumn, znajdowanie zduplikowanych rekordÃ³w, itp. library(janitor) test_df %&gt;% # aby na staÅ‚e zmieniÄ‡ nazwy zmiennych trzeba podstawienia clean_names() ## h_ig_hlo repeat_value repeat_value_2 x_successful_2009 abc ## 1 1.0263219 -0.4260190 -1.2152640 0.1009963 0.6798836 ## 2 1.3175625 0.7178012 -0.4132225 1.6192768 0.4576477 ## 3 -0.2323977 -0.7498732 1.6017626 0.6285578 -0.4910620 ## x ## 1 -0.4880386 ## 2 -0.4096234 ## 3 0.8594420 # przykÅ‚adowe dane x &lt;- data.frame(w1=c(1,4,2,NA),w2=c(NA,2,3,NA), w3=c(1,NA,1,NA)) x ## w1 w2 w3 ## 1 1 NA 1 ## 2 4 2 NA ## 3 2 3 1 ## 4 NA NA NA x %&gt;% remove_empty(&quot;rows&quot;) ## w1 w2 w3 ## 1 1 NA 1 ## 2 4 2 NA ## 3 2 3 1 2.1 Identyfikacja brakÃ³w danych Zanim usuniemy jakiekolwiek braki w zbiorze, powinniÅ›my je najpierw zidentyfikowaÄ‡, okreÅ›liÄ‡ ich charakter, a dopiero potem ewentualnie podjÄ…Ä‡ decyzjÄ™ o uzupeÅ‚nianiu brakÃ³w. algae &lt;- rio::import(&quot;data/algae.csv&quot;) # najproÅ›ciej jest wywoÅ‚aÄ‡ summary summary(algae) ## season size speed mxPH ## Length:200 Length:200 Length:200 Min. :5.600 ## Class :character Class :character Class :character 1st Qu.:7.700 ## Mode :character Mode :character Mode :character Median :8.060 ## Mean :8.012 ## 3rd Qu.:8.400 ## Max. :9.700 ## NA&#39;s :1 ## mnO2 Cl NO3 NH4 ## Min. : 1.500 Min. : 0.222 Min. : 0.050 Min. : 5.00 ## 1st Qu.: 7.725 1st Qu.: 10.981 1st Qu.: 1.296 1st Qu.: 38.33 ## Median : 9.800 Median : 32.730 Median : 2.675 Median : 103.17 ## Mean : 9.118 Mean : 43.636 Mean : 3.282 Mean : 501.30 ## 3rd Qu.:10.800 3rd Qu.: 57.824 3rd Qu.: 4.446 3rd Qu.: 226.95 ## Max. :13.400 Max. :391.500 Max. :45.650 Max. :24064.00 ## NA&#39;s :2 NA&#39;s :10 NA&#39;s :2 NA&#39;s :2 ## oPO4 PO4 Chla a1 ## Min. : 1.00 Min. : 1.00 Min. : 0.200 Min. : 0.00 ## 1st Qu.: 15.70 1st Qu.: 41.38 1st Qu.: 2.000 1st Qu.: 1.50 ## Median : 40.15 Median :103.29 Median : 5.475 Median : 6.95 ## Mean : 73.59 Mean :137.88 Mean : 13.971 Mean :16.92 ## 3rd Qu.: 99.33 3rd Qu.:213.75 3rd Qu.: 18.308 3rd Qu.:24.80 ## Max. :564.60 Max. :771.60 Max. :110.456 Max. :89.80 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :12 ## a2 a3 a4 a5 ## Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 3.000 Median : 1.550 Median : 0.000 Median : 1.900 ## Mean : 7.458 Mean : 4.309 Mean : 1.992 Mean : 5.064 ## 3rd Qu.:11.375 3rd Qu.: 4.925 3rd Qu.: 2.400 3rd Qu.: 7.500 ## Max. :72.600 Max. :42.800 Max. :44.600 Max. :44.400 ## ## a6 a7 ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 0.000 Median : 1.000 ## Mean : 5.964 Mean : 2.495 ## 3rd Qu.: 6.925 3rd Qu.: 2.400 ## Max. :77.600 Max. :31.600 ## ## wyÅ›wietl niekompletne wiersze algae[!complete.cases(algae),] %&gt;% head() ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 28 autumn small high 6.8 11.1 9.00 0.630 20 4.0 NA 2.7 30.3 1.9 0.0 ## 38 spring small high 8.0 NA 1.45 0.810 10 2.5 3.0 0.3 75.8 0.0 0.0 ## 48 winter small low NA 12.6 9.00 0.230 10 5.0 6.0 1.1 35.5 0.0 0.0 ## 55 winter small high 6.6 10.8 NA 3.245 10 1.0 6.5 NA 24.3 0.0 0.0 ## 56 spring small medium 5.6 11.8 NA 2.220 5 1.0 1.0 NA 82.7 0.0 0.0 ## 57 autumn small medium 5.7 10.8 NA 2.550 10 1.0 4.0 NA 16.8 4.6 3.9 ## a4 a5 a6 a7 ## 28 0.0 2.1 1.4 2.1 ## 38 0.0 0.0 0.0 0.0 ## 48 0.0 0.0 0.0 0.0 ## 55 0.0 0.0 0.0 0.0 ## 56 0.0 0.0 0.0 0.0 ## 57 11.5 0.0 0.0 0.0 ## policz niekompletne wiersze nrow(algae[!complete.cases(algae),]) ## [1] 16 ## sprawdzenie liczby brakÃ³w w wierszach apply(algae, 1, function(x) sum(is.na(x))) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ## 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## 2 2 2 2 2 2 2 6 1 0 0 0 0 0 0 0 0 0 ## 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 ## 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ## 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ## 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 199 200 ## 6 0 Wiele ciekawych funkcji do eksploracji danych znajduje siÄ™ w pakiecie DMwR (Torgo 2013), ktÃ³ry zostaÅ‚ przygotowany przy okazji publikacji ksiÄ…Å¼ki Data Mining with R. ## poszukiwanie wierszy zawierajÄ…cych wiele brakÃ³w ## w tym przypadku prÃ³g wyÅ›wietlania ustawiony jest na 0.2 ## czyli 20% wszystkich kolumn library(DMwR) manyNAs(algae) ## 62 199 ## 62 199 ## tworzenie zbioru pozbawionego wierszy zawierajÄ…cych wiele brakÃ³w algae2 &lt;- algae[-manyNAs(algae), ] ## sprawdzamy liczbÄ™ wybrakowanych wierszy ktÃ³re pozostaÅ‚y nrow(algae2[!complete.cases(algae2),]) ## [1] 14 ## usuwamy wszystkie wiersze z brakami algae3 &lt;- na.omit(algae) ## wyÅ›wietl wiersze z brakami algae3[!complete.cases(algae3),] %&gt;% head() ## [1] season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 ## [11] Chla a1 a2 a3 a4 a5 a6 a7 ## &lt;0 rows&gt; (or 0-length row.names) ## liczba pozostaÅ‚ych wybrakowanych wierszy nrow(algae3[!complete.cases(algae3),]) ## [1] 0 ## moÅ¼na oczywiÅ›cie teÅ¼ rÄ™cznie usuwaÄ‡ wiersze (nie polecam) algae4 &lt;- algae[-c(62,199),] MoÅ¼na teÅ¼ zbudowaÄ‡ funkcjÄ™, ktÃ³ra bÄ™dzie usuwaÅ‚a braki danych wg naszego upodobania. ## najpierw budujemy funkcjÄ™ i jÄ… kompilujemy aby R mÃ³gÅ‚ ja stosowaÄ‡ ## parametr prog ustala prÃ³g odciÄ™cia wierszy czysc.dane &lt;- function(dt, prog = 0){ licz.braki &lt;- apply(dt, 1, function(x) sum(is.na(x))) czyste.dt &lt;- dt[!(licz.braki/ncol(dt)&gt;prog), ] return(czyste.dt) } ## potem jÄ… moÅ¼emy stosowaÄ‡ algae4 &lt;- czysc.dane(algae) nrow(algae4[!complete.cases(algae4),]) ## [1] 0 ## czyÅ›cimy wiersze, ktÃ³rych liczba brakÃ³w przekracza 20% wszystkich kolumn algae5 &lt;- czysc.dane(algae, prog = 0.2) nrow(algae5[!complete.cases(algae5),]) ## [1] 14 Bardzo ciekawym narzÄ™dziem do znajdowania brakÃ³w danych jest funkcja md.pattern pakietu mice (van Buuren and Groothuis-Oudshoorn 2018). Wskazuje on ile brakÃ³w wystÄ™puje w ramach kaÅ¼dej zmiennej. library(mice) md.pattern(algae) Rysunek 2.1: Na czerwono zaznaczone sÄ… zmienne, ktÃ³re zwierajÄ… braki danych. Liczba w wierszu po lewej stronie wykresu wskazuje ile wierszy w bazie ma danÄ… charakterystykÄ™, a liczba po prawej oznacza ile zmiennych byÅ‚o wybrakowanych ## season size speed a1 a2 a3 a4 a5 a6 a7 mxPH mnO2 NO3 NH4 oPO4 PO4 Cl ## 184 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 ## 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 ## 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 ## 0 0 0 0 0 0 0 0 0 0 1 2 2 2 2 2 10 ## Chla ## 184 1 0 ## 3 0 1 ## 1 1 1 ## 7 0 2 ## 1 1 1 ## 1 0 6 ## 1 1 1 ## 1 0 6 ## 1 1 1 ## 12 33 2.2 ZastÄ™powanie brakÃ³w danych ZastÄ™powanie brakÃ³w danych (zwane takÅ¼e imputacjÄ… danych) jest kolejnym etapem procesu przygotowania danych do analiz. Nie moÅ¼na jednak wyrÃ³Å¼niÄ‡ uniwersalnego sposobu zastÄ™powania brakÃ³w dla wszystkich moÅ¼liwych sytuacji. WÅ›rÃ³d statystykÃ³w panuje przekonanie, Å¼e w przypadku wystÄ…pienia brakÃ³w danych moÅ¼na zastosowaÄ‡ trzy strategie: nic nie robiÄ‡ z brakami - co wydaje siÄ™ niedorzeczne ale wcale takie nie jest, poniewaÅ¼ istnieje wiele modeli statystycznych (np. drzewa decyzyjne), ktÃ³re Å›wietnie radzÄ… sobie w sytuacji brakÃ³w danych. Niestety nie jest to sposÃ³b, ktÃ³ry moÅ¼na stosowaÄ‡ zawsze, poniewaÅ¼ sÄ… rÃ³wnieÅ¼ modele wymagajÄ…ce kompletnoÅ›ci danych jak na przykÅ‚ad sieci neuronowe. usuwaÄ‡ braki wierszami9 - to metoda, ktÃ³ra jest stosowana domyÅ›lnie w przypadku kiedy twÃ³rca modelu nie zadecyduje o innym sposobie obsÅ‚ugi luk. Metoda ta ma swojÄ… niewÄ…tpliwÄ… zaletÄ™ w postaci jasnej i prostej procedury, ale szczegÃ³lnie w przypadku niewielkich zbiorÃ³w moÅ¼e skutkowaÄ‡ obciÄ…Å¼eniem estymatorÃ³w. Nie wiemy bowiem jaka wartoÅ›Ä‡ faktycznie jest przypisana danej cesze. JeÅ›li jest to wartoÅ›Ä‡ bliska np. Å›redniej, to nie wpÅ‚ynie znaczÄ…co na obciÄ…Å¼enie estymatora wartoÅ›ci oczekiwanej. W przypadku, gdy rÃ³Å¼ni siÄ™ ona znacznie od Å›redniej tej cechy, to estymator moÅ¼e juÅ¼ wykazywaÄ‡ obciÄ…Å¼enie. Jego wielkoÅ›Ä‡ zaleÅ¼y rÃ³wnieÅ¼ od liczby usuniÄ™tych elementÃ³w. Nie jest zalecane usuwanie wielu wierszy ze zbioru danych i na podstawie okrojonego zbioru wyciÄ…ganie wnioskÃ³w o populacji, poniewaÅ¼ prÃ³ba jest wÃ³wczas znaczÄ…co inna niÅ¼ populacja. Dodatkowo jeÅ›li estymatory sÄ… wyznaczane na podstawie zbioru wyraÅºnie mniej licznego, to precyzja estymatorÃ³w wyraÅ¼ona wariancjÄ… spada. ReasumujÄ…c, jeÅ›li liczba wierszy z brakujÄ…cymi danymi jest niewielka w stosunku do caÅ‚ego zbioru, to usuwanie wierszy jest sensownym rozwiÄ…zaniem. uzupeÅ‚nianie brakÃ³w - to procedura polegajÄ…ca na zastÄ™powaniu brakÃ³w rÃ³Å¼nymi technikami. Jej niewÄ…tpliwÄ… zaletÄ… jest fakt posiadania kompletnych danych bez koniecznoÅ›ci usuwania wierszy. Niestety wiÄ…Å¼e siÄ™ to rÃ³wnieÅ¼ z pewnymi wadami. ZbiÃ³r posiadajÄ…cy wiele brakÃ³w uzupeÅ‚nianych nawet bardzo wyrafinowanymi metodami moÅ¼e cechowaÄ‡ siÄ™ zaniÅ¼onÄ… wariancjÄ… poszczegÃ³lnych cech oraz tzw. przeuczeniem10. UzupeÅ‚nianie Å›redniÄ… - braki w zakresie danej zmiennej uzupeÅ‚niamy Å›redniÄ… tej zmiennej przypadkÃ³w uzupeÅ‚nionych. algae[is.na(algae$mxPH), ] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 a4 a5 ## 48 winter small low NA 12.6 9 0.23 10 5 6 1.1 35.5 0 0 0 0 ## a6 a7 ## 48 0 0 m &lt;- mean(algae$mxPH, na.rm = T) algae[is.na(algae$mxPH), &quot;mxPH&quot;] &lt;- m algae[is.na(algae$mxPH), ] ## [1] season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 ## [11] Chla a1 a2 a3 a4 a5 a6 a7 ## &lt;0 rows&gt; (or 0-length row.names) UzupeÅ‚nianie medianÄ… - braki w zakresie danej zmiennej uzupeÅ‚niamy medianÄ… tej zmiennej przypadkÃ³w uzupeÅ‚nionych. algae %&gt;% filter(is.na(Chla)) %&gt;% head ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 1 winter small high 6.6 10.8 NA 3.245 10 1 6.5 NA 24.3 0.0 0.0 ## 2 spring small medium 5.6 11.8 NA 2.220 5 1 1.0 NA 82.7 0.0 0.0 ## 3 autumn small medium 5.7 10.8 NA 2.550 10 1 4.0 NA 16.8 4.6 3.9 ## 4 spring small high 6.6 9.5 NA 1.320 20 1 6.0 NA 46.8 0.0 0.0 ## 5 summer small high 6.6 10.8 NA 2.640 10 2 11.0 NA 46.9 0.0 0.0 ## 6 autumn small medium 6.6 11.3 NA 4.170 10 1 6.0 NA 47.1 0.0 0.0 ## a4 a5 a6 a7 ## 1 0.0 0 0.0 0 ## 2 0.0 0 0.0 0 ## 3 11.5 0 0.0 0 ## 4 28.8 0 0.0 0 ## 5 13.4 0 0.0 0 ## 6 0.0 0 1.2 0 algae[is.na(algae$Chla), &quot;Chla&quot;] &lt;- median(algae$Chla, na.rm = T) WypeÅ‚nianie zmiennych typu wyliczeniowego, logicznego lub znakowego odbywa siÄ™ najczÄ™Å›ciej przez dobranie w miejsce brakujÄ…cej wartoÅ›ci, elementu powtarzajÄ…cego siÄ™ najczÄ™Å›ciej wÅ›rÃ³d obiektÃ³w obserwowanych. W pakiecie DMwR istnieje funkcja centralImputation, ktÃ³ra wypeÅ‚nia braki wartoÅ›ciÄ… centralnÄ… (w przypadku zmiennych typu liczbowego - medianÄ…, a dla wartoÅ›ci logicznych, wyliczeniowych lub tekstowych - modÄ…). algae[48, &quot;season&quot;] ## [1] &quot;winter&quot; algae[48, &quot;season&quot;] &lt;- NA algae.uzup &lt;- centralImputation(algae) algae.uzup[48,] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 winter small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 Jeszcze innym sposobem imputacji danych sÄ… algorytmy oparte o metodÄ™ \\(k\\)-najbliÅ¼szych sÄ…siadÃ³w. Algorytm opiera siÄ™ na prostej zasadzie, uzupeÅ‚niania brakujÄ…cych wartoÅ›ci medianÄ… (w przypadku zmiennych iloÅ›ciowych) lub modÄ… (w przypadku zmiennych jakoÅ›ciowych) elementÃ³w, ktÃ³re sÄ… \\(k\\)-tymi najbliÅ¼szymi sÄ…siadami w metryce \\[\\begin{equation}\\label{knn} d(x,y)=\\sqrt{\\sum_{i=1}^{p}\\delta_i(x_i,y_i)}, \\end{equation}\\] gdzie \\(\\delta_i\\) jest odlegÅ‚oÅ›ciÄ… pomiÄ™dzy dwoma elementami ze wzglÄ™du na \\(i\\)-tÄ… cech, okreÅ›lonÄ… nastÄ™pujÄ…co \\[\\begin{equation}\\label{metryka} \\delta_i(v_1, v_2)=\\begin{cases} 1,&amp; \\text{jeÅ›li zmienna jest jakoÅ›ciowa i }v_1\\neq v_2\\\\ 0,&amp; \\text{jeÅ›li zmienna jest jakoÅ›ciowa i }v_1=v_2\\\\ (v_1-v_2)^2,&amp; \\text{jeÅ›li zmienna jest iloÅ›ciowa.} \\end{cases} \\end{equation}\\] OdlegÅ‚oÅ›ci sÄ… mierzone dla zmiennych standaryzowanych. Istnieje teÅ¼ odmiana z wagami, ktÃ³re malejÄ… wraz ze wzrostem odlegÅ‚oÅ›ci pomiÄ™dzy sÄ…siadem a uzupeÅ‚nianym elementem (np. \\(w(d)=\\exp(d)\\)). algae[48, ] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 &lt;NA&gt; small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 algae &lt;- algae %&gt;% mutate_if(is.character, as.factor) algae.uzup &lt;- knnImputation(algae, k = 5, scale = F, meth = &quot;median&quot;) algae.uzup[48,] ## season size speed mxPH mnO2 Cl NO3 NH4 oPO4 PO4 Chla a1 a2 a3 ## 48 summer small low 8.011734 12.6 9 0.23 10 5 6 1.1 35.5 0 0 ## a4 a5 a6 a7 ## 48 0 0 0 0 IstniejÄ… rÃ³wnieÅ¼ duÅ¼o bardziej zÅ‚oÅ¼one algorytmy imputacji danych oparte na bardziej wyrafinowanych technikach, takich jak: predykcja modelami liniowymi, nieliniowymi, analiza dyskryminacyjna, drzewa klasyfikacyjne. Dwa najbardziej znane pakiety zawierajÄ…ce funkcje do imputacji w sposÃ³b zÅ‚oÅ¼ony, to Amelia i mice. Imputacja danych z zastosowaniem pakietu mice wymaga podjÄ™cia kilku decyzji przed przystÄ…pieniem do uzupeÅ‚niania danych: Czy dane sÄ… MAR (ang. Missing At Random) czy MNAR (ang. Missing Not At Random), co oznacza, Å¼e musimy siÄ™ zastanowiÄ‡ jakie mogÅ‚y byÄ‡ ÅºrÃ³dÅ‚a brakÃ³w danych, przypadkowe czy systematyczne? NaleÅ¼y siÄ™ zdecydowaÄ‡ na formÄ™ imputacji, okreÅ›lajÄ…c strukturÄ™ zaleÅ¼noÅ›ci pomiÄ™dzy cechami oraz rozkÅ‚ad bÅ‚Ä™du danej cechy? WybraÄ‡ zbiÃ³r danych, ktÃ³ry posÅ‚uÅ¼y nam za predyktory w imputacji (nie mogÄ… zawieraÄ‡ brakÃ³w). OkreÅ›lenie, ktÃ³re niepeÅ‚ne zmienne sÄ… funkcjami innych wybrakowanych zmiennych. OkreÅ›liÄ‡ w jakiej kolejnoÅ›ci dane bÄ™dÄ… imputowane. OkreÅ›liÄ‡ parametry startowe imputacji (liczbÄ™ iteracji, warunek zbieÅ¼noÅ›ci). OkreÅ›liÄ‡ liczÄ™ imputowanych zbiorÃ³w. Ad 1. WyrÃ³Å¼niamy nastÄ™pujÄ…ce rodzaje brakÃ³w danych: MCAR (ang. Missing Completely At Random) - z definicji to braki, ktÃ³rych pojawienie siÄ™ jest kompletnie losowe. PrzykÅ‚adowo gdy osoba poproszona o wypeÅ‚nienie wieku w ankiecie bÄ™dzie rzucaÄ‡ monetÄ… czy wypeÅ‚niÄ‡ tÄ… zmiennÄ…. MAR - oznacza, Å¼e obserwowane wartoÅ›ci i wybrakowane majÄ… inne rozkÅ‚ady ale da siÄ™ je oszacowaÄ‡ na podstawie danych obserwowanych. PrzykÅ‚adowo ciÅ›nienie tÄ™tnicze u osÃ³b, ktÃ³re nie wypeÅ‚niÅ‚y tej wartoÅ›ci jest wyÅ¼sze niÅ¼ u osÃ³b, ktÃ³re wpisaÅ‚y swoje ciÅ›nienie. Okazuje siÄ™, Å¼e osoby starsze z nadciÅ›nieniem nie wypeÅ‚niaÅ‚y ankiety w tym punkcie. MNAR - jeÅ›li nie jest speÅ‚niony warunek MCAR i MAR, wÃ³wczas brak ma charakter nielosowy. PrzykÅ‚adowo respondenci osiÄ…gajÄ…cy wyÅ¼sze zarobki sukcesywnie nie wypeÅ‚niajÄ… pola â€œzarobkiâ€ i dodatkowo nie ma w ankiecie zmiennych, ktÃ³re pozwoliÅ‚yby nam ustaliÄ‡, jakie to osoby. Ad 2. Decyzja o algorytmie imputacji wynika bezpoÅ›rednio ze skali w jakiej jest mierzona dana zmienna. Ze wzglÄ™du na rodzaj cechy uÅ¼ywaÄ‡ bÄ™dziemy nastÄ™pujÄ…cych metod: Tabela 2.1: Zestaw metod imputacji danych stosowanych w pakiecie mice method type description pmm any Predictive.mean.matching midastouch any Weighted predictive mean matching sample any Random sample from observed values cart any Classification and regression trees rf any Random forest imputations mean numeric Unconditional mean imputation norm numeric Bayesian linear regression norm.nob numeric Linear regression ignoring model error norm.boot numeric Linear regression using bootstrap norm.predict numeric Linear regression, predicted values quadratic numeric Imputation of quadratic terms ri numeric Random indicator for nonignorable data logreg binary Logistic regression logreg.boot binary Logistic regression with bootstrap polr ordered Proportional odds model polyreg unordered Polytomous logistic regression lda unordered Linear discriminant analysis 2l.norm numeric Level-1 normal heteroscedastic 2l.lmer numeric Level-1 normal homoscedastic, lmer 2l.pan numeric Level-1 normal homoscedastic, pan 2l.bin binary Level-1 logistic, glmer 2lonly.mean numeric Level-2 class mean 2lonly.norm numeric Level-2 class normal 2lonly.pmm any Level-2 class predictive mean matching KaÅ¼dy z czterech typÃ³w danych ma swÃ³j domyÅ›lny algorytm przeznaczony do imputacji: zmienna iloÅ›ciowa - pmm zmienna dychotomiczna (stany 0 lub 1) - logreg zmienna typu wyliczeniowego (nieuporzÄ…dkowana) - polyreg zmienna typu wyliczeniowego (uporzÄ…dkowana) - polr NiewÄ…tpliwÄ… zaletÄ… metody pmm jest to, Å¼e wartoÅ›ci imputowane sÄ… ograniczone jedynie do obserwowanych wartoÅ›ci. Metody norm i norm.nob uzupeÅ‚niajÄ… brakujÄ…ce wartoÅ›ci w oparciu o model liniowy. SÄ… one szybkie i efektywne w przypadku gdy reszty modelu sÄ… zbliÅ¼one rozkÅ‚adem do normalnoÅ›ci. Druga z tych technik nie bierze pod uwagÄ™ niepewnoÅ›ci zwiÄ…zanej z modelem imputujÄ…cym. Metoda 2L.norm opiera siÄ™ na dwupoziomowym heterogenicznym modelu liniowym (skupienia sÄ… wÅ‚Ä…czone jako efekt do modelu). Technika polyreg korzysta z funkcji multinom pakietu nnet tworzÄ…cej model wielomianowy. polr opiera siÄ™ o proporcjonalny model logitowy z pakietu MASS. lda to model dyskryminacyjny klasyfikujÄ…cy obiekty na podstawie prawdopodobieÅ„stw a posteriori. Metoda sample zastÄ™puje braki losowa wybranymi wartoÅ›ciami spoÅ›rÃ³d wartoÅ›ci obserwowanych. Ad 3. Do ustalenia predyktorÃ³w w modelu mice sÅ‚uÅ¼y funkcja predictorMatrix. Po pierwsze wyÅ›wietla ona domyÅ›lny ukÅ‚ad predyktorÃ³w wÅ‚Ä…czanych do modelu. MoÅ¼na go dowolnie zmieniÄ‡ i podstawiÄ‡ do modelu imputujÄ…cego dane parametrem predictorMatrix. Zera wystÄ™pujÄ…ce w kolejnych wierszach macierzy predyktorÃ³w oznaczajÄ… pominiÄ™cie tej zmiennej przy imputacji innej zmiennej. JeÅ›li dodatkowo chcemy by jakaÅ› zmienna nie byÅ‚a imputowana, to oprÃ³cz usuniÄ™cia jej z listy predyktorÃ³w, naleÅ¼y wymazaÄ‡ jÄ… z listy metod predykcji (method). OgÃ³lne zalecenia co do tego jakie zmienne stosowaÄ‡ jako predyktory jest takie, Å¼eby braÄ‡ ich jak najwiÄ™cej. Spowoduje to, Å¼e bardziej prawdopodobny staje siÄ™ brak typu MAR a nie MNAR. Z drugiej jednak strony, nierzadko zbiory zawierajÄ… olbrzymiÄ… liczbÄ™ zmiennych i wÅ‚Ä…czanie ich wszystkich do modelu imputujÄ…cego nie bÄ™dzie miaÅ‚o sensu. Zalecenia doboru zmiennych sÄ… nastÄ™pujÄ…ce: weÅº wszystkie te zmienne, ktÃ³re sÄ… wÅ‚Ä…czane do modelu wÅ‚aÅ›ciwego, czyli tego za pomocÄ… ktÃ³rego chcesz poznaÄ‡ strukturÄ™ zaleÅ¼noÅ›ci; czasem do modelu imputujÄ…cego naleÅ¼y teÅ¼ wÅ‚Ä…czyÄ‡ interakcje zmiennych z modelu wÅ‚aÅ›ciwego; dodaj zmienne, ktÃ³re mogÄ… mieÄ‡ wpÅ‚yw na wybrakowane cechy; wÅ‚Ä…cz zmienne istotnie podnoszÄ…ce poziom wyjaÅ›nionej wariancji modelu; na koniec usuÅ„ te zmienne spoÅ›rÃ³d predyktorÃ³w, ktÃ³re same zawierajÄ… zbyt wiele brakÃ³w. Ad 4-7. Decyzje podejmowane w tych punktach zaleÅ¼Ä… istotnie od analizowanego zbioru i bÄ™dÄ… przedmiotem oddzielnych analiz w kontekÅ›cie rozwaÅ¼anych zbiorÃ³w i zadaÅ„. PrzykÅ‚ad 2.1 Dokonamy imputacji zbioru airquality z wykorzystaniem pakietÃ³w mice i VIM (Templ et al. 2019) data &lt;- airquality summary(data) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## # tworzymy dodatkowe braki danych data[4:10,3] &lt;- rep(NA,7) data[1:5,4] &lt;- NA summary(data) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :57.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:73.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.806 Mean :78.28 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 NA&#39;s :7 NA&#39;s :5 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## md.pattern(data) ## Month Day Temp Solar.R Wind Ozone ## 104 1 1 1 1 1 1 0 ## 34 1 1 1 1 1 0 1 ## 3 1 1 1 1 0 1 1 ## 1 1 1 1 1 0 0 2 ## 4 1 1 1 0 1 1 1 ## 1 1 1 1 0 1 0 2 ## 1 1 1 1 0 0 1 2 ## 3 1 1 0 1 1 1 1 ## 1 1 1 0 1 0 1 2 ## 1 1 1 0 0 0 0 4 ## 0 0 5 7 7 37 56 Do ilustracji brakÃ³w danych moÅ¼na zastosowaÄ‡ funkcje pakietu VIM. library(VIM) aggr(data, numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7) ## ## Variables sorted by number of missings: ## Variable Count ## Ozone 0.24183007 ## Solar.R 0.04575163 ## Wind 0.04575163 ## Temp 0.03267974 ## Month 0.00000000 ## Day 0.00000000 Tak przedstawia siÄ™ wykres rozrzutu zmiennych Ozone i Solar.R z uwzglÄ™dnieniem poÅ‚oÅ¼enia brakÃ³w danych. marginplot(data[c(1,2)]) Dokonamy imputacji metodÄ… pmm. tempData &lt;- mice(data, maxit=50, meth=&#39;pmm&#39;, seed=44, printFlag = F) summary(tempData) ## Class: mids ## Number of multiple imputations: 5 ## Imputation methods: ## Ozone Solar.R Wind Temp Month Day ## &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## Ozone Solar.R Wind Temp Month Day ## Ozone 0 1 1 1 1 1 ## Solar.R 1 0 1 1 1 1 ## Wind 1 1 0 1 1 1 ## Temp 1 1 1 0 1 1 ## Month 1 1 1 1 0 1 ## Day 1 1 1 1 1 0 PoniewaÅ¼, funkcja mice domyÅ›lnie dokonuje 5 kompletnych imputacji, moÅ¼emy siÄ™ przekonaÄ‡ jak bardzo rÃ³Å¼niÄ… siÄ™ poszczegÃ³lne imputacje i zdecydowaÄ‡ siÄ™ na jednÄ… z nich. head(tempData$imp$Ozone) ## 1 2 3 4 5 ## 5 28 29 20 18 45 ## 10 23 13 9 13 12 ## 25 18 14 18 14 6 ## 26 12 37 1 20 28 ## 27 23 9 13 12 13 ## 32 45 41 20 23 46 Ostatecznie imputacji dokonujemy wybierajÄ…c jeden z zestawÃ³w danych uzupeÅ‚niajÄ…cych (np. pierwszy). completedData &lt;- mice::complete(tempData, 1) summary(completedData) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :57.00 ## 1st Qu.: 18.00 1st Qu.:115.0 1st Qu.: 7.400 1st Qu.:73.00 ## Median : 30.00 Median :207.0 Median : 9.700 Median :79.00 ## Mean : 41.46 Mean :185.8 Mean : 9.814 Mean :78.24 ## 3rd Qu.: 61.00 3rd Qu.:259.0 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 Za pomocÄ… funkcji pakietu mice moÅ¼emy rÃ³wnieÅ¼ przedstawiÄ‡ graficznie gdzie i jak zostaÅ‚y uzupeÅ‚nione dane. densityplot(tempData, ~Ozone+Solar.R+Wind+Temp) stripplot(tempData, Ozone+Solar.R+Wind+Temp~.imp, pch = 20, cex = 1.2) Bibliografia "],
["podzia-metod-data-mining.html", "3 PodziaÅ‚ metod data mining 3.1 Rodzaje wnioskowania 3.2 Modele regresyjne 3.3 Modele klasyfikacyjne 3.4 Modele grupujÄ…ce", " 3 PodziaÅ‚ metod data mining 3.1 Rodzaje wnioskowania Data mining to zestaw metod pozyskiwania wiedzy na podstawie danych. OwÄ… wiedzÄ™ zdobywamy w procesie wnioskowania na podstawie modeli. Wnioskowanie moÅ¼emy podzieliÄ‡ na dedukcyjne i indukcyjne. I tak z wnioskowaniem dedukcyjnym mamy do czynienia wÃ³wczas, gdy na podstawie obecnego stanu wiedzy potrafimy odpowiedzieÄ‡ na postawione pytanie dotyczÄ…ce nowej wiedzy, stosujÄ…c reguÅ‚y wnioskowania. O wnioskowaniem indukcyjnym powiemy, Å¼e jest to metoda pozyskiwania wiedzy na podstawie informacji ze zbioru uczÄ…cego. Znajduje ono szerokie zastosowanie w data mining i charakteryzuje siÄ™ omylnoÅ›ciÄ…, poniewaÅ¼ nawet najlepiej nauczony model na zbiorze uczÄ…cym nie zapewnia nam prawdziwoÅ›ci odpowiedzi w przypadku nowych danych, a jedynie je uprawdopodabnia. EsencjÄ… wnioskowania indukcyjnego w zakresie data mining, jest poszukiwanie na podstawie danych uczÄ…cych modelu charakteryzujÄ…cego siÄ™ najlepszymi wÅ‚aÅ›ciwoÅ›ciami predykcyjnymi i dajÄ…cego siÄ™ zastosowaÄ‡ do zupeÅ‚nie nowego zbioru danych. KaÅ¼dy proces uczenia z wykorzystaniem wnioskowania indukcyjnego skÅ‚ada siÄ™ z nastÄ™pujÄ…cych elementÃ³w. 3.1.1 Dziedzina Dziedzina to zbiÃ³r wszystkich obiektÃ³w pozostajÄ…cych w zainteresowaniu badacza, bÄ™dÄ…cych przedmiotem wnioskowania, oznaczana najczÄ™Å›ciej przez \\(X\\). PrzykÅ‚adowo mogÄ… to byÄ‡ zbiory osÃ³b, transakcji, urzÄ…dzeÅ„, instytucji, itp. 3.1.2 Obserwacja KaÅ¼dy element dziedziny \\(x\\in X\\) nazywamy obserwacjÄ…. ObserwacjÄ… nazywaÄ‡ bÄ™dziemy zarÃ³wno rekordy danych ze zbioru uczÄ…cego, jak i ze zbioru testowego. 3.1.3 Atrybuty obserwacji KaÅ¼dy obiekt z dziedziny \\(x\\in X\\) moÅ¼na opisaÄ‡ zestawem cech (atrybutÃ³w), ktÃ³re w notacji matematycznej oznaczymy przez \\(a:X\\to A\\), gdzie \\(A\\) jest przestrzeniÄ… wartoÅ›ci atrybutÃ³w. KaÅ¼da obserwacja \\(x\\) posiadajÄ…ca \\(k\\) cech da siÄ™ wyraziÄ‡ wektorowo jako \\((a_1(x), a_2(x), \\ldots, a_k(x))\\). Dla wiÄ™kszoÅ›ci algorytmÃ³w uczenia maszynowego wyrÃ³Å¼nia siÄ™ trzy typy atrybutÃ³w: nominalne - posiadajÄ…ce skoÅ„czonÄ… liczbÄ™ stanÃ³w, ktÃ³re posiadajÄ… porzÄ…dku; porzÄ…dkowe - posiadajÄ…ce skoÅ„czonÄ… liczbÄ™ stanÃ³w z zachowaniem porzÄ…dku; ciÄ…gÅ‚e - przyjmujÄ…ce wartoÅ›ci numeryczne. CzÄ™sto jeden z atrybutÃ³w speÅ‚nia specjalnÄ… rolÄ™, poniewaÅ¼ stanowi realizacjÄ™ cechy, ktÃ³rÄ… traktujemy jako wyjÅ›ciowÄ… (ang. target value attribute). W tym przypadku powiemy o nadzorowanym uczeniu maszynowym. JeÅ›li zmiennej wyjÅ›ciowej nie ma dziedzinie, to mÃ³wimy o nienadzorowanym uczeniu maszynowym. 3.1.4 ZbiÃ³r uczÄ…cy Zbiorem uczÄ…cym \\(T\\) (ang. training set) nazywamy podzbiÃ³r \\(D\\) dziedziny \\(X\\) (czyli \\(T\\subseteq D\\subseteq X\\)), gdzie zbiÃ³r \\(D\\) stanowi ogÃ³Å‚ dostÄ™pnych obserwacji z dziedziny \\(X\\). ZbiÃ³r uczÄ…cy zawiera informacje dotyczÄ…ce badanego zjawiska, na podstawie ktÃ³rych, dokonuje siÄ™ doboru modelu, selekcji cech istotnych z punktu widzenia wÅ‚asnoÅ›ci predykcyjnych lub jakoÅ›ci klasyfikacji, budowy modelu oraz optymalizacji jego parametrÃ³w. W przypadku uczenia z nauczycielem (nadzorowanego) zbiÃ³r \\(T\\) zawiera informacjÄ™ o wartoÅ›ciach atrybutÃ³w zmiennej wynikowej. 3.1.5 ZbiÃ³r testowy ZbiÃ³r testowy \\(T&#39;\\) (ang. test set) bÄ™dÄ…cy dopeÅ‚nieniem zbioru uczÄ…cego do zbioru \\(D\\), czyli \\(T&#39;=D\\setminus T\\), stanowi zestaw danych sÅ‚uÅ¼Ä…cy do oceny poprawnoÅ›ci modelu nadzorowanego. W przypadku metod nienadzorowanych raczej nie stosuje siÄ™ zbiorÃ³w testowych. 3.1.6 Model Model to narzÄ™dzie pozyskiwania wiedzy na podstawie zbioru uczÄ…cego. Nauczony model jest zbiorem reguÅ‚ \\(f\\), ktÃ³rego zadaniem jest oszacowanie wielkoÅ›ci wartoÅ›ci wynikowej lub odpowiednia klasyfikacja obiektÃ³w. W zadaniu grupowania obiektÃ³w (ang. clustering task), celem modelu jest podanie grup moÅ¼liwie najbardziej jednorodnych przy zadanym zestawie zmiennych oraz ustalonej liczbie skupieÅ„ (czasami wyznaczenie liczby skupieÅ„ jest rÃ³wnieÅ¼ czÄ™Å›ciÄ… zadania stawianego przed modelem). 3.1.7 JakoÅ›Ä‡ dopasowania modelu Do oceny jakoÅ›ci dopasowania modelu wykorzystuje siÄ™, w zaleÅ¼noÅ›ci od zadania, wiele wspÃ³Å‚czynnikÃ³w (np. dla zadaÅ„ regresyjnych sÄ… to bÅ‚Ä…d Å›rednio-kwadratowy - ang. Mean Square Error, a dla zadaÅ„ klasyfikacyjnych - trafnoÅ›Ä‡ - ang. Accuracy). MoÅ¼emy mÃ³wiÄ‡ dwÃ³ch rodzajach dopasowania modeli: poziom dopasowania na zbiorze uczÄ…cym poziom dopasowania na zbiorze testowym (oczywiÅ›cie z punktu widzenia utylitarnoÅ›ci modelu ten wspÃ³Å‚czynnik jest waÅ¼niejszy). W sytuacji, w ktÃ³rej model wykazuje dobre charakterystyki jakoÅ›ci dopasowania na zbiorze uczÄ…cym ale sÅ‚abe na testowym, mÃ³wimy o zjawisku przeuczenia modelu (ang. overfitting). Oznacza to, Å¼e model wskazuje predykcjÄ™ poprawnie jedynie dla zbioru treningowego ale ma sÅ‚aba wÅ‚asnoÅ›ci generalizacyjne nowe przypadki danych. Takie model nie przedstawiajÄ… znaczÄ…cej wartoÅ›ci w odkrywaniu wiedzy w sposÃ³b indukcyjny. Z drugiej strony parametry dopasowania modelu mogÄ… pokazywaÄ‡ sÅ‚abe dopasowanie, zarÃ³wno na zbiorze uczÄ…cym, jak i testowym. WÃ³wczas rÃ³wnieÅ¼ model nie jest uÅ¼yteczny w pozyskiwaniu wiedzy na temat badanego zjawiska, a sytuacjÄ™ takÄ… nazywamy niedouczeniem (ang. underfitting). Rysunek 3.1: PrzykÅ‚ady niedoucznia (wykresy 1 i 4), poprawego modelu (2 i 5) i przeuczenia (3 i 6). Pierwszy wiersz wykresÃ³w pokazuje klasyfikacjÄ™ na podstawie modelu na zbiorze uczÄ…cym, a drugi na zbiorze testowym. Wykres na dole pokazuje zwiÄ…zek pomiÄ™dzy zÅ‚oÅ¼onoÅ›ciÄ… modelu a wielkoÅ›ciÄ… bÅ‚Ä™du predykcji. Å¹rÃ³dÅ‚o: https://cambridgecoding.wordpress.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/ 3.2 Modele regresyjne Jednym z rodzajÃ³w zadaÅ„ bazujÄ…cym na wnioskowaniu indukcyjnym jest model regresyjny. NaleÅ¼y on do grupy metod nadzorowanych, ktÃ³rych celem jest oszacowanie wartoÅ›ci cechy wyjÅ›ciowej (ktÃ³ra jest iloÅ›ciowa) na podstawie zestawu predyktorÃ³w, ktÃ³re mogÄ… byÄ‡ iloÅ›ciowe i jakoÅ›ciowe. Uczenie takich modeli odbywa siÄ™ poprzez optymalizacjÄ™ funkcji celu (np. \\(MSE\\)) na podstawie zbioru uczÄ…cego. 3.3 Modele klasyfikacyjne Podobnie jak modele regresyjne, modele klasyfikacyjne naleÅ¼Ä… do grupy metod nadzorowanego uczenia maszynowego. Ich zadaniem jest wÅ‚aÅ›ciwa klasyfikacja obiektÃ³w na podstawie wielkoÅ›ci predyktorÃ³w. OdpowiedziÄ… modelu jest zawsze cecha typu jakoÅ›ciowego, natomiast predyktory mogÄ… mieÄ‡ dowolny typ. WyrÃ³Å¼nia siÄ™ klasyfikacjÄ™ dwu i wielostanowÄ…. Lista modeli realizujÄ…cych klasyfikacjÄ™ binarnÄ… jest nieco dÅ‚uÅ¼sza niÅ¼ w przypadku modeli z wielostanowÄ… cechÄ… wynikowÄ…. Proces uczenia modelu klasyfikacyjnego rÃ³wnieÅ¼ opiera siÄ™ na optymalizacji funkcji celu. Tym razem sÄ… to zupeÅ‚nie inne miary jakoÅ›ci dopasowania (np. trafnoÅ›Ä‡, czyli odsetek poprawnych klasyfikacji). 3.4 Modele grupujÄ…ce Bardzo szerokÄ… gamÄ™ modeli nienadzorowanych stanowiÄ… metody analizy skupieÅ„. Ich zadaniem jest grupowanie obiektÃ³w w moÅ¼liwie najbardziej jednorodne grupy, na podstawie wartoÅ›ci atrybutÃ³w poddanych analizie. PoniewaÅ¼ sÄ… to metody â€œbez nauczycielaâ€, to ocena ich przydatnoÅ›ci ma nieco inny charakter i choÄ‡ istniejÄ… rÃ³Å¼ne wskaÅºniki jakoÅ›ci grupowania, to trudno tu o obiektywne wskazanie najlepszego rozwiÄ…zania. "],
["drzewa-decyzyjne.html", "4 Drzewa decyzyjne 4.1 WÄ™zÅ‚y i gaÅ‚Ä™zie 4.2 Rodzaje reguÅ‚ podziaÅ‚u 4.3 Algorytm budowy drzewa 4.4 Kryteria zatrzymania 4.5 ReguÅ‚y podziaÅ‚u 4.6 Przycinanie drzewa decyzyjnego 4.7 ObsÅ‚uga brakÃ³w danych 4.8 Zalety i wady 4.9 Inne algorytmy budowy drzew decyzyjnych implementowane w R", " 4 Drzewa decyzyjne Drzewo decyzyjne11 jest strukturÄ… hierarchicznÄ… przedstawiajÄ…cÄ… model klasyfikacyjny lub regresyjny. Stosowane sÄ… szczegÃ³lnie czÄ™sto wÃ³wczas, gdy funkcyjna postaÄ‡ zwiÄ…zku pomiÄ™dzy predyktorami a zmiennÄ… wynikowÄ… jest nieznana lub ciÄ™Å¼ka do ustalenia. KaÅ¼de drzewo decyzyjne skÅ‚ada siÄ™ z korzenia (ang. root), wÄ™zÅ‚Ã³w (ang. nodes) i liÅ›ci (ang. leaves). Korzeniem nazywamy poczÄ…tkowy wÄ™zeÅ‚ drzewa, z ktÃ³rego poprzez podziaÅ‚y (ang. splits) powstajÄ… kolejne wÄ™zÅ‚y potomne. KoÅ„cowe wÄ™zÅ‚y, ktÃ³re nie podlegajÄ… podziaÅ‚om nazywamy liÅ›Ä‡mi, a linie Å‚Ä…czÄ…ce wÄ™zÅ‚y nazywamy gaÅ‚Ä™ziami (ang. branches). JeÅ›li drzewo sÅ‚uÅ¼y do zadaÅ„ klasyfikacyjnych, to liÅ›cie zawierajÄ… informacjÄ™ o tym, ktÃ³ra klasa w danym ciÄ…gu podziaÅ‚Ã³w jest najbardziej prawdopodobna. Natomiast, jeÅ›li drzewo jest regresyjne, to liÅ›cie zawierajÄ… warunkowe miary tendencji centralnej (najczÄ™Å›ciej Å›redniÄ…) wartoÅ›ci zmiennej wynikowej. Warunek stanowi szereg podziaÅ‚Ã³w doprowadzajÄ…cy do danego wÄ™zÅ‚a terminalnego (liÅ›cia). W obu przypadkach (klasyfikacji i regresji) drzewo â€œdÄ…Å¼yâ€ do takiego podziaÅ‚u by kolejne wÄ™zÅ‚y, a co za tym idzie rÃ³wnieÅ¼ liÅ›cie, byÅ‚y ja najbardziej jednorodne ze wzglÄ™du na zmiennÄ… wynikowÄ…. Rysunek 4.1: PrzykÅ‚ad dziaÅ‚ania drzewa regresyjnego. Wykes w lewym gÃ³rnym rogu pokazuje prawdziwÄ… zaleÅ¼noÅ›Ä‡, wyres po prawej stronie jest ilustracjÄ… drzewa decyzyjnego, a wykres w lewym dolnym rogu pokazuje dyskretyzacjÄ™ przestrzeni dokonanÄ… przez drzewo, czyli sposÃ³b jego dziaÅ‚ania. 4.1 WÄ™zÅ‚y i gaÅ‚Ä™zie KaÅ¼dy podziaÅ‚ rozdziela dziedzinÄ™ \\(X\\) na dwa lub wiÄ™cej podobszarÃ³w dziedziny i wÃ³wczas kaÅ¼da obserwacja wÄ™zÅ‚a nadrzÄ™dnego jest przyporzÄ…dkowana wÄ™zÅ‚om potomnym. KaÅ¼dy odchodzÄ…cy wÄ™zeÅ‚ potomny jest poÅ‚Ä…czony gaÅ‚Ä™ziÄ…, ktÃ³ra to wiÄ…Å¼e siÄ™ Å›ciÅ›le z moÅ¼liwymi wynikami podziaÅ‚u. KaÅ¼dy \\(\\mathbf{n}\\)-ty wÄ™zeÅ‚ moÅ¼na opisaÄ‡ jako podzbiÃ³r dziedziny w nastÄ™pujÄ…cy sposÃ³b \\[\\begin{equation} X_{\\mathbf{n}}=\\{x\\in X|t_1(x)=r_1,t_2(x)=r_2,\\ldots,t_k(x)=r_k\\}, \\end{equation}\\] gdzie \\(t_1,t_2,\\ldots,t_k\\) sÄ… podziaÅ‚ami, ktÃ³re przeprowadzajÄ… \\(x\\) w obszary \\(r_1, r_2,\\ldots, r_k\\). Przez \\[\\begin{equation} S_{\\mathbf{n}, t=r}=\\{x\\in S|t(x)=r\\} \\end{equation}\\] rozumiemy, Å¼e dokonano takiego ciÄ…gu podziaÅ‚Ã³w zbioru \\(S\\), Å¼e jego wartoÅ›ci znalazÅ‚y siÄ™ w \\(\\mathbf{n}\\)-tym wÄ™Åºle. 4.2 Rodzaje reguÅ‚ podziaÅ‚u NajczÄ™Å›ciej wystÄ™pujÄ…ce reguÅ‚y podziaÅ‚u w drzewach decyzyjnych sÄ… jednowymiarowe, czyli warunek podziaÅ‚u jest generowany na podstawie jednego atrybutu. IstniejÄ… podziaÅ‚y wielowymiarowe ale ze wzglÄ™du na zÅ‚oÅ¼onoÅ›Ä‡ obliczeniowÄ… sÄ… rzadziej stosowane. 4.2.1 PodziaÅ‚y dla atrybutÃ³w ze skali nominalnej IstniejÄ… dwa typy reguÅ‚ podziaÅ‚u dla skali nominalnej: oparte na wartoÅ›ci atrybutu (ang. value based) - wÃ³wczas funkcja testowa przyjmuje postaÄ‡ \\(t(x)=a(x)\\), czyli podziaÅ‚ generujÄ… wartoÅ›ci atrybutu; oparte na rÃ³wnoÅ›ci (ang. equality based) - gdzie funkcja testowa jest zdefiniowana jako \\[\\begin{equation} t(x)= \\begin{cases} 1, &amp;\\text{ gdy } a(x)=\\nu\\\\ 0, &amp; \\text{ w przeciwnym przypadku}, \\end{cases} \\end{equation}\\] gdzie \\(\\nu\\in A\\) i \\(A\\) jest zbiorem moÅ¼liwych wartoÅ›ci \\(a\\). W tym przypadku podziaÅ‚ jest dychotomiczny, albo obiekt ma wartoÅ›Ä‡ atrybutu rÃ³wnÄ… \\(\\nu\\), albo go nie ma. 4.2.2 PodziaÅ‚y dla atrybutÃ³w ze skali ciÄ…gÅ‚ej ReguÅ‚y podziaÅ‚u stosowane do skali ciÄ…gÅ‚ej, to: oparta na nierÃ³wnoÅ›ciach (ang. inequality based) - zdefiniowana jako \\[\\begin{equation} t(x) = \\begin{cases} 1, &amp;\\text{ gdy }a(x)\\leq \\nu\\\\ 0, &amp; \\text{w przeciwnym przypadku}, \\end{cases} \\end{equation}\\] gdzie \\(\\nu\\in A\\); przedziaÅ‚owa (ang. interval based) - zdefiniowana jako \\[\\begin{equation} t(x) = \\begin{cases} 1, &amp;\\text{ gdy }a(x) \\in I_1\\\\ 2, &amp;\\text{ gdy }a(x) \\in I_2\\\\ \\vdots &amp; \\\\ k, &amp;\\text{ gdy }a(x) \\in I_k\\\\ \\end{cases} \\end{equation}\\] gdzie \\(I_1,I_2,\\ldots,I_k\\subset A\\) stanowiÄ… rozÅ‚Ä…czny podziaÅ‚ (przedziaÅ‚ami) przeciwdziedziny \\(A\\). 4.2.3 PodziaÅ‚y dla atrybutÃ³w ze skali porzÄ…dkowej PodziaÅ‚y te mogÄ… wykorzystywaÄ‡ oba wczeÅ›niej wspomniane typy, w zaleÅ¼noÅ›ci od potrzeb. 4.3 Algorytm budowy drzewa stwÃ³rz poczÄ…tkowy wÄ™zeÅ‚ (korzeÅ„) i oznacz go jako otwarty; przypisz wszystkie moÅ¼liwe rekordy do wÄ™zÅ‚a poczÄ…tkowego; dopÃ³ki istniejÄ… otwarte wÄ™zÅ‚y wykonuj: wybierz wÄ™zeÅ‚ \\(\\mathbf{n}\\), wyznacz potrzebne statystyki opisowe zmiennej zaleÅ¼nej dla tego wÄ™zÅ‚a i przypisz wartoÅ›Ä‡ docelowÄ…; jeÅ›li kryterium zatrzymania podziaÅ‚u jest speÅ‚nione dla wÄ™zÅ‚a \\(n\\), to oznacz go za zamkniÄ™ty; w przeciwnym przypadku wybierz podziaÅ‚ \\(r\\) elementÃ³w wÄ™zÅ‚a \\(\\mathbf{n}\\), i dla kaÅ¼dego podzbioru podziaÅ‚u stwÃ³rz wÄ™zeÅ‚ niÅ¼szego rzÄ™du (potomka) \\(\\mathbf{n}_r\\) oraz oznacz go jako otwarty; nastÄ™pnie przypisz wszystkie przypadki generowane podziaÅ‚em \\(r\\) do odpowiednich wÄ™zÅ‚Ã³w potomkÃ³w \\(\\mathbf{n}_r\\); oznacza wÄ™zeÅ‚ \\(\\mathbf{n}\\) jako zamkniÄ™ty. SposÃ³b przypisywania wartoÅ›ci docelowej wiÄ…Å¼e siÄ™ Å›ciÅ›le z rodzajem drzewa. W drzewach regresyjnych chodzi o wyliczenie Å›redniej lub mediany dla obserwacji ujÄ™tych w danym wÄ™Åºle. Natomiast w przypadku drzewa klasyfikacyjnego, wyznacza siÄ™ wartoÅ›ci prawdopodobieÅ„stw przynaleÅ¼noÅ›ci obserwacji znajdujÄ…cej siÄ™ w danym wÄ™Åºle do poszczegÃ³lnych klas \\[\\begin{equation} \\P(d|\\mathbf{n})=\\P_{T_\\mathbf{n}}(d)=\\frac{|T_\\mathbf{n}^d|}{|T_\\mathbf{n}|}, \\end{equation}\\] gdzie \\(T_\\mathbf{n}\\) oznaczajÄ… obserwacje zbioru uczÄ…cego znajdujÄ…ce siÄ™ w wÄ™Åºle \\(\\mathbf{n}\\), a \\(T_\\mathbf{n}^d\\) oznacza dodatkowo podzbiÃ³r zbioru uczÄ…cego w \\(\\mathbf{n}\\) wÄ™Åºle, ktÃ³re naleÅ¼Ä… do klasy \\(d\\). OczywiÅ›cie klasyfikacja na podstawie otrzymanych prawdopodobieÅ„stw w danym wÄ™Åºle jest dokonana przez wybÃ³r klasy charakteryzujÄ…cej siÄ™ najwyÅ¼szym prawdopodobieÅ„stwem. 4.4 Kryteria zatrzymania Kryterium zatrzymania jest warunkiem, ktÃ³ry decyduje o tym, Å¼e dany wÄ™zeÅ‚ uznajemy za zamkniÄ™ty i nie dokonujemy dalszego jego podziaÅ‚u. WyrÃ³Å¼niamy nastÄ™pujÄ…ce kryteria zatrzymania: jednorodnoÅ›Ä‡ wÄ™zÅ‚a - w przypadku drzewa klasyfikacyjnego moÅ¼e zdarzyÄ‡ siÄ™ sytuacja, Å¼e wszystkie obserwacje wÄ™zÅ‚a bÄ™dÄ… pochodziÅ‚y z jednej klasy. WÃ³wczas nie ma sensu dokonywaÄ‡ dalszego podziaÅ‚u wÄ™zÅ‚a; wÄ™zeÅ‚ jest pusty - zbiÃ³r przypisanych obserwacji zbioru uczÄ…cego do \\(\\mathbf{n}\\)-tego wÄ™zÅ‚a jest pusty; brak reguÅ‚ podziaÅ‚u - wszystkie reguÅ‚y podziaÅ‚u zostaÅ‚y wykorzystane, zatem nie da siÄ™ stworzyÄ‡ potomnych wÄ™zÅ‚Ã³w, ktÃ³re charakteryzowaÅ‚yby siÄ™ wiÄ™kszÄ… homogenicznoÅ›ciÄ…; Warunki ujÄ™te w pierwszych dwÃ³ch kryteriach mogÄ… byÄ‡ nieco zÅ‚agodzone, poprzez zatrzymanie podziaÅ‚Ã³w wÃ³wczas, gdy prawdopodobieÅ„stwo przynaleÅ¼enia do pewnej klasy przekroczy ustalony prÃ³g lub gdy liczebnoÅ›Ä‡ wÄ™zÅ‚a spadnie poniÅ¼ej ustalonej wartoÅ›ci. W literaturze tematu istnieje jeszcze jedno czÄ™sto stosowane kryterium zatrzymania oparte na wielkoÅ›ci drzewa. WÄ™zeÅ‚ potomny ustala siÄ™ jako zamkniÄ™ty, gdy dÅ‚ugoÅ›Ä‡ Å›cieÅ¼ki dojÅ›cia do nie go przekroczy ustalonÄ… wartoÅ›Ä‡. 4.5 ReguÅ‚y podziaÅ‚u WaÅ¼nym elementem algorytmu tworzenia drzewa regresyjnego jest reguÅ‚a podziaÅ‚u. Dobierana jest w taki sposÃ³b aby zmaksymalizowaÄ‡ zdolnoÅ›ci generalizacyjne drzewa. ZÅ‚oÅ¼onoÅ›Ä‡ drzewa mierzona jest najczÄ™Å›ciej przeciÄ™tnÄ… liczbÄ… podziaÅ‚Ã³w potrzebnych do dotarcia do liÅ›cia zaczynajÄ…c od korzenia. LiÅ›cie sÄ… najczÄ™Å›ciej tworzone wÃ³wczas gdy dyspersja wartoÅ›ci wynikowej jest stosunkowo maÅ‚a lub wÄ™zeÅ‚ zawiera w miarÄ™ homogeniczne obserwacje ze wzglÄ™du na przynaleÅ¼noÅ›Ä‡ do klasy zmiennej wynikowej. W przypadku drzew regresyjnych zmiennoÅ›Ä‡ na poziomie wÄ™zÅ‚Ã³w jest dobrÄ… miarÄ… sÅ‚uÅ¼Ä…cÄ… do definiowania podziaÅ‚u w wÄ™Åºle. I tak, jeÅ›li pewien podziaÅ‚ generuje nam stosunkowo maÅ‚e dyspersje wartoÅ›ci docelowych w wÄ™zÅ‚ach potomnych, to moÅ¼na ten podziaÅ‚ uznaÄ‡ za wÅ‚aÅ›ciwy. JeÅ›li \\(T_n\\) oznacza zbiÃ³r rekordÃ³w naleÅ¼Ä…cych do wÄ™zÅ‚a \\(n\\), a \\(T_{n,t=r}\\) sÄ… podzbiorami generowanymi przez podziaÅ‚ \\(r\\) w wÄ™zÅ‚ach potomnych dla \\(n\\), to dyspersjÄ™ wartoÅ›ci docelowej \\(f\\) bÄ™dziemy oznaczali nastÄ™pujÄ…co \\[\\begin{equation}\\label{dyspersja} \\operatorname{disp}_{T_{n,t=r}}(f). \\end{equation}\\] ReguÅ‚Ä™ podziaÅ‚u moÅ¼emy okreÅ›laÄ‡ poprzez minimalizacjÄ™ Å›redniej waÅ¼onej dyspersji wartoÅ›ci docelowej nastÄ™pujÄ…cej postaci \\[\\begin{equation}\\label{reg_podz} \\operatorname{disp}_n(f|t)=\\sum_{r\\in R_t}\\frac{|T_{n,t=r}|}{|T_n|}\\operatorname{disp}_{T_{n,t=r}}(f), \\end{equation}\\] gdzie \\(|\\ |\\) oznacza moc zbioru, a \\(R_t\\) zbiÃ³r wszystkich moÅ¼liwych wartoÅ›ci reguÅ‚y podziaÅ‚u. Czasami wygodniej bÄ™dzie maksymalizowaÄ‡ przyrost dyspersji (lub spadek) \\[\\begin{equation}\\label{przyrost} \\bigtriangleup \\operatorname{disp}_n(f|t)=\\operatorname{disp}_n(f)-\\sum_{r\\in R_t}\\frac{|T_{n,t=r}|}{|T_n|}\\operatorname{disp}_{T_{n,t=r}}(f). \\end{equation}\\] MiarÄ… heterogenicznoÅ›ci wÄ™zÅ‚Ã³w ze wzglÄ™du na zmiennÄ… wynikowÄ… (ang. impurity) w drzewach klasyfikacyjnych, ktÃ³ra pozwala na tworzenie kolejnych podziaÅ‚Ã³w wÄ™zÅ‚a, sÄ… najczÄ™Å›ciej wskaÅºnik Giniâ€™ego i entropia (Breiman 1998). EntropiÄ… podzbioru uczÄ…cego w wÄ™Åºle \\(\\mathbf{n}\\), wyznaczamy wg wzoru \\[\\begin{equation} E_{T_{\\mathbf{n}}}(c|t) = \\sum_{x\\in R_t} \\frac{|T_{\\mathbf{n}, t=r}|}{|T_{\\mathbf{n}}|}E_{T_{\\mathbf{n}, t=r}}(c), \\end{equation}\\] gdzie \\(t\\) jest podziaÅ‚em (kandydatem), \\(r\\) potencjalnym wynikiem podziaÅ‚u \\(t\\), \\(c\\) jest oznaczeniem klasy zmiennej wynikowej, a \\[\\begin{equation} E_{T_{\\mathbf{n}, t=r}}(c) = \\sum_{d\\in C}-\\P_{T_{\\mathbf{n}, t=r}}(c=d)\\log\\P_{T_{\\mathbf{n}, t=r}}(c=d), \\end{equation}\\] przy czym \\[\\begin{equation} \\P_{T_{\\mathbf{n}, t=r}}(c=d)= \\P_{T_{\\mathbf{n}}}(c=d|t=r). \\end{equation}\\] Podobnie definiuje siÄ™ indeks Giniâ€™ego \\[\\begin{equation} Gi_{T_{\\mathbf{n}}}(c|t) = \\sum_{x\\in R_t} \\frac{|T_{\\mathbf{n}, t=r}|}{|T_{\\mathbf{n}}|}Gi_{T_{\\mathbf{n}, t=r}}(c), \\end{equation}\\] gdzie \\[\\begin{equation} Gi_{T_{\\mathbf{n}, t=r}}(c) = \\sum_{d\\in C}\\P_{T_{\\mathbf{n}, t=r}}(c=d)\\cdot(1-\\P_{T_{\\mathbf{n}, t=r}}(c=d))= 1-\\sum_{d\\in C}\\P^2_{T_{\\mathbf{n}, t=r}}(c=d). \\end{equation}\\] Dla tak zdefiniowanych miar â€œnieczystoÅ›ciâ€ wÄ™zÅ‚Ã³w, podziaÅ‚u dokonujemy w taki sposÃ³b, aby zminimalizowaÄ‡ wspÃ³Å‚czynnik Giniâ€™ego lub entropiÄ™. Im niÅ¼sze miary nieczystoÅ›ci, tym bardziej obserwacje znajdujÄ…ce siÄ™ w wÄ™Åºle sÄ… monokulturÄ…12. Nierzadko korzysta siÄ™ rÃ³wnieÅ¼ z wspÃ³Å‚czynnika przyrostu informacji (ang. information gain) \\[\\begin{equation} \\Delta E_{T_{\\mathbf{n}}}(c|t)=E_{T_{\\mathbf{n}}}(c)-E_{T_{\\mathbf{n}}}(c|t). \\end{equation}\\] Istnieje rÃ³wnieÅ¼ jego odpowiednik dla indeksu Giniâ€™ego. W obu przypadkach optymalnego podziaÅ‚u szukamy poprzez maksymalizacjÄ™ przyrostu informacji. 4.6 Przycinanie drzewa decyzyjnego Uczenie drzewa decyzyjnego wiÄ…Å¼e siÄ™ z ryzykiem przeuczenia modelu (podobnie jak to siÄ™ ma w przypadku innych modeli predykcyjnych). WczeÅ›niej przytoczone reguÅ‚y zatrzymania (np. gÅ‚Ä™bokoÅ›Ä‡ drzewa czy zatrzymanie przy osiÄ…gniÄ™ciu jednorodnoÅ›ci na zadanym poziomie) pomagajÄ… kontrolowaÄ‡ poziom generalizacji drzewa ale czasami bÄ™dzie dodatkowo potrzebne przyciÄ™cie drzewa, czyli usuniÄ™cie pewnych podziaÅ‚Ã³w, a co za tym idzie, rÃ³wnieÅ¼ liÅ›ci (wÄ™zÅ‚Ã³w). 4.6.1 Przycinanie redukujÄ…ce bÅ‚Ä…d JednÄ… ze strategii przycinania drzewa jest przycinanie redukujÄ…ce bÅ‚Ä…d (ang. reduced error pruning). Polega ono na porÃ³wnaniu bÅ‚Ä™dÃ³w (najczÄ™Å›ciej uÅ¼ywana jest miara odsetka bÅ‚Ä™dnych klasyfikacji lub MSE) liÅ›cia \\(\\mathbf{l}\\) i wÄ™zÅ‚a do ktÃ³rego drzewo przycinamy \\(\\mathbf{n}\\) na caÅ‚kiem nowym zbiorze uczÄ…cym \\(R\\). Niech \\(e_R(\\mathbf{l})\\) i \\(e_R(\\mathbf{n})\\) oznaczajÄ… odpowiednio bÅ‚Ä™dy na zbiorze \\(R\\) liÅ›cia i wÄ™zÅ‚a. Przez bÅ‚Ä…d wÄ™zÅ‚a rozumiemy bÅ‚Ä…d pod-drzewa o korzeniu \\(\\mathbf{n}\\). WÃ³wczas jeÅ›li zachodzi warunek \\[\\begin{equation} e_R(\\mathbf{l})\\leq e_R(\\mathbf{n}), \\end{equation}\\] to zaleca siÄ™ zastÄ…piÄ‡ wÄ™zeÅ‚ \\(\\mathbf{n}\\) liÅ›ciem \\(\\mathbf{l}\\). 4.6.2 Przycinanie minimalizujÄ…ce bÅ‚Ä…d Przycinanie minimalizujÄ…ce bÅ‚Ä…d opiera siÄ™ na spostrzeÅ¼eniu, Å¼e bÅ‚Ä…d drzewa przyciÄ™tego charakteryzuje siÄ™ zbyt pesymistycznÄ… ocenÄ… i dlatego wymaga korekty. WÄ™zeÅ‚ drzewa klasyfikacyjnego \\(\\mathbf{n}\\) zastÄ™pujemy liÅ›ciem \\(\\mathbf{l}\\), jeÅ›li \\[\\begin{equation} \\hat{e}_T(\\mathbf{l})\\leq \\hat{e}_T(\\mathbf{n}), \\end{equation}\\] gdzie \\[\\begin{equation} \\hat{e}_T(\\mathbf{n})=\\sum_{\\mathbf{n}&#39;\\in N(\\mathbf{n})}\\frac{|T_{\\mathbf{n}&#39;}|}{|T_\\mathbf{n}|}\\hat{e}_T(\\mathbf{n}&#39;), \\end{equation}\\] a \\(N(\\mathbf{n})\\) jest zbiorem wszystkich moÅ¼liwych wÄ™zÅ‚Ã³w potomnych wÄ™zÅ‚a \\(\\mathbf{n}\\) i \\[\\begin{equation} \\hat{e}_T(\\mathbf{l})=1-\\frac{|\\{x\\in T_\\mathbf{l}|c(x)=d_{\\mathbf{l}}\\}|+mp}{|T_\\mathbf{l}|+m}, \\end{equation}\\] gdzie \\(p\\) jest prawdopodobieÅ„stwem przynaleÅ¼noÅ›ci do klasy \\(d_{\\mathbf{l}}\\) ustalona na podstawie zewnÄ™trznej wiedzy (gdy jej nie posiadamy przyjmujemy \\(p=1/|C|\\)). W przypadku drzewa regresyjnego znajdujemy wiele analogii, poniewaÅ¼ jeÅ›li dla pewnego zbioru rekordÃ³w \\(T\\) speÅ‚niony jest warunek \\[\\begin{equation}\\label{kryterium1} \\operatorname{mse}_T(\\mathbf{l})\\leq\\operatorname{mse}_T(\\mathbf{n}), \\end{equation}\\] gdzie \\(\\mathbf{l}\\) i \\(\\mathbf{n}\\) oznaczajÄ… odpowiednio liÅ›Ä‡ i wÄ™zeÅ‚, to wÃ³wczas zastÄ™pujemy wÄ™zeÅ‚ \\(\\mathbf{n}\\) przez liÅ›Ä‡ \\(\\mathbf{l}\\). Estymatory wyznaczone na podstawie niewielkiej prÃ³by, mogÄ… byÄ‡ obarczone znaczÄ…cym bÅ‚Ä™dem. Wyliczanie bÅ‚Ä™du Å›rednio-kwadratowego dla podzbioru nowych wartoÅ›ci moÅ¼e siÄ™ charakteryzowaÄ‡ takim obciÄ…Å¼eniem. Dlatego stosuje siÄ™ statystyki opisowe z poprawkÄ…, ktÃ³rej pochodzenie moÅ¼e mieÄ‡ trzy ÅºrÃ³dÅ‚a: wiedza merytoryczna na temat szukanej wartoÅ›ci, zaÅ‚oÅ¼eÅ„ modelu lub na podstawie wyliczeÅ„ opartych o caÅ‚y zbiÃ³r wartoÅ›ci. Skorygowany estymator bÅ‚Ä™du Å›rednio-kwadratowego ma nastÄ™pujÄ…cÄ… postaÄ‡ \\[\\begin{equation}\\label{mse} \\widehat{\\operatorname{mse}}_T(\\mathbf{l})=\\frac{\\sum_{x\\in T}(f(x)-m_{\\mathbf{l},m,m_0}(f))^2+mS_0^2}{|T_\\mathbf{l}|+m}, \\end{equation}\\] gdzie \\[\\begin{equation}\\label{poprawka} m_{\\mathbf{l},m,m_0}(f)=\\frac{\\sum_{x\\in T_\\mathbf{l}}f(x)+mm_0}{|T_\\mathbf{l}|+m}, \\end{equation}\\] a \\(m_0\\) i \\(S_0^2\\) sÄ… Å›redniÄ… i wariancjÄ… wyznaczonymi na caÅ‚ej prÃ³bie uczÄ…cej. BÅ‚Ä…d Å›rednio-kwadratowy wÄ™zÅ‚a \\(\\mathbf{n}\\) ma postaÄ‡ \\[\\begin{equation}\\label{propagacja} \\widehat{\\operatorname{mse}}_T(\\mathbf{n})=\\sum_{\\mathbf{n}&#39;\\in N(\\mathbf{n})}\\frac{|T_{\\mathbf{n}&#39;}|}{|T_\\mathbf{n}|}\\widehat{\\operatorname{mse}}_T(\\mathbf{n}&#39;). \\end{equation}\\] WÃ³wczas kryterium podciÄ™cia moÅ¼na zapisaÄ‡ w nastÄ™pujÄ…cy sposÃ³b \\[\\begin{equation}\\label{kryterium2} \\widehat{\\operatorname{mse}}_T(\\mathbf{l}) \\leq \\widehat{\\operatorname{mse}}_T(\\mathbf{n}) \\end{equation}\\] 4.6.3 Przycinanie ze wzglÄ™du na wspÃ³Å‚czynnik zÅ‚oÅ¼onoÅ›ci drzewa Przycinanie ze wzglÄ™du na wspÃ³Å‚czynnik zÅ‚oÅ¼onoÅ›ci drzewa (ang. cost-complexity pruning) polega na wprowadzeniu â€œkaryâ€ za zwiÄ™kszonÄ… zÅ‚oÅ¼onoÅ›Ä‡ drzewa. Drzewa klasyfikacyjne przycinamy gdy speÅ‚niony jest warunek \\[\\begin{equation} e_T(\\mathbf{l})\\leq e_T(\\mathbf{n})+\\alpha C(\\mathbf{n}), \\end{equation}\\] gdzie \\(C(\\mathbf{n})\\) oznacza zÅ‚oÅ¼onoÅ›Ä‡ drzewa mierzonÄ… liczbÄ… liÅ›ci, a \\(\\alpha\\) parametrem wagi kary za zÅ‚oÅ¼onoÅ›Ä‡ drzewa. Wspomniane kryterium przyciÄ™cia dla drzew regresyjnych bazuje na wzglÄ™dnym bÅ‚Ä™dzie Å›rednio-kwadratowym (ang. relative square error), czyli \\[\\begin{equation}\\label{rse} \\widehat{\\operatorname{rse}}_T(\\mathbf{n})=\\frac{|T|\\widehat{\\operatorname{mse}}_T(\\mathbf{n})}{(|T|-1)S^2_T(f)}, \\end{equation}\\] gdzie \\(T\\) oznacza podzbiÃ³r \\(X\\), \\(S^2_T\\) wariancjÄ™ na zbiorze \\(T\\). WÃ³wczas kryterium podciÄ™cia wyglÄ…da nastÄ™pujÄ…co \\[\\begin{equation}\\label{kryterium3} \\widehat{\\operatorname{rse}}_T(\\mathbf{l})\\leq \\widehat{\\operatorname{rse}}_T(\\mathbf{n})+\\alpha C(\\mathbf{n}). \\end{equation}\\] 4.7 ObsÅ‚uga brakÃ³w danych Drzewa decyzyjne wyjÄ…tkowo dobrze radzÄ… sobie z obsÅ‚uga zbiorÃ³w z brakami. Stosowane sÄ… gÅ‚Ã³wnie dwie strategie: udziaÅ‚Ã³w obserwacji (ang. fractional instances) - rozwaÅ¼ane sÄ… wszystkie moÅ¼liwe podziaÅ‚y dla brakujÄ…cej obserwacji i przypisywana jest im odpowiednia waga lub prawdopodobieÅ„stwo, w oparciu o zaobserwowany rozkÅ‚ad znanych obserwacji. Te same wagi sÄ… stosowane do predykcji wartoÅ›ci na podstawie drzewa z brakami danych. podziaÅ‚Ã³w zastÄ™pczych (ang. surrogate splits) - jeÅ›li wynik podziaÅ‚u nie moÅ¼e byÄ‡ ustalony dla obserwacji z brakami, to uÅ¼ywany jest podziaÅ‚ zastÄ™pczy (pierwszy), jeÅ›li i ten nie moÅ¼e zostaÄ‡ ustalony, to stosuje siÄ™ kolejny. Kolejne podziaÅ‚y zastÄ™pcze sÄ… generowane tak, aby wynik podziaÅ‚u moÅ¼liwie najbardziej przypominaÅ‚ podziaÅ‚ wÅ‚aÅ›ciwy. 4.8 Zalety i wady 4.8.1 Zalety Å‚atwe w interpretacji; nie wymagajÄ… Å¼mudnego przygotowania danych (brak standaryzacji, wprowadzania zmiennych binarnych, dopuszcza wystÄ™powanie brakÃ³w danych); dziaÅ‚a na obu typach zmiennych - jakoÅ›ciowych i iloÅ›ciowych; dopuszcza nieliniowoÅ›Ä‡ zwiÄ…zku miÄ™dzy zmiennÄ… wynikowÄ… a predyktorami; odporny na odstÄ™pstwa od zaÅ‚oÅ¼eÅ„; pozwala na obsÅ‚ugÄ™ duÅ¼ych zbiorÃ³w danych. 4.8.2 Wady brak jawnej postaci zaleÅ¼noÅ›ci; zaleÅ¼noÅ›Ä‡ struktury drzewa od uÅ¼ytego algorytmu; przegrywa jakoÅ›ciÄ… predykcji z innymi metodami nadzorowanego uczenia maszynowego. PrzykÅ‚ad 4.1 PrzykÅ‚adem zastosowania drzew decyzyjnych bÄ™dzie klasyfikacja irysÃ³w na podstawie dÅ‚ugoÅ›ci i szerokoÅ›ci kielicha i pÅ‚atka. PrzykÅ‚adem zastosowania drzew decyzyjnych bÄ™dzie klasyfikacja irysÃ³w na podstawie dÅ‚ugoÅ›ci i szerokoÅ›ci kielicha i pÅ‚atka. library(tidyverse) library(rpart) # pakiet do tworzenia drzew typu CART library(rpart.plot) # pakiet do rysowania drzew KaÅ¼de zadanie ucznia maszynowego zaczynamy od czyszczenia danych i odpowiedniego ich przygotowania ale w tym przypadku skupimy siÄ™ jedynie na budowie, optymalizacji i ewaluacji modelu. PodziaÅ‚ zbioru na prÃ³bÄ™ uczÄ…cÄ… i testowÄ… set.seed(2019) dt.train &lt;- iris %&gt;% sample_frac(size = 0.7) dt.test &lt;- setdiff(iris, dt.train) str(dt.train) ## &#39;data.frame&#39;: 105 obs. of 5 variables: ## $ Sepal.Length: num 4.8 6.7 6.2 5.4 7.7 5 5.7 5 6.3 6.8 ... ## $ Sepal.Width : num 3.4 3.3 2.2 3.9 2.6 2 3.8 3 3.4 3 ... ## $ Petal.Length: num 1.9 5.7 4.5 1.3 6.9 3.5 1.7 1.6 5.6 5.5 ... ## $ Petal.Width : num 0.2 2.1 1.5 0.4 2.3 1 0.3 0.2 2.4 2.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 3 2 1 3 2 1 1 3 3 ... str(dt.test) ## &#39;data.frame&#39;: 45 obs. of 5 variables: ## $ Sepal.Length: num 4.4 5.4 4.8 4.3 5.7 5.1 5.2 5.2 5.2 4.9 ... ## $ Sepal.Width : num 2.9 3.7 3 3 4.4 3.8 3.5 3.4 4.1 3.1 ... ## $ Petal.Length: num 1.4 1.5 1.4 1.1 1.5 1.5 1.5 1.4 1.5 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.1 0.1 0.4 0.3 0.2 0.2 0.1 0.2 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Budowa drzewa Budowy drzewa dokonujemy za pomocÄ… funkcji rpart pakietu rpart (Therneau and Atkinson 2018) stosujÄ…c zapis formuÅ‚y zaleÅ¼noÅ›ci. Drzewo zostanie zbudowane z uwzglÄ™dnieniem kilku kryteriÃ³w zatrzymania: minimalna liczebnoÅ›Ä‡ wÄ™zÅ‚a, ktÃ³ry moÅ¼e zostaÄ‡ podzielony to 10 - ze wzglÄ™du na maÅ‚Ä… liczebnoÅ›Ä‡ zbioru uczÄ…cego; minimalna liczebnoÅ›Ä‡ liÅ›cia to 5 - aby nie dopuÅ›ciÄ‡ do przeuczenia modelu; maksymalna gÅ‚Ä™bokoÅ›Ä‡ drzewa to 4 - aby nie dopuÅ›ciÄ‡ do przeuczenia modelu. mod.rpart &lt;- rpart(Species~., data = dt.train, control = rpart.control(minsplit = 10, minbucket = 5, maxdepth = 4)) summary(mod.rpart) ## Call: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.51470588 0 1.00000000 1.1176471 0.06737554 ## 2 0.39705882 1 0.48529412 0.4852941 0.06995514 ## 3 0.02941176 2 0.08823529 0.1617647 0.04614841 ## 4 0.01000000 3 0.05882353 0.1617647 0.04614841 ## ## Variable importance ## Petal.Width Petal.Length Sepal.Length Sepal.Width ## 33 32 20 15 ## ## Node number 1: 105 observations, complexity param=0.5147059 ## predicted class=virginica expected loss=0.647619 P(node) =1 ## class counts: 35 33 37 ## probabilities: 0.333 0.314 0.352 ## left son=2 (35 obs) right son=3 (70 obs) ## Primary splits: ## Petal.Length &lt; 2.6 to the left, improve=35.03810, (0 missing) ## Petal.Width &lt; 0.8 to the left, improve=35.03810, (0 missing) ## Sepal.Length &lt; 5.45 to the left, improve=25.60255, (0 missing) ## Sepal.Width &lt; 3.35 to the right, improve=14.70881, (0 missing) ## Surrogate splits: ## Petal.Width &lt; 0.8 to the left, agree=1.000, adj=1.000, (0 split) ## Sepal.Length &lt; 5.45 to the left, agree=0.933, adj=0.800, (0 split) ## Sepal.Width &lt; 3.35 to the right, agree=0.848, adj=0.543, (0 split) ## ## Node number 2: 35 observations ## predicted class=setosa expected loss=0 P(node) =0.3333333 ## class counts: 35 0 0 ## probabilities: 1.000 0.000 0.000 ## ## Node number 3: 70 observations, complexity param=0.3970588 ## predicted class=virginica expected loss=0.4714286 P(node) =0.6666667 ## class counts: 0 33 37 ## probabilities: 0.000 0.471 0.529 ## left son=6 (37 obs) right son=7 (33 obs) ## Primary splits: ## Petal.Width &lt; 1.75 to the left, improve=24.297670, (0 missing) ## Petal.Length &lt; 4.75 to the left, improve=24.174190, (0 missing) ## Sepal.Length &lt; 5.75 to the left, improve= 4.483555, (0 missing) ## Sepal.Width &lt; 2.55 to the left, improve= 3.793760, (0 missing) ## Surrogate splits: ## Petal.Length &lt; 4.75 to the left, agree=0.886, adj=0.758, (0 split) ## Sepal.Length &lt; 6.15 to the left, agree=0.671, adj=0.303, (0 split) ## Sepal.Width &lt; 2.65 to the left, agree=0.671, adj=0.303, (0 split) ## ## Node number 6: 37 observations, complexity param=0.02941176 ## predicted class=versicolor expected loss=0.1351351 P(node) =0.352381 ## class counts: 0 32 5 ## probabilities: 0.000 0.865 0.135 ## left son=12 (31 obs) right son=13 (6 obs) ## Primary splits: ## Petal.Length &lt; 4.95 to the left, improve=4.0464980, (0 missing) ## Petal.Width &lt; 1.35 to the left, improve=1.0296010, (0 missing) ## Sepal.Width &lt; 3.05 to the right, improve=0.2615519, (0 missing) ## Sepal.Length &lt; 5.95 to the left, improve=0.1828101, (0 missing) ## ## Node number 7: 33 observations ## predicted class=virginica expected loss=0.03030303 P(node) =0.3142857 ## class counts: 0 1 32 ## probabilities: 0.000 0.030 0.970 ## ## Node number 12: 31 observations ## predicted class=versicolor expected loss=0.03225806 P(node) =0.2952381 ## class counts: 0 30 1 ## probabilities: 0.000 0.968 0.032 ## ## Node number 13: 6 observations ## predicted class=virginica expected loss=0.3333333 P(node) =0.05714286 ## class counts: 0 2 4 ## probabilities: 0.000 0.333 0.667 rpart.plot(mod.rpart) Rysunek 4.2: Obraz drzewa klasyfikacyjnego. PowyÅ¼szy wykres przedstawia strukturÄ™ drzewa klasyfikacyjnego. Kolorami sÄ… oznaczone klasy, ktÃ³re w danym wÄ™Åºle dominujÄ…. Nasycenie barwy decyduje o sile tej dominacji. W kaÅ¼dym wÄ™Åºle podana jest klasa, do ktÃ³rej najprawdopodobniej naleÅ¼Ä… jego obserwacje. Ponadto podane sÄ… proporcje przynaleÅ¼noÅ›ci do klas zmiennej wynikowej oraz procent obserwacji zbioru uczÄ…cego naleÅ¼Ä…cych do danego wÄ™zÅ‚a. Pod kaÅ¼dym wÄ™zÅ‚em podana jest reguÅ‚a podziaÅ‚u. Przycinanie drzewa Zanim przystÄ…pimy do przycinania drzewa naleÅ¼y sprawdziÄ‡, jakie sÄ… zdolnoÅ›ci generalizacyjne modelu. Oceny tej dokonujemy najczÄ™Å›ciej sprawdzajÄ…c macierz klasyfikacji. pred.prob &lt;- predict(mod.rpart, newdata = dt.test) pred.prob[10:20,] ## setosa versicolor virginica ## 10 1 0.0000000 0.00000000 ## 11 1 0.0000000 0.00000000 ## 12 1 0.0000000 0.00000000 ## 13 1 0.0000000 0.00000000 ## 14 1 0.0000000 0.00000000 ## 15 1 0.0000000 0.00000000 ## 16 0 0.9677419 0.03225806 ## 17 0 0.9677419 0.03225806 ## 18 0 0.9677419 0.03225806 ## 19 0 0.9677419 0.03225806 ## 20 0 0.9677419 0.03225806 pred.class &lt;- predict(mod.rpart, newdata = dt.test, type = &quot;class&quot;) pred.class ## 1 2 3 4 5 6 ## setosa setosa setosa setosa setosa setosa ## 7 8 9 10 11 12 ## setosa setosa setosa setosa setosa setosa ## 13 14 15 16 17 18 ## setosa setosa setosa versicolor versicolor versicolor ## 19 20 21 22 23 24 ## versicolor versicolor versicolor versicolor versicolor versicolor ## 25 26 27 28 29 30 ## versicolor versicolor versicolor versicolor versicolor versicolor ## 31 32 33 34 35 36 ## versicolor versicolor virginica virginica virginica virginica ## 37 38 39 40 41 42 ## virginica virginica virginica virginica virginica virginica ## 43 44 45 ## virginica virginica virginica ## Levels: setosa versicolor virginica tab &lt;- table(predykcja = pred.class, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Jak widaÄ‡ z powyÅ¼szej tabeli, model caÅ‚kiem dobrze radzi sobie z poprawnÄ… klasyfikacjÄ… obserwacji do odpowiednich kategorii. W dalszej kolejnoÅ›ci sprawdzimy, czy nie jest konieczne przyciÄ™cie drzewa. Jednym z kryteriÃ³w przycinania drzewa jest przycinanie ze wzglÄ™du na zÅ‚oÅ¼onoÅ›Ä‡ drzewa. W tym przypadku jest wyraÅ¼ony parametrem cp. Istnieje powszechnie stosowana reguÅ‚a jednego odchylenia standardowego, ktÃ³ra mÃ³wi, Å¼e drzewo naleÅ¼y przyciÄ…Ä‡ wÃ³wczas, gdy bÅ‚Ä…d oszacowany na podstawie sprawdzianu krzyÅ¼owego (xerror), pierwszy raz zejdzie poniÅ¼ej poziomu wyznaczonego przez najniÅ¼szÄ… wartoÅ›Ä‡ bÅ‚Ä™du powiÄ™kszonego o odchylenie standardowe tego bÅ‚Ä™du (xstd). Na podstawie poniÅ¼szej tabeli moÅ¼na ustaliÄ‡, Å¼e poziomem odciÄ™cia jest wartoÅ›Ä‡ \\(0.16176+0.046148=0.207908\\). Pierwszy raz bÅ‚Ä…d przyjmuje wartoÅ›Ä‡ mniejszÄ… od \\(0.16176\\) po drugim podziale (nsplit=2). Temu poziomowi odpowiada cp o wartoÅ›ci \\(0.029412\\) i to jest zÅ‚oÅ¼onoÅ›Ä‡ drzewa, ktÃ³rÄ… powinniÅ›my przyjÄ…Ä‡ do przyciÄ™cia drzewa. printcp(mod.rpart) ## ## Classification tree: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## ## Variables actually used in tree construction: ## [1] Petal.Length Petal.Width ## ## Root node error: 68/105 = 0.64762 ## ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.514706 0 1.000000 1.11765 0.067376 ## 2 0.397059 1 0.485294 0.48529 0.069955 ## 3 0.029412 2 0.088235 0.16176 0.046148 ## 4 0.010000 3 0.058824 0.16176 0.046148 plotcp(mod.rpart) Rysunek 4.3: Na wykresie bÅ‚Ä™dÃ³w punkt odciÄ™cia zaznaczony jest liniÄ… przerywanÄ… PrzyciÄ™te drzewo wyglÄ…da nastÄ™pujÄ…co: mod.rpart2 &lt;- prune(mod.rpart, cp = 0.029412) summary(mod.rpart2) ## Call: ## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, ## minbucket = 5, maxdepth = 4)) ## n= 105 ## ## CP nsplit rel error xerror xstd ## 1 0.5147059 0 1.00000000 1.1176471 0.06737554 ## 2 0.3970588 1 0.48529412 0.4852941 0.06995514 ## 3 0.0294120 2 0.08823529 0.1617647 0.04614841 ## ## Variable importance ## Petal.Width Petal.Length Sepal.Length Sepal.Width ## 34 31 20 15 ## ## Node number 1: 105 observations, complexity param=0.5147059 ## predicted class=virginica expected loss=0.647619 P(node) =1 ## class counts: 35 33 37 ## probabilities: 0.333 0.314 0.352 ## left son=2 (35 obs) right son=3 (70 obs) ## Primary splits: ## Petal.Length &lt; 2.6 to the left, improve=35.03810, (0 missing) ## Petal.Width &lt; 0.8 to the left, improve=35.03810, (0 missing) ## Sepal.Length &lt; 5.45 to the left, improve=25.60255, (0 missing) ## Sepal.Width &lt; 3.35 to the right, improve=14.70881, (0 missing) ## Surrogate splits: ## Petal.Width &lt; 0.8 to the left, agree=1.000, adj=1.000, (0 split) ## Sepal.Length &lt; 5.45 to the left, agree=0.933, adj=0.800, (0 split) ## Sepal.Width &lt; 3.35 to the right, agree=0.848, adj=0.543, (0 split) ## ## Node number 2: 35 observations ## predicted class=setosa expected loss=0 P(node) =0.3333333 ## class counts: 35 0 0 ## probabilities: 1.000 0.000 0.000 ## ## Node number 3: 70 observations, complexity param=0.3970588 ## predicted class=virginica expected loss=0.4714286 P(node) =0.6666667 ## class counts: 0 33 37 ## probabilities: 0.000 0.471 0.529 ## left son=6 (37 obs) right son=7 (33 obs) ## Primary splits: ## Petal.Width &lt; 1.75 to the left, improve=24.297670, (0 missing) ## Petal.Length &lt; 4.75 to the left, improve=24.174190, (0 missing) ## Sepal.Length &lt; 5.75 to the left, improve= 4.483555, (0 missing) ## Sepal.Width &lt; 2.55 to the left, improve= 3.793760, (0 missing) ## Surrogate splits: ## Petal.Length &lt; 4.75 to the left, agree=0.886, adj=0.758, (0 split) ## Sepal.Length &lt; 6.15 to the left, agree=0.671, adj=0.303, (0 split) ## Sepal.Width &lt; 2.65 to the left, agree=0.671, adj=0.303, (0 split) ## ## Node number 6: 37 observations ## predicted class=versicolor expected loss=0.1351351 P(node) =0.352381 ## class counts: 0 32 5 ## probabilities: 0.000 0.865 0.135 ## ## Node number 7: 33 observations ## predicted class=virginica expected loss=0.03030303 P(node) =0.3142857 ## class counts: 0 1 32 ## probabilities: 0.000 0.030 0.970 rpart.plot(mod.rpart2) Rysunek 4.4: Drzewo klasyfikacyjne po przyciÄ™ciu Ocena dopasowania modelu Na koniec budowy modelu naleÅ¼y sprawdziÄ‡ jego jakoÅ›Ä‡ na zbiorze testowym. pred.class2 &lt;- predict(mod.rpart2, newdata = dt.test, type = &quot;class&quot;) tab2 &lt;- table(predykcja = pred.class2, obserwacja = dt.test$Species) tab2 ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Mimo przyciÄ™cia drzewa, klasyfikacja pozostaje na niezmienionym poziomie. Odsetek poprawnych klasyfikacji moÅ¼emy oszacowaÄ‡ za pomocÄ… round(sum(diag(tab2))/sum(tab2)*100,1) ## [1] 100 4.9 Inne algorytmy budowy drzew decyzyjnych implementowane w R OprÃ³cz najbardziej znanego algorytmu CART implementowanego w postaci funkcji pakietu rpart, istniejÄ… rÃ³wnieÅ¼ inne algorytmy, ktÃ³re znalazÅ‚y swoje implementacje w R. SÄ… to: CHAID13 - algorytm przeznaczony do budowy drzew klasyfikacyjnych, gdzie zarÃ³wno zmienna wynikowa, jak i zmienne niezaleÅ¼ne muszÄ… byÄ‡ ze skali jakoÅ›ciowej. GÅ‚Ã³wnÄ… rÃ³Å¼nicÄ… w stosunku do drzew typu CART jest sposÃ³b budowy podziaÅ‚Ã³w, oparty na teÅ›cie niezaleÅ¼noÅ›ci \\(\\chi^2\\) Pearsona. Wyboru reguÅ‚y podziaÅ‚u dokonuje siÄ™ poprzez testowanie niezaleÅ¼noÅ›ci zmiennej niezaleÅ¼nej z predyktorami. ReguÅ‚a o najwiÄ™kszej wartoÅ›ci statystyki \\(\\chi^2\\) jest stosowana w pierwszej kolejnoÅ›ci. Implementacja tego algorytmu znajduje siÄ™ w pakiecie CHAID14 (funkcja do tworzenia drzewa o tej samej nazwie chaid) (Team 2015). Ctree15 - algorytm zbliÅ¼ony zasadÄ… dziaÅ‚ania do CHAID, poniewaÅ¼ rÃ³wnieÅ¼ wykorzystuje testowanie do wyboru reguÅ‚y podziaÅ‚u. RÃ³Å¼ni siÄ™ jednak tym, Å¼e moÅ¼e byÄ‡ stosowany do zmiennych dowolnego typu oraz tym, Å¼e moÅ¼e byÄ‡ zarÃ³wno drzewem klasyfikacyjnym jak i regresyjnym. ImplementacjÄ™ R-owÄ… moÅ¼na znaleÅºÄ‡ w pakietach party (Hothorn, Hornik, and Zeileis 2006) lub partykit (Hothorn and Zeileis 2015) - funkcjÄ… do tworzenia modelu jest ctree. C4.5 - algorytm stworzony przez Quinlan (1993) w oparciu, o rÃ³wnieÅ¼ jego autorstwa, algorytm ID3. SÅ‚uÅ¼y jedynie do zadaÅ„ klasyfikacyjnych. W duÅ¼ym uproszczeniu, dobÃ³r reguÅ‚ podziaÅ‚u odbywa siÄ™ na podstawie przyrostu informacji (patrz ReguÅ‚y podziaÅ‚u). W przeciwieÅ„stwie do pierwotnego algorytmu ID3, C4.5 nie raczej nie przeucza drzew. Implementacja R-owa znajduje siÄ™ w pakiecie RWeka (Hornik, Buchta, and Zeileis 2009) - funkcja do budowy drzewa to J48. C5.0 - kolejny algorytm autorstwa Kuhn and Quinlan (2018) jest usprawnieniem algorytmu C4.5, generujÄ…cym mniejsze drzewa automatycznie przycinane na podstawie zÅ‚oÅ¼onoÅ›ci drzewa. SÅ‚uÅ¼y jedynie do zadaÅ„ klasyfikacyjnych. Jest szybszy od poprzednika i pozwala na zastosowanie metody boosting16. Implementacja R-owa znajduje siÄ™ w pakiecie C50, a funkcja do budowy drzewa to C5.0. PrzykÅ‚ad 4.2 W celu porÃ³wnania wynikÃ³w klasyfikacji na podstawie drzew decyzyjnych o rÃ³Å¼nych algorytmach, zostanÄ… nauczone modele w oparciu o funkcje ctree, J48 i C5.0 dla tego samego zestawu danych co w przykÅ‚adzie wczeÅ›niejszym 4.1. Drzewo ctree Na poczÄ…tek ustalamy parametry ograniczajÄ…ce rozrost drzewa podobne jak w poprzednim przykÅ‚adzie. library(partykit) tree2 &lt;- ctree(Species~., data = dt.train, control = ctree_control(minsplit = 10, minbucket = 5, maxdepth = 4)) tree2 ## ## Model formula: ## Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width ## ## Fitted party: ## [1] root ## | [2] Petal.Length &lt;= 1.9: setosa (n = 35, err = 0.0%) ## | [3] Petal.Length &gt; 1.9 ## | | [4] Petal.Width &lt;= 1.7 ## | | | [5] Petal.Length &lt;= 4.9: versicolor (n = 31, err = 3.2%) ## | | | [6] Petal.Length &gt; 4.9: virginica (n = 6, err = 33.3%) ## | | [7] Petal.Width &gt; 1.7: virginica (n = 33, err = 3.0%) ## ## Number of inner nodes: 3 ## Number of terminal nodes: 4 plot(tree2) Rysunek 4.5: Wykres drzewa decyzyjnego zbudowanego metodÄ… ctree Wydaje siÄ™, Å¼e drzewo nie jest optymalne, poniewaÅ¼ w wÄ™Åºle 6 obserwacje z grup versicolor i virginica sÄ… nieco pomieszane. Ostateczne oceny dokonujemy na podstawie prÃ³by testowej. pred2 &lt;- predict(tree2, newdata = dt.test) tab &lt;- table(predykcja = pred2, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Dopiero ocena jakoÅ›ci klasyfikacji na podstawie prÃ³by testowej pokazuje, Å¼e model zbudowany za pomocÄ… ctree daje podobnÄ… precyzjÄ™ jak rpart przyciÄ™ty. Drzewo J48 W tym przypadku model sam poszukuje optymalnego rozwiÄ…zania przycinajÄ…c siÄ™ automatycznie. library(RWeka) tree3 &lt;- J48(Species~., data = dt.train) tree3 ## J48 pruned tree ## ------------------ ## ## Petal.Width &lt;= 0.6: setosa (35.0) ## Petal.Width &gt; 0.6 ## | Petal.Width &lt;= 1.7 ## | | Petal.Length &lt;= 4.9: versicolor (31.0/1.0) ## | | Petal.Length &gt; 4.9 ## | | | Petal.Width &lt;= 1.5: virginica (3.0) ## | | | Petal.Width &gt; 1.5: versicolor (3.0/1.0) ## | Petal.Width &gt; 1.7: virginica (33.0/1.0) ## ## Number of Leaves : 5 ## ## Size of the tree : 9 plot(tree3) Rysunek 4.6: Wykres drzewa decyzyjnego zbudowanego metodÄ… J48 Drzewo jest nieco bardziej rozbudowane niÅ¼ tree2 i mod.rpart2. summary(tree3) ## ## === Summary === ## ## Correctly Classified Instances 102 97.1429 % ## Incorrectly Classified Instances 3 2.8571 % ## Kappa statistic 0.9571 ## Mean absolute error 0.0331 ## Root mean squared error 0.1286 ## Relative absolute error 7.4482 % ## Root relative squared error 27.2918 % ## Total Number of Instances 105 ## ## === Confusion Matrix === ## ## a b c &lt;-- classified as ## 35 0 0 | a = setosa ## 0 32 1 | b = versicolor ## 0 2 35 | c = virginica Podsumowanie dopasowania drzewa na prÃ³bie uczÄ…cej jest bardzo dobre, bo poprawnych klasyfikacji jest ponad 97%. Oceny dopasowania i tak dokonujemy na zbiorze testowym. pred3 &lt;- predict(tree3, newdata = dt.test) tab &lt;- table(predykcja = pred3, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Otrzymujemy identycznÄ… macierz klasyfikacji jak w poprzednich przypadkach. Drzewo C50 Tym razem rÃ³wnieÅ¼ nie trzeba ustawiaÄ‡ parametrÃ³w drzewa, poniewaÅ¼ algorytm dziaÅ‚a tak aby zapobiec rozrostowi drzewa przy jednoczesnej wysokiej poprawnoÅ›ci klasyfikacji. library(C50) tree4 &lt;- C5.0(Species~., data = dt.train) summary(tree4) ## ## Call: ## C5.0.formula(formula = Species ~ ., data = dt.train) ## ## ## C5.0 [Release 2.07 GPL Edition] Mon May 06 21:35:06 2019 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 105 cases (5 attributes) from undefined.data ## ## Decision tree: ## ## Petal.Length &lt;= 1.9: setosa (35) ## Petal.Length &gt; 1.9: ## :...Petal.Width &gt; 1.7: virginica (33/1) ## Petal.Width &lt;= 1.7: ## :...Petal.Length &lt;= 4.9: versicolor (31/1) ## Petal.Length &gt; 4.9: virginica (6/2) ## ## ## Evaluation on training data (105 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 4 4( 3.8%) &lt;&lt; ## ## ## (a) (b) (c) &lt;-classified as ## ---- ---- ---- ## 35 (a): class setosa ## 30 3 (b): class versicolor ## 1 36 (c): class virginica ## ## ## Attribute usage: ## ## 100.00% Petal.Length ## 66.67% Petal.Width ## ## ## Time: 0.0 secs Otrzymujemy identyczne drzewo jak w przypadku zastosowania algorytmu ctree. plot(tree4) Rysunek 4.7: Wykres drzewa decyzyjnego zbudowanego metodÄ… C5.0 Dla pewnoÅ›ci przeprowadzimy sprawdzenie na zbiorze testowym. pred4 &lt;- predict(tree4, newdata = dt.test) tab &lt;- table(predykcja = pred4, obserwacja = dt.test$Species) tab ## obserwacja ## predykcja setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 0 13 Bibliografia "],
["pochodne-drzew-decyzyjnych.html", "5 Pochodne drzew decyzyjnych 5.1 Bagging 5.2 Lasy losowe 5.3 Boosting", " 5 Pochodne drzew decyzyjnych PrzykÅ‚ad zastosowania drzew decyzyjnych na zbiorze iris w poprzednich przykÅ‚adach moÅ¼e skÅ‚aniaÄ‡ do przypuszczenia, Å¼e drzewa decyzyjne zawsze dobrze radzÄ… sobie z predykcjÄ… wartoÅ›ci wynikowej. Niestety w przykÅ‚adach nieco bardziej skomplikowanych, gdzie chociaÅ¼by klasy zmiennej wynikowej nie sÄ… tak wyraÅºnie separowalne, drzewa decyzyjne wypadajÄ… gorzej w porÃ³wnaniu z innymi modelami nadzorowanego uczenia maszynowego. I tak u podstaw metod bazujÄ…cych na prostych drzewach decyzyjnych staÅ‚ pomysÅ‚, Å¼e skoro jedno drzewo nie ma wystarczajÄ…cych wÅ‚asnoÅ›ci predykcyjnych, to moÅ¼e zastosowanie wielu drzew poÅ‚Ä…czonych w pewien sposÃ³b poprawi je. Tak powstaÅ‚y metody bagging, random forest i boosting17. NaleÅ¼y zaznaczyÄ‡, Å¼e metody znajdujÄ… swoje zastosowanie rÃ³wnieÅ¼ w innych modelach nadzorowanego uczenia maszynowego. 5.1 Bagging Technika ta zostaÅ‚a wprowadzona przez Breiman (1996) i ma na celu zmniejszenie wariancji modelu pojedynczego drzewa. Podobnie jak technika bootstrap, w ktÃ³rej statystyki sÄ… wyliczane na wielu prÃ³bach pobranych z tego samego rozkÅ‚adu (prÃ³by), w metodzie bagging losuje siÄ™ wiele prÃ³b ze zbioru uczÄ…cego (najczÄ™Å›ciej poprzez wielokrotne losowanie prÃ³by o rozmiarze zbioru uczÄ…cego ze zwracaniem), a nastÄ™pnie dla kaÅ¼dej prÃ³by bootstrapowej buduje siÄ™ drzewo. W ten sposÃ³b otrzymujemy \\(B\\) drzew decyzyjnych \\(\\hat{f}^1(x), \\hat{f}^2(x),\\ldots, \\hat{f}^B(x)\\). Na koniec poprzez uÅ›rednienie otrzymujemy model charakteryzujÄ…cy siÄ™ wiÄ™kszÄ… precyzjÄ… \\[\\begin{equation} \\hat{f}_{bag}(x)=\\frac1B\\sum_{b=1}^B\\hat{f}^b(x). \\end{equation}\\] PoniewaÅ¼ podczas budowy drzew na podstawie prÃ³b bootstrapowych nie kontrolujemy zÅ‚oÅ¼onoÅ›ci, to w rezultacie kaÅ¼de z drzew moÅ¼e charakteryzowaÄ‡ siÄ™ duÅ¼Ä… wariancjÄ…. Poprzez uÅ›rednianie wynikÃ³w pojedynczych drzew otrzymujemy mniejsze obciÄ…Å¼enie ale rÃ³wnieÅ¼ przy dostatecznie duÅ¼ej liczbie prÃ³b (\\(B\\) czÄ™sto liczy siÄ™ w setkach, czy tysiÄ…cach) zmniejszamy wariancjÄ™ â€œÅ›redniejâ€ predykcji z drzew. OczywiÅ›cie metodÄ™ tÄ… trzeba dostosowaÄ‡ do zadaÅ„ klasyfikacyjnych, poniewaÅ¼ nie istnieje Å›rednia klasyfikacji z wielu drzew. W miejsce Å›redniej stosuje siÄ™ modÄ™, czyli wartoÅ›Ä‡ dominujÄ…cÄ…. Przyjrzyjmy siÄ™ jak maszyna losuje obserwacje ze zwracaniem n &lt;- NULL m &lt;- NULL for(i in 1:1000){ x &lt;- sample(1:500, size = 500, replace = T) y &lt;- setdiff(1:500, x) z &lt;- unique(x) n[i] &lt;- length(z) m[i] &lt;- length(y) } mean(n)/500*100 ## [1] 63.2802 mean(m)/500*100 ## [1] 36.7198 Faktycznie uczenie modelu metodÄ… bagging odbywa siÄ™ Å›rednio na 2/3 obserwacji zbioru uczÄ…cego wylosowanych do prÃ³b bootstrapowych, a pozostaÅ‚a 1/3 (ang. out-of-bag) jest wykorzystana do oceny jakoÅ›ci predykcji. NiewÄ…tpliwÄ… zaletÄ… drzew decyzyjnych byÅ‚a ich Å‚atwa interpretacja. W przypadku metody bagging jest ona znacznie utrudniona, poniewaÅ¼ jej wynik skÅ‚ada siÄ™ z agregacji wielu drzew. MoÅ¼na natomiast oceniÄ‡ waÅ¼noÅ›Ä‡ predyktorÃ³w (ang. variable importance). I tak, przez obserwacjÄ™ spadku \\(RSS\\) dla baggingu regresyjnego przy zastosowaniu danego predyktora w podziaÅ‚ach drzewa i uÅ›rednieniu wyniku otrzymamy wskaÅºnik waÅ¼noÅ›ci predyktora duÅ¼o lepszy niÅ¼ dla pojedynczego drzewa. W przypadku baggingu klasyfikacyjnego w miejsce \\(RSS\\) stosujemy indeks Giniâ€™ego. Implementacja R-owa metody bagging znajduje siÄ™ w pakiecie ipred, a funkcja do budowy modelu nazywa siÄ™ bagging (Peters and Hothorn 2018). MoÅ¼na rÃ³wnieÅ¼ stosowaÄ‡ funkcjÄ™ randomForest pakietu randomForest (Liaw and Wiener 2002) - powody takiego dziaÅ‚ania wyjaÅ›niÄ… siÄ™ w podrozdziale Lasy losowe. PrzykÅ‚ad 5.1 Tym razem cel zadania jest regresyjny i polega na ustaleniu miary tendencji centralnej ceny mieszkaÅ„ w Bostonie na podstawie zmiennych umieszczonych w zbiorze Boston pakietu MASS (Venables and Ripley 2002). ZmiennÄ… zaleÅ¼nÄ… bÄ™dzie mediana cen mieszkaÅ„ na przedmieÅ›ciach Bostonu (medv). library(MASS) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 set.seed(2019) boston.train &lt;- Boston %&gt;% sample_frac(size = 2/3) boston.test &lt;- setdiff(Boston, boston.train) Aby mÃ³c porÃ³wnaÄ‡ wyniki predykcji z metody bagging, najpierw zostanie zbudowane jedno drzewo decyzyjne w oparciu o algorytm CART. library(rpart) library(rpart.plot) boston.rpart &lt;- rpart(medv~., data = boston.train) x &lt;- summary(boston.rpart) ## Call: ## rpart(formula = medv ~ ., data = boston.train) ## n= 337 ## ## CP nsplit rel error xerror xstd ## 1 0.49839799 0 1.0000000 1.0086928 0.10259521 ## 2 0.15725128 1 0.5016020 0.5442932 0.06125724 ## 3 0.07485605 2 0.3443507 0.4031978 0.05139310 ## 4 0.03672387 3 0.2694947 0.3127794 0.04599170 ## 5 0.03552748 4 0.2327708 0.2974517 0.04560807 ## 6 0.01695185 5 0.1972433 0.2553208 0.04022970 ## 7 0.01422576 6 0.1802915 0.2713816 0.04099092 ## 8 0.01103490 7 0.1660657 0.2744789 0.04107777 ## 9 0.01000000 8 0.1550308 0.2720415 0.04119266 ## ## Variable importance ## rm lstat indus ptratio crim age nox dis zn ## 33 19 9 8 7 6 6 5 3 ## tax rad chas ## 2 1 1 ## ## Node number 1: 337 observations, complexity param=0.498398 ## mean=22.69792, MSE=79.32964 ## left son=2 (286 obs) right son=3 (51 obs) ## Primary splits: ## rm &lt; 6.92 to the left, improve=0.4983980, (0 missing) ## lstat &lt; 9.725 to the right, improve=0.4424796, (0 missing) ## indus &lt; 6.66 to the right, improve=0.2796065, (0 missing) ## ptratio &lt; 19.65 to the right, improve=0.2600149, (0 missing) ## nox &lt; 0.6695 to the right, improve=0.2346383, (0 missing) ## Surrogate splits: ## ptratio &lt; 14.55 to the right, agree=0.884, adj=0.235, (0 split) ## lstat &lt; 4.915 to the right, agree=0.878, adj=0.196, (0 split) ## zn &lt; 87.5 to the left, agree=0.864, adj=0.098, (0 split) ## indus &lt; 1.605 to the right, agree=0.864, adj=0.098, (0 split) ## crim &lt; 0.013355 to the right, agree=0.852, adj=0.020, (0 split) ## ## Node number 2: 286 observations, complexity param=0.1572513 ## mean=20.04266, MSE=37.17489 ## left son=4 (114 obs) right son=5 (172 obs) ## Primary splits: ## lstat &lt; 14.405 to the right, improve=0.3954065, (0 missing) ## nox &lt; 0.6695 to the right, improve=0.3012249, (0 missing) ## crim &lt; 8.37969 to the right, improve=0.2817286, (0 missing) ## ptratio &lt; 20.15 to the right, improve=0.2392532, (0 missing) ## dis &lt; 2.4737 to the left, improve=0.2295258, (0 missing) ## Surrogate splits: ## age &lt; 84.3 to the right, agree=0.808, adj=0.518, (0 split) ## dis &lt; 2.23935 to the left, agree=0.773, adj=0.430, (0 split) ## crim &lt; 4.067905 to the right, agree=0.762, adj=0.404, (0 split) ## nox &lt; 0.5765 to the right, agree=0.762, adj=0.404, (0 split) ## indus &lt; 16.57 to the right, agree=0.759, adj=0.395, (0 split) ## ## Node number 3: 51 observations, complexity param=0.07485605 ## mean=37.58824, MSE=54.4677 ## left son=6 (34 obs) right son=7 (17 obs) ## Primary splits: ## rm &lt; 7.47 to the left, improve=0.72041550, (0 missing) ## lstat &lt; 3.99 to the right, improve=0.34223650, (0 missing) ## ptratio &lt; 15.05 to the right, improve=0.21227430, (0 missing) ## rad &lt; 2.5 to the left, improve=0.10053340, (0 missing) ## tax &lt; 267 to the right, improve=0.07935891, (0 missing) ## Surrogate splits: ## lstat &lt; 3.99 to the right, agree=0.824, adj=0.471, (0 split) ## indus &lt; 1.215 to the right, agree=0.706, adj=0.118, (0 split) ## chas &lt; 0.5 to the left, agree=0.706, adj=0.118, (0 split) ## tax &lt; 225 to the right, agree=0.706, adj=0.118, (0 split) ## crim &lt; 1.3713 to the left, agree=0.686, adj=0.059, (0 split) ## ## Node number 4: 114 observations, complexity param=0.03552748 ## mean=15.33333, MSE=21.50994 ## left son=8 (77 obs) right son=9 (37 obs) ## Primary splits: ## crim &lt; 0.69916 to the right, improve=0.3873341, (0 missing) ## nox &lt; 0.6615 to the right, improve=0.3541892, (0 missing) ## dis &lt; 2.3497 to the left, improve=0.3182514, (0 missing) ## ptratio &lt; 19.45 to the right, improve=0.3102781, (0 missing) ## tax &lt; 567.5 to the right, improve=0.2823826, (0 missing) ## Surrogate splits: ## ptratio &lt; 19.95 to the right, agree=0.895, adj=0.676, (0 split) ## indus &lt; 14.345 to the right, agree=0.868, adj=0.595, (0 split) ## nox &lt; 0.5825 to the right, agree=0.868, adj=0.595, (0 split) ## tax &lt; 397 to the right, agree=0.868, adj=0.595, (0 split) ## rad &lt; 16 to the right, agree=0.860, adj=0.568, (0 split) ## ## Node number 5: 172 observations, complexity param=0.03672387 ## mean=23.16395, MSE=23.11579 ## left son=10 (82 obs) right son=11 (90 obs) ## Primary splits: ## lstat &lt; 9.645 to the right, improve=0.24693150, (0 missing) ## rm &lt; 6.543 to the left, improve=0.17749260, (0 missing) ## ptratio &lt; 17.85 to the right, improve=0.07815189, (0 missing) ## nox &lt; 0.5125 to the right, improve=0.07760816, (0 missing) ## tax &lt; 267.5 to the right, improve=0.07238020, (0 missing) ## Surrogate splits: ## nox &lt; 0.5125 to the right, agree=0.756, adj=0.488, (0 split) ## indus &lt; 7.625 to the right, agree=0.750, adj=0.476, (0 split) ## rm &lt; 6.26 to the left, agree=0.738, adj=0.451, (0 split) ## age &lt; 65.25 to the right, agree=0.727, adj=0.427, (0 split) ## dis &lt; 3.8824 to the left, agree=0.709, adj=0.390, (0 split) ## ## Node number 6: 34 observations ## mean=33.15882, MSE=13.41419 ## ## Node number 7: 17 observations ## mean=46.44706, MSE=18.85661 ## ## Node number 8: 77 observations, complexity param=0.0110349 ## mean=13.33247, MSE=15.64998 ## left son=16 (37 obs) right son=17 (40 obs) ## Primary splits: ## lstat &lt; 20.1 to the right, improve=0.24481010, (0 missing) ## crim &lt; 15.718 to the right, improve=0.23250740, (0 missing) ## dis &lt; 2.0037 to the left, improve=0.17113480, (0 missing) ## nox &lt; 0.6615 to the right, improve=0.11757680, (0 missing) ## rm &lt; 5.5675 to the left, improve=0.09054612, (0 missing) ## Surrogate splits: ## dis &lt; 1.9733 to the left, agree=0.792, adj=0.568, (0 split) ## rm &lt; 5.632 to the left, agree=0.727, adj=0.432, (0 split) ## age &lt; 95.35 to the right, agree=0.675, adj=0.324, (0 split) ## crim &lt; 9.08499 to the right, agree=0.662, adj=0.297, (0 split) ## black &lt; 396.295 to the right, agree=0.623, adj=0.216, (0 split) ## ## Node number 9: 37 observations ## mean=19.4973, MSE=8.034858 ## ## Node number 10: 82 observations ## mean=20.66098, MSE=6.55677 ## ## Node number 11: 90 observations, complexity param=0.01695185 ## mean=25.44444, MSE=27.29425 ## left son=22 (83 obs) right son=23 (7 obs) ## Primary splits: ## age &lt; 86.7 to the left, improve=0.1844883, (0 missing) ## lstat &lt; 4.46 to the right, improve=0.1773076, (0 missing) ## dis &lt; 3.0037 to the right, improve=0.1652768, (0 missing) ## crim &lt; 0.628575 to the left, improve=0.1203635, (0 missing) ## nox &lt; 0.5585 to the left, improve=0.1122403, (0 missing) ## Surrogate splits: ## nox &lt; 0.5585 to the left, agree=0.978, adj=0.714, (0 split) ## dis &lt; 2.1491 to the right, agree=0.978, adj=0.714, (0 split) ## crim &lt; 0.643205 to the left, agree=0.967, adj=0.571, (0 split) ## indus &lt; 16.57 to the left, agree=0.956, adj=0.429, (0 split) ## ptratio &lt; 14.75 to the right, agree=0.956, adj=0.429, (0 split) ## ## Node number 16: 37 observations ## mean=11.2973, MSE=10.14026 ## ## Node number 17: 40 observations ## mean=15.215, MSE=13.37128 ## ## Node number 22: 83 observations, complexity param=0.01422576 ## mean=24.79277, MSE=13.56694 ## left son=44 (55 obs) right son=45 (28 obs) ## Primary splits: ## rm &lt; 6.543 to the left, improve=0.3377388, (0 missing) ## lstat &lt; 5.41 to the right, improve=0.2548210, (0 missing) ## tax &lt; 267.5 to the right, improve=0.2210129, (0 missing) ## ptratio &lt; 18.05 to the right, improve=0.1394682, (0 missing) ## dis &lt; 6.4889 to the right, improve=0.1125739, (0 missing) ## Surrogate splits: ## lstat &lt; 5.055 to the right, agree=0.783, adj=0.357, (0 split) ## ptratio &lt; 15.75 to the right, agree=0.723, adj=0.179, (0 split) ## crim &lt; 0.39646 to the left, agree=0.699, adj=0.107, (0 split) ## chas &lt; 0.5 to the left, agree=0.687, adj=0.071, (0 split) ## age &lt; 74.15 to the left, agree=0.687, adj=0.071, (0 split) ## ## Node number 23: 7 observations ## mean=33.17143, MSE=125.3192 ## ## Node number 44: 55 observations ## mean=23.26545, MSE=8.880443 ## ## Node number 45: 28 observations ## mean=27.79286, MSE=9.189949 rpart.plot(boston.rpart) Rysunek 5.1: Drzewo regresyjne peÅ‚ne Przycinamy drzewoâ€¦ printcp(boston.rpart) ## ## Regression tree: ## rpart(formula = medv ~ ., data = boston.train) ## ## Variables actually used in tree construction: ## [1] age crim lstat rm ## ## Root node error: 26734/337 = 79.33 ## ## n= 337 ## ## CP nsplit rel error xerror xstd ## 1 0.498398 0 1.00000 1.00869 0.102595 ## 2 0.157251 1 0.50160 0.54429 0.061257 ## 3 0.074856 2 0.34435 0.40320 0.051393 ## 4 0.036724 3 0.26949 0.31278 0.045992 ## 5 0.035527 4 0.23277 0.29745 0.045608 ## 6 0.016952 5 0.19724 0.25532 0.040230 ## 7 0.014226 6 0.18029 0.27138 0.040991 ## 8 0.011035 7 0.16607 0.27448 0.041078 ## 9 0.010000 8 0.15503 0.27204 0.041193 plotcp(boston.rpart) boston.rpart2 &lt;- prune(boston.rpart, cp = 0.016952) rpart.plot(boston.rpart2) Rysunek 5.2: Drzewo regresyjne przyciÄ™te Predykcja na podstawie drzewa na zbiorze testowym. boston.pred &lt;- predict(boston.rpart2, newdata = boston.test) rmse &lt;- function(pred, obs) sqrt(1/length(pred)*sum((pred-obs)^2)) rmse(boston.pred, boston.test$medv) ## [1] 5.830722 Teraz zbudujemy model metodÄ… bagging. library(randomForest) boston.bag &lt;- randomForest(medv~., data = boston.train, mtry = ncol(boston.train)-1) boston.bag ## ## Call: ## randomForest(formula = medv ~ ., data = boston.train, mtry = ncol(boston.train) - 1) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 13 ## ## Mean of squared residuals: 12.03374 ## % Var explained: 84.83 Predykcja na podstawie modelu boston.pred2 &lt;- predict(boston.bag, newdata = boston.test) rmse(boston.pred2, boston.test$medv) ## [1] 4.359119 Zatem predykcja na podstawie modelu bagging jest nico lepsza niÅ¼ z pojedynczego drzewa. Dodatkowo moÅ¼emy oceniÄ‡ waÅ¼noÅ›Ä‡ zmiennych uÅ¼ytych w budowie drzew. varImpPlot(boston.bag) Rysunek 5.3: Wykres waÅ¼noÅ›ci predyktorÃ³w importance(boston.bag) ## IncNodePurity ## crim 1335.62584 ## zn 21.35274 ## indus 134.28748 ## chas 24.07230 ## nox 423.26229 ## rm 15413.69291 ## age 380.78172 ## dis 1204.86690 ## rad 88.28151 ## tax 454.99800 ## ptratio 309.58412 ## black 216.15512 ## lstat 6217.95834 x$variable.importance ## rm lstat indus ptratio crim age ## 16276.30598 9170.91941 4427.10554 4039.00112 3412.53062 3170.82658 ## nox dis zn tax rad chas ## 3063.70694 2681.24858 1306.29569 800.17910 539.07271 262.60146 ## black ## 63.78554 W porÃ³wnaniu do waÅ¼noÅ›ci zmiennych dla pojedynczego drzewa widaÄ‡ pewne rÃ³Å¼nice. 5.2 Lasy losowe Lasy losowe sÄ… uogÃ³lnieniem metody bagging, polegajÄ…cÄ… na losowaniu dla kaÅ¼dego drzewa wchodzÄ…cego w skÅ‚ad lasu \\(m\\) predyktorÃ³w spoÅ›rÃ³d \\(p\\) dostÄ™pnych, a nastÄ™pnie budowaniu drzew z wykorzystaniem tylko tych predyktorÃ³w (Ho 1995). DziÄ™ki temu za kaÅ¼dy razem drzewo jest budowane w oparciu o nowy zestaw cech (najczÄ™Å›ciej przyjmujemy \\(m=\\sqrt{p}\\)). W przypadku modeli bagging za kaÅ¼dym razem najsilniejszy predyktor wchodziÅ‚ w skÅ‚ad zbioru uczÄ…cego, a co za tym idzie rÃ³wnieÅ¼ uczestniczyÅ‚ w tworzeniu reguÅ‚ podziaÅ‚u. WÃ³wczas wiele drzew zawieraÅ‚o reguÅ‚y stosujÄ…ce dany atrybut, a wtedy predykcje otrzymywane za pomocÄ… drzew byÅ‚y skorelowane. Dlatego nawet duÅ¼a liczba prÃ³b bootstrapowych nie zapewniaÅ‚a poprawy precyzji. Implementacja tej metody znajduje siÄ™ w pakiecie randomForest. PrzykÅ‚ad 5.2 KontynuujÄ…c poprzedni przykÅ‚ad 5.1 moÅ¼emy zbudowaÄ‡ las losowy aby przekonaÄ‡ siÄ™ czy nastÄ…pi poprawa predykcji zmiennej wynikowej. boston.rf &lt;- randomForest(medv~., data = boston.train) boston.rf ## ## Call: ## randomForest(formula = medv ~ ., data = boston.train) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 4 ## ## Mean of squared residuals: 12.05123 ## % Var explained: 84.81 PorÃ³wnanie MSE na prÃ³bach uczÄ…cych pomiÄ™dzy lasem losowym i modelem bagging wypada nieco na korzyÅ›Ä‡ bagging. boston.pred3 &lt;- predict(boston.rf, newdata = boston.test) rmse(boston.pred3, boston.test$medv) ## [1] 3.79973 WaÅ¼noÅ›Ä‡ zmiennych rÃ³wnieÅ¼ siÄ™ nieco rÃ³Å¼ni. varImpPlot(boston.rf) 5.3 Boosting RozwaÅ¼ania na temat metody boosting zaczÄ™Å‚y siÄ™ od pytaÅ„ postawionych w publikacji Kearns and Valiant (1989), czy da siÄ™ na podstawie na podstawie zbioru sÅ‚abych modeli stworzyÄ‡ jeden dobry? Odpowiedzi pozytywnej na nie udzielili, najpierw Schapire (1990), a potem Breiman (1998). W metodzie boosting nie stosuje siÄ™ prÃ³b bootstrapowych ale odpowiednio modyfikuje siÄ™ drzewo wyjÅ›ciowe w kolejnych krokach na tym samym zbiorze uczÄ…cym. Algorytm dla drzewa regresyjnego jest nastÄ™pujÄ…cy: Ustal \\(\\hat{f}(x)=0\\) i \\(r_i=y_i\\) dla kaÅ¼dego \\(i\\) w zbiorze uczÄ…cym. Dla \\(b=1,2,\\ldots, B\\) powtarzaj: naucz drzewo \\(\\hat{f}^b\\) o \\(d\\) reguÅ‚ach podziaÅ‚u (czyli \\(d+1\\) liÅ›ciach) na zbiorze \\((X_i, r_i)\\), zaktualizuj drzewo do nowej â€œskurczonejâ€ wersji \\[\\begin{equation} \\hat{f}(x)\\leftarrow \\hat{f}(x)+\\lambda\\hat{f}^b(x), \\end{equation}\\] zaktualizuj reszty \\[\\begin{equation} r_i\\leftarrow r_i-\\lambda\\hat{f}^b(x_i). \\end{equation}\\] Wyznacz boosted model \\[\\begin{equation} \\hat{f}(x) = \\sum_{b=1}^B\\lambda\\hat{f}^b(x) \\end{equation}\\] Uczenie drzew klasyfikacyjnego metoda boosting przebiega w podobny sposÃ³b. Wynik uczenia drzew metodÄ… boosting zaleÅ¼y od trzech parametrÃ³w: Liczby drzew \\(B\\). W przeciwieÅ„stwie do metody bagging i lasÃ³w losowych, zbyt duÅ¼e \\(B\\) moÅ¼e doprowadziÄ‡ do przeuczenia modelu. \\(B\\) ustala siÄ™ najczÄ™Å›ciej na podstawie walidacji krzyÅ¼owej. Parametru â€œkurczeniaâ€ (ang. shrinkage) \\(\\lambda\\). Kontroluje on szybkoÅ›Ä‡ uczenia siÄ™ kolejnych drzew. Typowe wartoÅ›ci \\(\\lambda\\) to 0.01 lub 0.001. Bardzo maÅ‚e \\(\\lambda\\) moÅ¼e wymagaÄ‡ dobrania wiÄ™kszego \\(B\\), aby zapewniÄ‡ dobrÄ… jakoÅ›Ä‡ predykcyjnÄ… modelu. Liczby podziaÅ‚Ã³w w drzewach \\(d\\), ktÃ³ra decyduje o zÅ‚oÅ¼onoÅ›ci drzewa. Bywa, Å¼e nawet \\(d=1\\) daje dobre rezultaty, poniewaÅ¼ model wÃ³wczas uczy siÄ™ powoli. ImplementacjÄ™ metody boosting moÅ¼na znaleÅºÄ‡ w pakiecie gbm (Greenwell et al. 2019) PrzykÅ‚ad 5.3 MetodÄ™ boosting zastosujemy do zadania predykcji ceny mieszkaÅ„ na przedmieÅ›ciach Bostonu. DobÃ³r parametrÃ³w modelu bÄ™dzie arbitralny, wiÄ™c niekoniecznie model bÄ™dzie najlepiej dopasowany. library(gbm) boston.boost &lt;- gbm(medv~., data = boston.train, distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 2, shrinkage = 0.01) boston.boost ## gbm(formula = medv ~ ., distribution = &quot;gaussian&quot;, data = boston.train, ## n.trees = 5000, interaction.depth = 2, shrinkage = 0.01) ## A gradient boosted model with gaussian loss function. ## 5000 iterations were performed. ## There were 13 predictors of which 13 had non-zero influence. summary(boston.boost) ## var rel.inf ## rm rm 38.3955886 ## lstat lstat 29.4805422 ## dis dis 9.0886721 ## crim crim 5.7399540 ## nox nox 3.7754214 ## ptratio ptratio 3.2740541 ## black black 3.1164954 ## age age 2.9063950 ## tax tax 1.8433918 ## chas chas 0.9067974 ## indus indus 0.7627923 ## rad rad 0.5523485 ## zn zn 0.1575472 Predykcja na podstawie metody boosting boston.pred4 &lt;- predict(boston.boost, newdata = boston.test, n.trees = 5000) rmse(boston.pred4, boston.test$medv) ## [1] 3.801233 \\(RMSE\\) jest w tym przypadku nieco wiÄ™ksze niÅ¼ w lasach losowych ale sporo mniejsze niÅ¼ w metodzie bagging. Wszystkie metody wzmacnianych drzew dajÄ… wyniki lepsze niÅ¼ pojedyncze drzewa. Bibliografia "],
["klasyfikatory-liniowe.html", "6 Klasyfikatory liniowe 6.1 Reprezentacja progowa 6.2 Reprezentacja logitowa 6.3 Wady klasyfikatorÃ³w liniowych", " 6 Klasyfikatory liniowe ObszernÄ… rodzinÄ™ klasyfikatorÃ³w stanowiÄ… modele liniowe (ang. linear classification models). Klasyfikacji w tej rodzinie technik dokonuje siÄ™ na podstawie modeli funkcji kombinacji liniowej predyktorÃ³w. Jest to ujÄ™cie parametryczne, w ktÃ³rym klasyfikacji nowej wartoÅ›ci dokonujemy na podstawie atrybutÃ³w obserwacji i wektora parametrÃ³w. Uczenie na podstawie zestawu treningowego polega na oszacowaniu parametrÃ³w modelu. W odrÃ³Å¼nieniu od metod nieparametrycznych postaÄ‡ modelu tym razem jest znana. KaÅ¼dy klasyfikator liniowy skÅ‚ad siÄ™ z funkcji wewnÄ™trznej (ang. inner representation function) i funkcji zewnÄ™trznej (ang. outer representation function). Pierwsza jest funkcjÄ… rzeczywistÄ… parametrÃ³w modelu i wartoÅ›ci atrybutÃ³w obserwacji \\[\\begin{equation} g(x) = F(\\mathbf{a}(x),\\mathbf{w})=\\sum_{i=0}^pw_ia_i(x)=\\mathbf{w}\\circ \\mathbf{a}(x), \\end{equation}\\] przyjmujÄ…c, Å¼e \\(a_0(x)=1\\). Funkcja zewnÄ™trzna przyporzÄ…dkowuje binarnie klasy na podstawie wartoÅ›ci funkcji wewnÄ™trznej. IstniejÄ… dwa gÅ‚Ã³wne typy tych klasyfikacji: brzegowa - przyjmujemy, Å¼e funkcje wewnÄ™trzne tworzÄ… granice zbiorÃ³w obserwacji rÃ³Å¼nych klas, probabilistyczna - bazujÄ…ca na tym, Å¼e funkcje wewnÄ™trzne mogÄ… poÅ›rednio wykazywaÄ‡ prawdopodobieÅ„stwo przynaleÅ¼noÅ›ci do danej klasy. Pierwsza dzieli przestrzeÅ„ obserwacji za pomocÄ… hiperpÅ‚aszczyzn na obszary jednorodne pod wzglÄ™dem przynaleÅ¼noÅ›ci do klas. Druga jest prÃ³bÄ… parametrycznej reprezentacji prawdopodobieÅ„stw przynaleÅ¼noÅ›ci do klas. Klasyfikacji na podstawie prawdopodobieÅ„stw moÅ¼na dokonaÄ‡ na rÃ³Å¼ne sposoby, stosujÄ…c: najwiÄ™ksze prawdopodobieÅ„stwo, funkcjÄ™ najmniejszego kosztu bÅ‚Ä™dnej klasyfikacji, krzywych ROC (ang. Receiver Operating Characteristic - o tym pÃ³Åºniej). PodejÅ›cie brzegowe lub probabilistyczne prowadzi najczÄ™Å›ciej do dwÃ³ch typÃ³w reprezentacji funkcji zewnÄ™trznej: reprezentacji progowej (ang. threshold representation) - najczÄ™Å›ciej przy podejÅ›ciu brzegowym, reprezentacji logistycznej (ang. logit representation) - przy podejÅ›ciu probabilistycznym. 6.1 Reprezentacja progowa W przypadku klasyfikacji dwustanowej, dziedzina jest dzielona na dwa regiony (pozytywny i negatywny) poprzez porÃ³wnanie funkcji zewnÄ™trznej z wartoÅ›ciÄ… progowÄ…. Bez straty ogÃ³lnoÅ›ci moÅ¼na sprawiÄ‡, Å¼e bÄ™dzie to wartoÅ›Ä‡ 0 \\[\\begin{equation} h(x)=H(g(x))= \\begin{cases} 1, &amp;\\text{ jeÅ›li } g(x)\\geq 0\\\\ 0, &amp;\\text{ w przeciwnym przypadku.} \\end{cases} \\end{equation}\\] Czasami uÅ¼ywa siÄ™ parametryzacji \\(\\{-1,1\\}\\). Przez porÃ³wnanie \\(g(x)\\) z 0 definiuje siÄ™ hiperpÅ‚aszczyznÄ™ w \\(p\\) wymiarowej przestrzeni, ktÃ³ra rozdziela dziedzinÄ™ na regiony pozytywne i negatywne. W tym ujÄ™ciu mÃ³wimy o liniowej separowalnoÅ›ci obserwacji rÃ³Å¼nych klas, jeÅ›li istnieje hiperpÅ‚aszczyzna je rozdzielajÄ…ca. 6.2 Reprezentacja logitowa Najbardziej popularnÄ… reprezentacjÄ… parametrycznÄ… stosowanÄ… w klasyfikacji jest reprezentacja logitowa \\[\\begin{equation} \\P(y=1|x)=\\frac{e^{g(x)}}{e^{g(x)}+1}. \\end{equation}\\] WÃ³wczas \\(g(x)\\) nie reprezentuje bezpoÅ›rednio \\(\\P(y=1|x)\\) ale jego logit \\[\\begin{equation} g(x)=\\logit(\\P(y=1|x)), \\end{equation}\\] gdzie \\(\\logit(p)=\\ln\\frac{p}{1-p}\\). Dlatego wÅ‚aÅ›ciwa postaÄ‡ reprezentacji jest nastÄ™pujÄ…ca \\[\\begin{equation} \\P(y=1|x)=\\logit^{-1}(g(x)). \\end{equation}\\] W ten sposÃ³b reprezentacja logitowa jest rÃ³wnowaÅ¼na reprezentacji progowej, poniewaÅ¼ \\[\\begin{equation} g(x)=\\ln\\frac{\\P(y=1|x)}{1-\\P(y=1|x)}=\\ln\\frac{\\P(y=1|x)}{\\P(y=0|x)}&gt;0. \\end{equation}\\] Jednak zaletÄ… reprezentacji logitowej, w porÃ³wnaniu do progowej, jest to, Å¼e moÅ¼na wyznaczyÄ‡ prawdopodobieÅ„stwa przynaleÅ¼noÅ›ci do obu klas. W przypadku klasyfikacji wielostanowej uczymy tyle funkcji \\(h\\) ile jest klas. 6.3 Wady klasyfikatorÃ³w liniowych tylko w przypadku prostych funkcji wewnÄ™trznych jesteÅ›my w stanie oceniÄ‡ wpÅ‚yw poszczegÃ³lnych predykorÃ³w na klasyfikacjÄ™, jakoÅ›Ä‡ predykcji zaleÅ¼y od doboru funkcji wewnÄ™trznej (liniowa w Å›cisÅ‚ym sensie jest najczÄ™Å›ciej niewystarczajÄ…ca), nie jest w stanie klasyfikowaÄ‡ poprawnie stanÃ³w (nie jest liniowo separowalna) w zagadnieniach typu XOR. "],
["regresja-logistyczna.html", "7 Regresja logistyczna 7.1 Model 7.2 Estymacja parametrÃ³w modelu 7.3 Interpretacja", " 7 Regresja logistyczna 7.1 Model Regresja logistyczna (ang. logistic regression) jest technikÄ… z rodziny klasyfikatorÃ³w liniowych z reprezentacjÄ… logistycznÄ…, a formalnie naleÅ¼y do rodziny uogÃ³lnionych modeli liniowych (GLM). Stosowana jest wÃ³wczas, gdy zmienna wynikowa posiada dwa stany (sukces i poraÅ¼ka), kodowane najczÄ™Å›ciej za pomocÄ… 1 i 0. W tej metodzie modelowane jest warunkowe prawdopodobieÅ„stwo sukcesu za pomocÄ… kombinacji liniowej predyktorÃ³w \\(X\\). OgÃ³lna postaÄ‡ modelu \\[\\begin{align} Y\\sim &amp;B(1, p)\\\\ p(X)=&amp;\\E(Y|X)=\\frac{\\exp(\\beta X)}{1+\\exp(\\beta X)}, \\end{align}\\] gdzie \\(B(1,p)\\) jest rozkÅ‚adem dwumianowym o prawdopodobieÅ„stwie sukcesu \\(p\\), a \\(\\beta X\\) oznacza kombinacjÄ™ liniowÄ… parametrÃ³w modelu i wartoÅ›ci zmiennych niezaleÅ¼nych, przyjmujÄ…c, Å¼e \\(x_0=1\\). Jako funkcji Å‚Ä…czÄ…cej (czyli opisujÄ…cej zwiÄ…zek miÄ™dzy kombinacjÄ… liniowÄ… predyktorÃ³w i prawdopodobieÅ„stwem sukcesu) uÅ¼yto logitu. Pozwala on na wygodnÄ… interpretacjÄ™ wynikÃ³w w terminach szans. SzansÄ… (ang. odds) nazywamy stosunek prawdopodobieÅ„stwa sukcesu do prawdopodobieÅ„stwa poraÅ¼ki \\[\\begin{equation} o = \\frac{p}{1-p}. \\end{equation}\\] PoniewaÅ¼ bÄ™dziemy przyjmowali, Å¼e \\(p\\in (0,1)\\), to \\(o\\in (0,\\infty)\\), a jej logarytm naleÅ¼y do przedziaÅ‚u \\((-\\infty, \\infty)\\). Zatem logarytm szansy jest kombinacjÄ… liniowÄ… predyktorÃ³w \\[\\begin{equation} \\log\\left[\\frac{p(X)}{1-p(X)}\\right]=\\beta_0+\\beta_1x_1+\\ldots+\\beta_dx_d. \\end{equation}\\] 7.2 Estymacja parametrÃ³w modelu Estymacji parametrÃ³w modelu logistycznego dokonujemy za pomocÄ… metody najwiÄ™kszej wiarogodnoÅ›ci. Funkcja wiarogodnoÅ›ci w tym przypadku przyjmuje postaÄ‡ \\[\\begin{equation} L(X_1,\\ldots,X_n,\\beta)=\\prod_{i=1}^{n}p(X_i)^Y_i[1-p(X_i)]^{1-Y_i}, \\end{equation}\\] gdzie wektor \\(\\beta\\) jest uwikÅ‚any w funkcji \\(p(X_i)\\). Maksymalizacji dokonujemy raczej po naÅ‚oÅ¼eniu na funkcjÄ™ wiarogodnoÅ›ci logarytmu, bo to uÅ‚atwia szukanie ekstremum. \\[\\begin{equation} \\log L(X_1,\\ldots,X_n,\\beta) = \\sum_{i=1}^n(Y_i\\log p(X_i)+(1-Y_i)\\log(1-p(X_i))). \\end{equation}\\] 7.3 Interpretacja Interpretacja (lat. ceteris paribus - â€œinne takie samoâ€) poszczegÃ³lnych parametrÃ³w modelu jest nastÄ™pujÄ…ca: jeÅ›li \\(b_i&gt;0\\) - to zmienna \\(x_i\\) ma wpÅ‚yw stymulujÄ…cy pojawienie siÄ™ sukcesu, jeÅ›li \\(b_i&lt;0\\) - to zmienna \\(x_i\\) ma wpÅ‚yw ograniczajÄ…cy pojawienie siÄ™ sukcesu, jeÅ›li \\(b_i=0\\) - to zmienna \\(x_i\\) nie ma wpÅ‚ywu na pojawienie siÄ™ sukcesu. Iloraz szans (ang. odds ratio) stosuje siÄ™ w przypadku porÃ³wnywania dwÃ³ch klas obserwacji. Jest on jak sama nazwa wskazuje ilorazem szans zajÅ›cia sukcesu w obu klasach \\[\\begin{equation} OR = \\frac{p_1}{1-p_1}\\frac{1-p_2}{p_2}, \\end{equation}\\] gdzie \\(p_i\\) oznacza zajÅ›cie sukcesu w \\(i\\)-tej klasie. Interpretujemy go nastÄ™pujÄ…co: jeÅ›li \\(OR&gt;1\\) - to w pierwszej grupie zajÅ›cie sukcesu jest bardziej prawdopodobne, jeÅ›li \\(OR&lt;1\\) - to w drugiej grupie zajÅ›cie sukcesu jest bardziej prawdopodobne, jeÅ›li \\(OR=1\\) - to w obu grupach zajÅ›cie sukcesu jest jednakowo prawdopodobne. PrzykÅ‚ad 7.1 Jako ilustracjÄ™ dziaÅ‚ania regresji logistycznej uÅ¼yjemy modelu dla danych ze zbioru Default pakietu ISLR. library(ISLR) head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 ZmiennÄ… zaleÅ¼nÄ… jest default, a pozostaÅ‚e sÄ… predyktorami. najpierw dokonamy podziaÅ‚u prÃ³by na uczÄ…ca i testowÄ…, a nastÄ™pnie zbudujemy model. set.seed(2019) ind &lt;- sample(1:nrow(Default), size = 2/3*nrow(Default)) dt.ucz &lt;- Default[ind,] dt.test &lt;- Default[-ind,] mod.logit &lt;- glm(default~., dt.ucz, family = binomial(&quot;logit&quot;)) summary(mod.logit) ## ## Call: ## glm(formula = default ~ ., family = binomial(&quot;logit&quot;), data = dt.ucz) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1913 -0.1410 -0.0537 -0.0192 3.7527 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.128e+01 6.169e-01 -18.287 &lt;2e-16 *** ## studentYes -4.627e-01 2.862e-01 -1.617 0.106 ## balance 5.830e-03 2.883e-04 20.221 &lt;2e-16 *** ## income 9.460e-06 9.833e-06 0.962 0.336 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1967.2 on 6665 degrees of freedom ## Residual deviance: 1046.5 on 6662 degrees of freedom ## AIC: 1054.5 ## ## Number of Fisher Scoring iterations: 8 Tylko income nie ma Å¼adnego wpÅ‚ywu na prawdopodobieÅ„stwo stanu Yes zmiennej default. Zmienna balance wpÅ‚ywa stymulujÄ…co na prawdopodobieÅ„stwo pojawienia siÄ™ sukcesu. Natomiast jeÅ›li badana osoba jest studentem (studentYes), to ma wpÅ‚yw ograniczajÄ…cy na pojawienie siÄ™ sukcesu. ChcÄ…c porÃ³wnaÄ‡ dwie grupy obserwacji, przykÅ‚adowo studentÃ³w z nie-studentami, moÅ¼emy wykorzystaÄ‡ iloraz szans. exp(cbind(OR = coef(mod.logit), confint(mod.logit))) %&gt;% kable(digits = 4) OR 2.5 % 97.5 % (Intercept) 0.0000 0.0000 0.0000 studentYes 0.6296 0.3598 1.1060 balance 1.0058 1.0053 1.0064 income 1.0000 1.0000 1.0000 Z powyÅ¼szej tabeli wynika, Å¼e bycie studentem zmniejsza szanse na Yes w zmiennej default o okoÅ‚o 37% (w stosunku do nie-studentÃ³w). Natomiast wzrost zmiennej balance przy zachowaniu pozostaÅ‚ych zmiennych na tym samym poziomie skutkuje wzrostem szans na Yes o okoÅ‚o 0.6%. ChcÄ…c przeprowadziÄ‡ predykcjÄ™ na podstawie modelu dla ustalonych wartoÅ›ci cech (np. student = Yes, balance = $1000 i income = $40000) postÄ™pujemy nastÄ™pujÄ…co dt.new &lt;- data.frame(student = &quot;Yes&quot;, balance = 1000, income = 40000) predict(mod.logit, newdata = dt.new, type = &quot;response&quot;) ## 1 ## 0.003931398 Otrzymany wynik jest oszacowanym prawdopodobieÅ„stwem warunkowym wystÄ…pienia sukcesu (default = Yes). WidaÄ‡ zatem, Å¼e poziomy badanych cech sprzyjajÄ… raczej poraÅ¼ce. JeÅ›li chcemy sprawdziÄ‡ jakoÅ›Ä‡ klasyfikacji na zbiorze testowym, to musimy ustaliÄ‡ na jakim poziomie prawdopodobieÅ„stwa bÄ™dziemy uznawaÄ‡ obserwacjÄ™ za sukces. W zaleÅ¼noÅ›ci od tego, na predykcji jakiego stanu zaleÅ¼y nam bardziej, moÅ¼emy rÃ³Å¼nie dobieraÄ‡ ten prÃ³g (bez Å¼adnych dodatkowych przesÅ‚anek najczÄ™Å›ciej jest to 0.5). pred &lt;- predict(mod.logit, newdata = dt.test, type = &quot;response&quot;) pred.class &lt;- ifelse(pred &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) (tab &lt;- table(pred.class, dt.test$default)) ## ## pred.class No Yes ## No 3211 71 ## Yes 15 37 (acc &lt;- sum(diag(prop.table(tab)))) ## [1] 0.9742052 Klasyfikacja na poziomie 97% wskazuje na dobre dopasowanie modelu. "],
["LDA.html", "8 Analiza dyskryminacyjna 8.1 Liniowa analiza dyskryminacyjna Fisherâ€™a 8.2 Liniowa analiza dyskryminacyjna - podejÅ›cie probabilistyczne 8.3 Analiza dyskryminacyjna metodÄ… czÄ™Å›ciowych najmniejszych kwadratÃ³w 8.4 Regularyzowana analiza dyskryminacyjna 8.5 Analiza dyskryminacyjna mieszana 8.6 Elastyczna analiza dyskryminacyjna", " 8 Analiza dyskryminacyjna Analiza dyskryminacyjna (ang. discriminant analysis) jest grupÄ… technik dyskryminacji obserwacji wzglÄ™dem przynaleÅ¼noÅ›ci do klas. CzÄ™Å›Ä‡ z nich naleÅ¼y do klasyfikatorÃ³w liniowych (choÄ‡ nie zawsze w Å›cisÅ‚ym sensie). Za autorÃ³w tej metody uwaÅ¼a siÄ™ Fisherâ€™a (1936) i Welchâ€™a (1939). KaÅ¼dy z nich prezentowaÅ‚ nieco inne podejÅ›cie do tematu klasyfikacji. Welch poszukiwaÅ‚ klasyfikacji minimalizujÄ…cej prawdopodobieÅ„stwo bÅ‚Ä™dnej klasyfikacji, znane jako klasyfikatory bayesowskie. PodejÅ›cie Fisherâ€™a skupiaÅ‚o siÄ™ raczej na porÃ³wnaniu zmiennoÅ›ci miÄ™dzygrupowej do zmiennoÅ›ci wewnÄ…trzgrupowej. WychodzÄ…c z zaÅ‚oÅ¼enia, Å¼e iloraz tych wariancji powinien byÄ‡ stosunkowo duÅ¼y przy rÃ³Å¼nych klasach, jeÅ›li do ich opisu uÅ¼yjemy odpowiednich zmiennych niezaleÅ¼nych. W istocie chodzi o znalezienie takiego wektora, w kierunku ktÃ³rego wspomniany iloraz wariancji jest najwiÄ™kszy. 8.1 Liniowa analiza dyskryminacyjna Fisherâ€™a 8.1.1 Dwie kategorie zmiennej grupujÄ…cej Niech \\(\\boldsymbol D\\) bÄ™dzie zbiorem zawierajÄ…cym \\(n\\) punktÃ³w \\(\\{\\boldsymbol x, y\\}\\), gdzie \\(\\boldsymbol x\\in \\mathbb{R}^d\\), a \\(y\\in \\{c_1,\\ldots,c_k\\}\\). Niech \\(\\boldsymbol D_i\\) oznacza podzbiÃ³r punktÃ³w zbioru \\(\\boldsymbol D\\), ktÃ³re naleÅ¼Ä… do klasy \\(c_i\\), czyli \\(\\boldsymbol D_i=\\{\\boldsymbol x|y=c_i\\}\\) i niech \\(|\\boldsymbol D_i|=n_i\\). Na poczÄ…tek zaÅ‚Ã³Å¼my, Å¼e \\(\\boldsymbol D\\) skÅ‚ada siÄ™ tylko z \\(\\boldsymbol D_1\\) i \\(\\boldsymbol D_2\\). Niech \\(\\boldsymbol w\\) bÄ™dzie wektorem jednostkowym (\\(\\boldsymbol w&#39;\\boldsymbol w=1\\)), wÃ³wczas rzut ortogonalny punku \\(\\boldsymbol x_i\\) na wektor \\(\\boldsymbol w\\) moÅ¼na zapisaÄ‡ nastÄ™pujÄ…co \\[\\begin{equation} \\tilde{\\boldsymbol x}=\\left(\\frac{\\boldsymbol w&#39;\\boldsymbol x}{\\boldsymbol w&#39;\\boldsymbol w}\\right)\\boldsymbol w=(\\boldsymbol w&#39;\\boldsymbol x)\\boldsymbol w = a\\boldsymbol w, \\end{equation}\\] gdzie \\(a\\) jest wspÃ³Å‚rzÄ™dnÄ… punktu \\(\\tilde{\\boldsymbol x}\\) w kierunku wektora \\(\\boldsymbol w\\), czyli \\[\\begin{equation} a=\\boldsymbol w&#39;\\boldsymbol x. \\end{equation}\\] Zatem \\((a_1,\\ldots,a_n)\\) reprezentujÄ… odwzorowanie \\(\\mathbb{R}^d\\) w \\(\\mathbb{R}\\), czyli z \\(d\\)-wymiarowej przestrzeni w przestrzeÅ„ generowanÄ… przez \\(\\boldsymbol w\\). Rysunek 8.1: Rzut ortogonalny punktÃ³w w kierunku wektora \\(\\boldsymbol w\\) KaÅ¼dy punkt naleÅ¼y do pewnej klasy, dlatego moÅ¼emy wyliczyÄ‡ \\[\\begin{align} m_1=&amp;\\frac{1}{n_1}\\sum_{ \\boldsymbol x\\in \\boldsymbol D_1}a=\\\\ =&amp;\\frac{1}{n_1}\\sum_{ \\boldsymbol x\\in \\boldsymbol D_1} \\boldsymbol w&#39; \\boldsymbol x=\\\\ =&amp; \\boldsymbol w&#39;\\left(\\frac{1}{n_1}\\sum_{ \\boldsymbol x\\in \\boldsymbol D_1} \\boldsymbol x \\right)=\\\\ =&amp; \\boldsymbol w&#39; \\boldsymbol{\\mu}_1, \\tag{8.1} \\end{align}\\] gdzie \\(\\boldsymbol \\mu_1\\) jest wektorem Å›rednich punktÃ³w z \\(\\boldsymbol D_1\\). W podobny sposÃ³b moÅ¼na policzyÄ‡ \\(m_2 = \\boldsymbol w&#39; \\boldsymbol \\mu_2\\). Oznacza to, Å¼e Å›rednia projekcji jest projekcjÄ… Å›rednich. RozsÄ…dnym wydaje siÄ™ teraz poszukaÄ‡ takiego wektora, aby \\(|m_1-m_2|\\) byÅ‚a maksymalnie duÅ¼a przy zachowaniu niezbyt duÅ¼ej zmiennoÅ›ci wewnÄ…trz grup. Dlatego kryterium Fisherâ€™a przyjmuje postaÄ‡ \\[\\begin{equation} \\max_{ \\boldsymbol w}J(\\boldsymbol w)=\\frac{(m_1-m_2)^2}{ss_1^2+ss_2^2}, \\tag{8.2} \\end{equation}\\] gdzie \\(ss_j^2=\\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}(a-m_j)^2=n_j\\sigma_j^2.\\) ZauwaÅ¼my, Å¼e licznik w (8.2) da siÄ™ zapisaÄ‡ jako \\[\\begin{align} (m_1-m_2)^2=&amp; ( \\boldsymbol w&#39;( \\boldsymbol \\mu_1- \\boldsymbol \\mu_2))^2=\\\\ =&amp; \\boldsymbol w&#39;(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)&#39;\\boldsymbol w=\\\\ =&amp; \\boldsymbol w&#39; \\boldsymbol B \\boldsymbol w \\end{align}\\] gdzie \\(\\boldsymbol B=(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)(\\boldsymbol \\mu_1- \\boldsymbol \\mu_2)&#39;\\) jest macierzÄ… \\(d\\times d\\). Ponadto \\[\\begin{align} ss_j^2=&amp;\\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}(a-m_j)^2=\\\\ =&amp;\\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}( \\boldsymbol w&#39; \\boldsymbol x- \\boldsymbol w&#39; \\boldsymbol\\mu_j)^2=\\\\ =&amp; \\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}( \\boldsymbol{w}&#39;( \\boldsymbol{x}- \\boldsymbol{\\mu}_j))^2=\\\\ =&amp; \\boldsymbol{w}&#39;\\left(\\sum_{ \\boldsymbol x\\in \\boldsymbol D_j}(\\boldsymbol{x}-\\boldsymbol \\mu_j)(\\boldsymbol x- \\boldsymbol \\mu_j)&#39;\\right) \\boldsymbol{w}=\\\\ =&amp; \\boldsymbol{w}&#39; \\boldsymbol{S}_j \\boldsymbol{w}, \\tag{8.3} \\end{align}\\] gdzie \\(\\boldsymbol{S}_j=n_j \\boldsymbol{\\Sigma}_j\\). Zatem mianownik (8.2) moÅ¼emy zapisaÄ‡ jako \\[\\begin{equation} ss_1^2+ss_2^2= \\boldsymbol{w}&#39;(\\boldsymbol{S}_1+ \\boldsymbol{S}_2) \\boldsymbol{w}= \\boldsymbol{w}&#39; \\boldsymbol{S} \\boldsymbol{w}, \\end{equation}\\] gdzie \\(\\boldsymbol{S}=\\boldsymbol{S}_1+\\boldsymbol{S}_2\\). Ostatecznie warunek Fisherâ€™a przyjmuje postaÄ‡ \\[\\begin{equation} \\max_{ \\boldsymbol{w}}J( \\boldsymbol{w})=\\frac{ \\boldsymbol{w}&#39; \\boldsymbol{B} \\boldsymbol{w}}{ \\boldsymbol{w}&#39; \\boldsymbol{S} \\boldsymbol{w}}. \\tag{8.4} \\end{equation}\\] RÃ³Å¼niczkujÄ…c (8.4) po \\(\\boldsymbol{w}\\) i przyrÃ³wnujÄ…c go do 0, otrzymamy warunek \\[\\begin{equation} \\boldsymbol{B} \\boldsymbol{w} = \\lambda \\boldsymbol{S} \\boldsymbol{w}, \\tag{8.5} \\end{equation}\\] gdzie \\(\\lambda=J(\\boldsymbol{w})\\). Maksimum (8.5) jest osiÄ…gane dla wektora \\(\\boldsymbol{w}\\) rÃ³wnego wektorowi wÅ‚asnemu odpowiadajÄ…cemu najwiÄ™kszej wartoÅ›ci wÅ‚asnej rÃ³wnania charakterystycznego \\(|\\boldsymbol{B}-\\lambda\\boldsymbol{S}|=0\\). JeÅ›li \\(\\boldsymbol{S}\\) nie jest osobliwa, to rozwiÄ…zanie (8.5) otrzymujemy przez znalezienie najwiÄ™kszej wartoÅ›ci wÅ‚asnej macierzy \\(\\boldsymbol{B}\\boldsymbol{S}^{-1}\\) lub bez wykorzystania wartoÅ›ci i wektorÃ³w wÅ‚asnych. PoniewaÅ¼ \\(\\boldsymbol{B}\\boldsymbol w=\\left((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\right)\\boldsymbol{w}\\) jest macierzÄ… \\(d \\times 1\\) rzÄ™du 1, to \\(\\boldsymbol{B}\\boldsymbol{w}\\) jest punktem na kierunku wyznaczonym przez wektor \\(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2\\), bo \\[\\begin{align} \\boldsymbol{B}\\boldsymbol{w}=&amp; \\left((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\right)\\boldsymbol{w}=\\\\ =&amp;(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)((\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\boldsymbol{w})=\\\\ =&amp; b(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2), \\end{align}\\] gdzie \\(b = (\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)&#39;\\boldsymbol{w}\\) jest skalarem. WÃ³wczas (8.5) zapiszemy jako \\[\\begin{gather} b(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2) = \\lambda\\boldsymbol{S}\\boldsymbol{w}\\\\ \\boldsymbol{w}= \\frac{b}{\\lambda}\\boldsymbol{S}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2) \\end{gather}\\] A poniewaÅ¼ \\(b/\\lambda\\) jest liczbÄ…, to kierunek najlepszej dyskryminacji grup wyznacza wektor \\[\\begin{equation} \\boldsymbol{w}=\\boldsymbol{S}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2). \\end{equation}\\] Rysunek 8.2: Rzut ortogonalny w kierunku wektora \\(\\boldsymbol{w}\\), bÄ™dÄ…cego najlepiej dyskryminujÄ…cym obie grupy obserwacji 8.1.2 \\(k\\)-kategorii zmiennej grupujÄ…cej UogÃ³lnieniem tej teorii na przypadek \\(k\\) klas otrzymujemy przez uwzglÄ™dnienie \\(k-1\\) funkcji dyskryminacyjnych. ZmiennoÅ›Ä‡ wewnÄ…trzgrupowa przyjmuje wÃ³wczas postaÄ‡ \\[\\begin{equation} \\boldsymbol{S}_W=\\sum_{i=1}^k\\boldsymbol{S}_i, \\end{equation}\\] gdzie \\(\\boldsymbol{S}_i\\) jest zdefiniowane jak w (8.3). Niech Å›rednia i zmiennoÅ›Ä‡ caÅ‚kowita bÄ™dÄ… dane wzorami \\[\\begin{equation} \\boldsymbol{m}=\\frac{1}{n}\\sum_{i=1}^kn_i\\boldsymbol{m}_i, \\end{equation}\\] \\[\\begin{equation} \\boldsymbol{S}_T=\\sum_{j=1}^k\\sum_{\\boldsymbol{x}\\in D_j}(\\boldsymbol{x}-\\boldsymbol{m})(\\boldsymbol{x}-\\boldsymbol{m})&#39; \\end{equation}\\] gdzie \\(\\boldsymbol{m}_i\\) jest okreÅ›lone jak w (8.1). Wtedy zmiennoÅ›Ä‡ miÄ™dzygrupowÄ… moÅ¼emy wyraziÄ‡ jako \\[\\begin{equation} \\boldsymbol{S}_B=\\sum_{i=1}^kn_i(\\boldsymbol{m}_i-\\boldsymbol{m})(\\boldsymbol{m}_i-\\boldsymbol{m})&#39;, \\end{equation}\\] bo \\(\\boldsymbol{S}_T=\\boldsymbol{S}_W+\\boldsymbol{S}_B.\\) OkreÅ›lamy projekcjÄ™ \\(d\\)-wymiarowej przestrzeni na \\(k-1\\)-wymiarowÄ… przestrzeÅ„ za pomocÄ… \\(k-1\\) funkcji dyskryminacyjnych postaci \\[\\begin{equation} a_j=\\boldsymbol{w}_j&#39;\\boldsymbol{x}, \\quad j=1,\\ldots,k-1. \\end{equation}\\] PoÅ‚Ä…czone wszystkie \\(k-1\\) rzutÃ³w moÅ¼emy zapisaÄ‡ jako \\[\\begin{equation} \\boldsymbol{a}=\\boldsymbol{W}&#39;\\boldsymbol{x}. \\end{equation}\\] W nowej przestrzeni \\(k-1\\)-wymiarowej moÅ¼emy zdefiniowaÄ‡ \\[\\begin{equation} \\tilde{\\boldsymbol{m}}=\\frac{1}{n}\\sum_{i=1}^kn_i\\tilde{\\boldsymbol{m}}_i, \\end{equation}\\] gdzie \\(\\tilde{\\boldsymbol{m}}_i= \\frac{1}{n_i}\\sum_{\\boldsymbol{a}\\in A_i}\\boldsymbol{a}\\), a \\(A_i\\) jest projekcjÄ… obiektÃ³w z \\(i\\)-tej klasy na hiperpowierzchniÄ™ generowanÄ… przez \\(\\boldsymbol{W}\\). Dalej moÅ¼emy zdefiniowaÄ‡ zmiennoÅ›ci miedzy- i wewnÄ…trzgrupowe dla obiektÃ³w przeksztaÅ‚conych przez \\(\\boldsymbol{W}\\) \\[\\begin{align} \\tilde{\\boldsymbol{S}}_W=&amp;\\sum_{i=1}^k\\sum_{\\boldsymbol{a}\\in A_i}(\\boldsymbol{a}-\\tilde{\\boldsymbol{m}})(\\boldsymbol{a}-\\tilde{\\boldsymbol{m}})&#39;\\\\ \\tilde{\\boldsymbol{S}}_B=&amp;\\sum_{i=1}^kn_i(\\tilde{\\boldsymbol{m}}_i-\\tilde{\\boldsymbol{m}})(\\tilde{\\boldsymbol{m}}_i-\\tilde{\\boldsymbol{m}})&#39;. \\end{align}\\] Åatwo moÅ¼na zatem pokazaÄ‡, Å¼e \\[\\begin{align} \\tilde{\\boldsymbol{S}}_W = &amp; \\boldsymbol{W}&#39;\\boldsymbol{S}_W\\boldsymbol{W}\\\\ \\tilde{\\boldsymbol{S}}_B = &amp; \\boldsymbol{W}&#39;\\boldsymbol{S}_B\\boldsymbol{W}. \\end{align}\\] Ostatecznie warunek (8.2) w \\(k\\)-wymiarowym ujÄ™ciu moÅ¼na przedstawiÄ‡ jako \\[\\begin{equation} \\max_{\\boldsymbol{W}}J(\\boldsymbol{W})=\\frac{\\tilde{\\boldsymbol{S}}_W}{\\tilde{\\boldsymbol{S}}_B}=\\frac{\\boldsymbol{W}&#39;\\boldsymbol{S}_W\\boldsymbol{W}}{\\boldsymbol{W}&#39;\\boldsymbol{S}_B\\boldsymbol{W}}. \\end{equation}\\] Maksimum moÅ¼na znaleÅºÄ‡ poprzez rozwiÄ…zanie rÃ³wnania charakterystycznego \\[\\begin{equation} |\\boldsymbol{S}_B-\\lambda_i\\boldsymbol{S}_W|=0 \\end{equation}\\] dla kaÅ¼dego \\(i\\). PrzykÅ‚ad 8.1 Dla danych ze zbioru iris przeprowadzimy analizÄ™ dyskryminacji. ImplementacjÄ™ metody LDA znajdziemy w pakiecie MASS w postaci funkcji lda. Zaczynamy od standaryzacji zmiennych i podziaÅ‚u prÃ³by na uczÄ…cÄ… i testowÄ…. library(MASS) library(tidyverse) iris.std &lt;- iris %&gt;% mutate_if(is.numeric, scale) set.seed(44) ind &lt;- sample(nrow(iris.std), size = 100) dt.ucz &lt;- iris.std[ind,] dt.test &lt;- iris.std[-ind,] Budowa modelu mod.lda &lt;- lda(Species~., data = dt.ucz) mod.lda$prior ## setosa versicolor virginica ## 0.32 0.32 0.36 PrawdopodobieÅ„stwa a priori przynaleÅ¼noÅ›ci do klas przyjÄ™to na podstawie prÃ³by uczÄ…cej. mod.lda$means ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa -1.05240186 0.7790042 -1.2968064 -1.2536554 ## versicolor 0.04201557 -0.6764307 0.2521529 0.1607657 ## virginica 0.87352122 -0.2398799 1.0182730 1.0868584 W czÄ™Å›ci means wyÅ›wietlone sÄ… Å›rednie poszczegÃ³lnych zmiennych niezaleÅ¼nych w podziale na grupy. DziÄ™ki temu moÅ¼na okreÅ›liÄ‡ poÅ‚oÅ¼enia Å›rodkÃ³w ciÄ™Å¼koÅ›ci poszczegÃ³lnych klas w oryginalnej przestrzeni. mod.lda$scaling ## LD1 LD2 ## Sepal.Length 0.5884101 0.04738098 ## Sepal.Width 0.7566030 -0.97757574 ## Petal.Length -3.2910346 1.41170784 ## Petal.Width -2.3799488 -1.95325155 PowyÅ¼sza tabela zawiera wspÃ³Å‚rzÄ™dne wektorÃ³w wyznaczajÄ…cych funkcje dyskryminacyjne. Na ich podstawie moÅ¼emy okreÅ›liÄ‡, ktÃ³ra z nich wpÅ‚ywa najmocniej na tworzenie siÄ™ nowej przestrzeni. Obiekt svd przechowuje pierwiastki z \\(\\lambda_i\\), dlatego podnoszÄ…c je do kwadratu i dzielÄ…c przez ich sumÄ™ otrzymamy udziaÅ‚ poszczegÃ³lnych zmiennych w dyskryminacji przypadkÃ³w. Jak widaÄ‡ pierwsza funkcja dyskryminacyjna w zupeÅ‚noÅ›ci by wystarczyÅ‚a. mod.lda$svd^2/sum(mod.lda$svd^2) ## [1] 0.992052359 0.007947641 Klasyfikacja na podstawie modelu pred.lda &lt;- predict(mod.lda, dt.test) Wynik predykcji przechowuje trzy rodzaje obiektÃ³w: klasy, ktÃ³re przypisaÅ‚ obiektom model (class); prawdopodobieÅ„stwa a posteriori przynaleÅ¼noÅ›ci do klas na podstawie modelu (posterior); wspÃ³Å‚rzÄ™dne w nowej przestrzeni LD1, LD2 (x). Sprawdzenie jakoÅ›ci klasyfikacji tab &lt;- table(pred = pred.lda$class, obs = dt.test$Species) tab ## obs ## pred setosa versicolor virginica ## setosa 18 0 0 ## versicolor 0 17 1 ## virginica 0 1 13 sum(diag(prop.table(tab))) ## [1] 0.96 Jak widaÄ‡ z powyÅ¼szej tabeli model dobrze sobie radzi z klasyfikacjÄ… obiektÃ³w. Odsetek poprawnych klasyfikacji wynosi 96%. cbind.data.frame(obs = dt.test$Species, pred.lda$x, pred = pred.lda$class) %&gt;% ggplot(aes(x = LD1, y = LD2))+ geom_point(aes(color = pred, shape = obs), size = 2) Rysunek 8.3: Klasyfikacja w przestrzeni LD1, LD2 na podstawie modelu mod.lda 8.2 Liniowa analiza dyskryminacyjna - podejÅ›cie probabilistyczne Jak wspomniano na wstÄ™pie (patrz rozdziaÅ‚ 8), podejÅ›cie prezentowane przez Welcha polegaÅ‚o na minimalizacji prawdopodobieÅ„stwa popeÅ‚nienia bÅ‚Ä™du przy klasyfikacji. CaÅ‚a rodzina klasyfikatorÃ³w Bayesa (patrz rozdziaÅ‚ 9) polega na wyznaczeniu prawdopodobieÅ„stw a posteriori, na podstawie ktÃ³rych dokonuje siÄ™ decyzji o klasyfikacji obiektÃ³w. Tym razem dodajemy rÃ³wnieÅ¼ zaÅ‚oÅ¼enie, Å¼e zmienne niezaleÅ¼ne \\(\\boldsymbol{x}=(\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_d)\\) charakteryzujÄ… siÄ™ wielowymiarowym rozkÅ‚adem normalnym \\[\\begin{equation} f(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})&#39;\\boldsymbol{\\Sigma}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right], \\tag{8.6} \\end{equation}\\] gdzie \\(\\boldsymbol{\\mu}\\) jest wektorem Å›rednich \\(\\boldsymbol{x}\\), a \\(\\boldsymbol{\\Sigma}\\) jest macierzÄ… kowariancji \\(\\boldsymbol{x}\\). Uwaga. Liniowa kombinacja zmiennych losowych o normalnym rozkÅ‚adzie Å‚Ä…cznym ma rÃ³wnieÅ¼ rozkÅ‚ad Å‚Ä…czny normalny. W szczegÃ³lnoÅ›ci, jeÅ›li \\(A\\) jest macierzÄ… wymiaru \\(d\\times k\\) i \\(\\boldsymbol{y} = A&#39;\\boldsymbol{x}\\), to \\(f(\\boldsymbol{y})\\sim N(A&#39;\\boldsymbol{\\mu}, A&#39;\\boldsymbol{\\Sigma}A)\\). Odpowiednia forma macierzy przeksztaÅ‚cenia \\(A_w\\), sprawia, Å¼e zmienne po transformacji charakteryzujÄ… siÄ™ rozkÅ‚adem normalnym Å‚Ä…cznym o wariancji okreÅ›lonej przez \\(I\\). JeÅ›li \\(\\boldsymbol{\\Phi}\\) jest macierzÄ…, ktÃ³rej kolumny sÄ… ortonormalnymi wektorami wÅ‚asnymi macierzy \\(\\boldsymbol{\\Sigma}\\), a \\(\\boldsymbol{\\Lambda}\\) macierzÄ… diagonalnÄ… wartoÅ›ci wÅ‚asnych, to transformacja \\(A_w=\\boldsymbol{\\Phi}\\boldsymbol{\\Lambda}^{-1}\\) przeksztaÅ‚ca \\(\\boldsymbol{x}\\) w \\(\\boldsymbol{y}\\sim N(A_w&#39;\\boldsymbol{\\mu}, I)\\). Rysunek 8.4: Transformacje rozkÅ‚adu normalnego Å‚Ä…cznego. Å¹rÃ³dÅ‚o: Duda, Hart, and Stork (2001) Definicja 8.1 Niech \\(g_i(\\boldsymbol{x}),\\ i=1,\\ldots,k\\) bÄ™dÄ… pewnymi funkcjami dyskryminacyjnymi. WÃ³wczas obiekt \\(\\boldsymbol{x}\\) klasyfikujemy do grupy \\(c_i\\) jeÅ›li speÅ‚niony jest warunek \\[\\begin{equation} g_i(\\boldsymbol{x})&gt;g_j(\\boldsymbol{x}), \\quad j\\neq i. \\end{equation}\\] W podejÅ›ciu polegajÄ…cym na minimalizacji prawdopodobieÅ„stwa bÅ‚Ä™dnej klasyfikacji, przyjmuje siÄ™ najczÄ™Å›ciej, Å¼e \\[\\begin{equation} g_i(\\boldsymbol{x})=\\P(c_i|\\boldsymbol{x}), \\end{equation}\\] czyli jako prawdopodobieÅ„stwo a posteriori. Wszystkie trzy poniÅ¼sze postaci funkcji dyskryminacyjnych sÄ… dopuszczalne i rÃ³wnowaÅ¼ne ze wzglÄ™du na rezultat grupowania \\[\\begin{align} g_i(\\boldsymbol{x})=&amp;\\P(c_i|\\boldsymbol{x})=\\frac{\\P(\\boldsymbol{x}|c_i)\\P(c_i)}{\\sum_{i=1}^k\\P(\\boldsymbol{x}|c_i)\\P(c_i)},\\\\ g_i(\\boldsymbol{x})=&amp;\\P(\\boldsymbol{x}|c_i)\\P(c_i),\\\\ g_i(\\boldsymbol{x})=&amp;\\log\\P(\\boldsymbol{x}|c_i)+\\log\\P(c_i) \\tag{8.7} \\end{align}\\] W przypadku gdy \\(\\boldsymbol{x}|c_i\\sim N(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)\\), to na podstawie (8.6) \\(g_i\\) danej jako (8.7) przyjmuje postaÄ‡ \\[\\begin{equation} g_i(\\boldsymbol{x})=-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_i)&#39;\\boldsymbol{\\Sigma}_i^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_i)-\\frac{d}{2}\\log(2\\pi)-\\frac{1}{2}\\log|\\boldsymbol{\\Sigma}_i|+\\log\\P(c_i). \\end{equation}\\] W kolejnych podrozdziaÅ‚ach przeanalizujemy trzy moÅ¼liwe formy macierzy kowariancji. 8.2.1 Przypadek gdy \\(\\boldsymbol{\\Sigma}_i=\\sigma^2I\\) To najprostszy przypadek, zakÅ‚adajÄ…cy niezaleÅ¼noÅ›Ä‡ zmiennych wchodzÄ…cych w skÅ‚ad \\(\\boldsymbol x\\), ktÃ³rych wariancje sÄ… staÅ‚e \\(\\sigma^2\\). WÃ³wczas \\(g_i\\) przyjmuje postaÄ‡ \\[\\begin{equation} g_i(\\boldsymbol x)=-\\frac{||\\boldsymbol x-\\boldsymbol \\mu_i||^2}{2\\sigma^2}+\\log\\P(c_i), \\tag{8.8} \\end{equation}\\] gdzie \\(||\\cdot ||\\) jest normÄ… euklidesowÄ…. RozpisujÄ…c licznik rÃ³wnania (8.8) mamy \\[\\begin{equation} ||\\boldsymbol x-\\boldsymbol \\mu_i||^2=(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;(\\boldsymbol x-\\boldsymbol \\mu_i). \\end{equation}\\] Zatem \\[\\begin{equation} g_i(\\boldsymbol x)=-\\frac{1}{2\\sigma^2}[\\boldsymbol x&#39;\\boldsymbol x-2\\boldsymbol \\mu_i&#39;\\boldsymbol x+\\boldsymbol \\mu_i&#39;\\boldsymbol \\mu_i]+\\log\\P(c_i). \\end{equation}\\] A poniewaÅ¼ \\(\\boldsymbol x&#39;\\boldsymbol x\\) nie zaleÅ¼y do \\(i\\), to funkcjÄ™ dyskryminacyjnÄ… moÅ¼emy zapisaÄ‡ jako \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0}, \\end{equation}\\] gdzie \\(\\boldsymbol w_i=\\frac{1}{\\sigma^2}\\boldsymbol \\mu_i\\), a \\(w_{i0}=\\frac{-1}{2\\sigma^2}\\boldsymbol \\mu_i&#39;\\boldsymbol \\mu_i+\\log\\P(c_i).\\) Na podstawie funkcji dyskryminacyjnych wyznaczamy hiperpÅ‚aszczyzny decyzyjne jako zbiory punktÃ³w dla ktÃ³rych \\(g_i(\\boldsymbol x)=g_j(\\boldsymbol x)\\), gdzie \\(g_i, g_j\\) sÄ… najwiÄ™ksze. MoÅ¼emy to zapisaÄ‡ w nastÄ™pujÄ…cy sposÃ³b \\[\\begin{equation} \\boldsymbol w&#39;(\\boldsymbol x-\\boldsymbol x_0)=0, \\tag{8.9} \\end{equation}\\] gdzie \\[\\begin{equation} \\boldsymbol w = \\boldsymbol \\mu_i-\\boldsymbol \\mu_j, \\end{equation}\\] oraz \\[\\begin{equation} \\boldsymbol x_0 = \\frac12(\\boldsymbol \\mu_i+\\boldsymbol\\mu_j)-\\frac{\\sigma^2}{||\\boldsymbol \\mu_i-\\boldsymbol \\mu_j||^2}\\log\\frac{\\P(c_i)}{\\P(c_j)}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j). \\end{equation}\\] RÃ³wnanie (8.9) okreÅ›la hiperpÅ‚aszczyznÄ™ przechodzÄ…cÄ… przez \\(\\boldsymbol x_0\\) i prostopadÅ‚Ä… do \\(\\boldsymbol w\\). Rysunek 8.5: Dyskrymiancja hiperpÅ‚aszczyznami w sygucaji dwÃ³ch klas. Wykres po lewej, to ujÄ™cie jednowymiarowe, wykresy po Å›rodu - ujÄ™cie 2-wymiarowe i wykresy po prawej, to ujÄ™cie 3-wymiarowe. Å¹rÃ³dÅ‚o: Duda, Hart, and Stork (2001) 8.2.2 Przypadek gdy \\(\\boldsymbol \\Sigma_i=\\boldsymbol \\Sigma\\) Przypadek ten opisuje sytuacjÄ™, gdy rozkÅ‚ady \\(\\boldsymbol x\\) sÄ… identyczne we wszystkich grupach ale zmienne w ich skÅ‚ad wchodzÄ…ce nie sÄ… niezaleÅ¼ne. W tym przypadku funkcje dyskryminacyjne przyjmujÄ… postaÄ‡ \\[\\begin{equation} g_i(\\boldsymbol x)=\\frac12(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;\\boldsymbol\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu_i)+\\log\\P(c_i). \\tag{8.10} \\end{equation}\\] JeÅ›li \\(\\P(c_i)\\) sÄ… identyczne dla wszystkich klas, to moÅ¼na je pominÄ…Ä‡ we wzorze (8.10). Metryka euklidesowa ze wzoru (8.8) zostaÅ‚a zastÄ…piona przez odlegÅ‚oÅ›Ä‡ Mahalonobisâ€™a. Podobnie ja w przypadku gdy \\(\\boldsymbol \\Sigma_i=\\sigma^2I\\), tak i teraz moÅ¼na uproÅ›ciÄ‡ (8.10) przez rozpisanie formy kwadratowej, aby otrzymaÄ‡, Å¼e \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0}, \\end{equation}\\] gdzie \\(\\boldsymbol w_i=\\boldsymbol\\Sigma^{-1}\\boldsymbol \\mu_i\\), a \\(w_{i0}=-\\frac{1}{2}\\boldsymbol \\mu_i&#39;\\boldsymbol\\Sigma^{-1}\\boldsymbol \\mu_i+\\log\\P(c_i).\\) PoniewaÅ¼ funkcje dyskryminacyjne sÄ… liniowe, to hiperpÅ‚aszczyzny sÄ… ograniczeniami obszarÃ³w obserwacji kaÅ¼dej z klas \\[\\begin{equation} \\boldsymbol w&#39;(\\boldsymbol x-\\boldsymbol x_0)=0, \\tag{8.11} \\end{equation}\\] gdzie \\[\\begin{equation} \\boldsymbol w = \\boldsymbol\\Sigma^{-1} (\\boldsymbol \\mu_i-\\boldsymbol \\mu_j), \\end{equation}\\] oraz \\[\\begin{equation} \\boldsymbol x_0 = \\frac12(\\boldsymbol \\mu_i+\\boldsymbol\\mu_j)-\\frac{\\log[ \\P(c_i)/\\P(c_j)]}{(\\boldsymbol x-\\boldsymbol \\mu_i)&#39;\\boldsymbol\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu_i)}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j). \\end{equation}\\] Tym razem \\(\\boldsymbol{w}=\\Sigma^{-1}(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j)\\) nie jest wektorem w kierunku \\(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j\\), wiÄ™c hiperpÅ‚aszczyzna rozdzielajÄ…ca obiekty rÃ³Å¼nych klas nie jest prostopadÅ‚a do wektora \\(\\boldsymbol \\mu_i-\\boldsymbol \\mu_j\\) ale przecina go w poÅ‚owie (w punkcie \\(\\boldsymbol x_0\\)). Rysunek 8.6: HiperpÅ‚aszczyzna rozdzielajÄ…ca obszary innych klas moÅ¼e byÄ‡ przesuniÄ™ta w kierunku bardziej prawdopodobnej klasy, jeÅ›li prawdopodobieÅ„stwa a priori sÄ… rÃ³Å¼ne. Å¹rÃ³dÅ‚o: Duda, Hart, and Stork (2001) 8.2.3 Przypadek gdy \\(\\boldsymbol \\Sigma_i\\) jest dowolnej postaci Jest to najbardziej ogÃ³lny przypadek, kiedy nie nakÅ‚ada siÄ™ Å¼adnych ograniczeÅ„ na macierze kowariancji grupowych. PostaÄ‡ funkcji dyskryminacyjnych jest nastÄ™pujÄ…ca \\[\\begin{equation} g_i(\\boldsymbol x)=\\boldsymbol x&#39;\\boldsymbol W_i\\boldsymbol x+\\boldsymbol w_i&#39;\\boldsymbol x+w_{i0} \\tag{8.12} \\end{equation}\\] gdzie \\[\\begin{align} \\boldsymbol W_i = &amp;-\\frac12 \\boldsymbol\\Sigma_i^{-1},\\\\ \\boldsymbol w_i=&amp; \\boldsymbol\\Sigma_i^{-1}\\boldsymbol\\mu_i,\\\\ w_{i0} = &amp;-\\frac12\\boldsymbol\\mu_i&#39;\\boldsymbol\\Sigma_i^{-1}\\boldsymbol\\mu_i-\\frac12\\log|\\boldsymbol\\Sigma_i|+\\log\\P(c_i). \\end{align}\\] Ograniczenia w ten sposÃ³b budowane sÄ… hiperpowierzchniami (nie koniecznie hiperpÅ‚aszczyznami). W literaturze ta metoda znana jest pod nazwÄ… kwadratowej analizy dyskryminacyjnej (ang. Quadratic Discriminant Analysis). Rysunek 8.7: PrzykÅ‚ad zastosowania kwadratowej analizy dyskryminacyjnej. Pokazane sÄ… dopuszczalne postaci zbiorÃ³w ograniczajÄ…cych. Å¹rÃ³dÅ‚o: Duda, Hart, and Stork (2001) PrzykÅ‚ad 8.2 Przeprowadzimy klasyfikacjÄ™ na podstawie zbioru Smarket pakietu ILSR. Dane zawierajÄ… kursy indeksu gieÅ‚dowego S&amp;P500 w latach 2001-2005. Na podstawie wartoÅ›ci waloru z poprzednich 2 dni bÄ™dziemy chcieli przewidzieÄ‡ czy ruch w kolejnym okresie czasu bÄ™dzie w gÃ³rÄ™ czy w dÃ³Å‚. library(ISLR) head(Smarket) ## Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction ## 1 2001 0.381 -0.192 -2.624 -1.055 5.010 1.1913 0.959 Up ## 2 2001 0.959 0.381 -0.192 -2.624 -1.055 1.2965 1.032 Up ## 3 2001 1.032 0.959 0.381 -0.192 -2.624 1.4112 -0.623 Down ## 4 2001 -0.623 1.032 0.959 0.381 -0.192 1.2760 0.614 Up ## 5 2001 0.614 -0.623 1.032 0.959 0.381 1.2057 0.213 Up ## 6 2001 0.213 0.614 -0.623 1.032 0.959 1.3491 1.392 Up set.seed(44) dt.ucz &lt;- Smarket %&gt;% mutate_if(is.numeric, scale) %&gt;% sample_frac(size = 2/3) dt.test &lt;- Smarket[-as.numeric(rownames(dt.ucz)),] mod.qda &lt;- qda(Direction~Lag1+Lag2, data = dt.ucz) mod.qda ## Call: ## qda(Direction ~ Lag1 + Lag2, data = dt.ucz) ## ## Prior probabilities of groups: ## Down Up ## 0.4909964 0.5090036 ## ## Group means: ## Lag1 Lag2 ## Down 0.0328367 0.06722714 ## Up -0.0671615 -0.08814914 PoniewaÅ¼ funkcje dyskryminacyjne mogÄ… byÄ‡ nieliniowe, to podsumowanie modelu nie zawiera wspÃ³Å‚czynnikÃ³w funkcji. Podsumowanie zawiera tylko prawdopodobieÅ„stwa a priori i Å›rednie poszczegÃ³lnych zmiennych niezaleÅ¼nych w klasach. pred.qda &lt;- predict(mod.qda, dt.test) tab &lt;- table(pred = pred.qda$class, dt.test$Direction) tab ## ## pred Down Up ## Down 42 34 ## Up 139 202 sum(diag(prop.table(tab))) ## [1] 0.5851319 library(klaR) partimat(Direction ~ Lag1+Lag2, data = dt.ucz, method = &quot;qda&quot;, col.correct=&#39;blue&#39;, col.wrong=&#39;red&#39;) Rysunek 8.8: Wykres klasyfikacji na podstawie QDA. Obserwacje zaznczone kolorem niebieskim sÄ… prawidÅ‚owo zaklasyfikowane, a czerwonym Åºle 8.3 Analiza dyskryminacyjna metodÄ… czÄ™Å›ciowych najmniejszych kwadratÃ³w Analiza dyskryminacyjna metodÄ… czÄ™Å›ciowych najmniejszych kwadratÃ³w (ang. Partial Least Squares Discriminant Analysis) jest wykorzystywana szczegÃ³lnie w sytuacjach gdy zestaw predyktorÃ³w zwiera zmienne silnie ze sobÄ… skorelowane. Jak wiadomo z wczeÅ›niejszych rozwaÅ¼aÅ„, metody dyskryminacji obserwacji sÄ… maÅ‚o odporne na nadmiarowoÅ›Ä‡ zmiennych niezaleÅ¼nych. StÄ…d powstaÅ‚ pomysÅ‚ zastosowania poÅ‚Ä…czenia LDA z PLS (Partial Least Squares), ktÃ³rej celem jest redukcja wymiaru przestrzeni jednoczeÅ›nie maksymalizujÄ…c korelacjÄ™ zmiennych niezaleÅ¼nych ze zmiennÄ… wynikowÄ…. Parametrem, ktÃ³ry jest kontrolowany podczas budowy modelu jest liczba ukrytych zmiennych. Metoda PLSDA ma kilka implementacji w R, ale najbardziej znana jest funkcja plsda z pakietu caret (Jed Wing et al. 2018). PrzykÅ‚ad 8.3 KontynujÄ…c poprzedni przykÅ‚ad przeprowadzimy klasyfikacje ruchu waloru korzystajÄ…c z metody PLSDA. W przeciwieÅ„stwie do poprzednich funkcji plsda potrzebuje przekazania zbioru predyktorÃ³w i wektora zmiennej wynikowej oddzielnie, a nie za pomocÄ… formuÅ‚y. Doboru liczby zmiennych latentnych dokonamy arbitralnie. library(caret) mod.plsda &lt;- plsda(dt.ucz[,-c(1,7:9)], as.factor(dt.ucz$Direction), ncomp = 2) mod.plsda$loadings ## ## Loadings: ## Comp 1 Comp 2 ## Lag1 -0.548 0.375 ## Lag2 -0.805 -0.204 ## Lag3 -0.520 ## Lag4 -0.143 0.652 ## Lag5 0.186 0.351 ## ## Comp 1 Comp 2 ## SS loadings 1.004 1.001 ## Proportion Var 0.201 0.200 ## Cumulative Var 0.201 0.401 Dwie ukryte zmienne uÅ¼yte do redukcji wymiaru przestrzeni wyjaÅ›niajÄ… okoÅ‚o 40% zmiennoÅ›ci pierwotnych zmiennych. Åadunki (Loadings) pokazujÄ… kontrybucje poszczegÃ³lnych zmiennych w tworzenie siÄ™ zmiennych ukrytych. pred.plsda &lt;- predict(mod.plsda, dt.test[,-c(1,7:9)]) tab &lt;- table(pred.plsda, dt.test$Direction) tab ## ## pred.plsda Down Up ## Down 87 97 ## Up 94 139 sum(diag(prop.table(tab))) ## [1] 0.5419664 PoniewaÅ¼ korelacje pomiÄ™dzy predyktorami w naszym przypadku nie byÅ‚y duÅ¼e, to zastosowanie PLSDA nie poprawiÅ‚o klasyfikacji w stosunku do metody QDA. cor(dt.ucz[,2:6]) ## Lag1 Lag2 Lag3 Lag4 Lag5 ## Lag1 1.000000000 -0.003318026 -0.004329303 0.02574559 0.01831679 ## Lag2 -0.003318026 1.000000000 -0.057166931 0.01209305 0.02127975 ## Lag3 -0.004329303 -0.057166931 1.000000000 0.01574896 -0.07541592 ## Lag4 0.025745592 0.012093049 0.015748958 1.00000000 -0.04607207 ## Lag5 0.018316786 0.021279754 -0.075415921 -0.04607207 1.00000000 8.4 Regularyzowana analiza dyskryminacyjna Regularyzowana analiza dyskryminacyjna (ang. Regularized Discriminant Analysis) powstaÅ‚a jako technika rÃ³wnowaÅ¼Ä…ca zalety i wady LDA i QDA. Ze wzglÄ™du na zdolnoÅ›ci generalizacyjne model LDA jest lepszy od QDA (mniejsza wariancja modelu), ale jednoczeÅ›nie QDA ma bardziej elastycznÄ… postaÄ‡ hiperpowierzchni brzegowych rozdzielajÄ…cych obiekty rÃ³Å¼nych klas. Dlatego Friedman (1989) wprowadziÅ‚ technikÄ™ bÄ™dÄ…cÄ… kompromisem pomiÄ™dzy LDA i QDA poprzez odpowiednie okreÅ›lenie macierzy kowariancji \\[\\begin{equation} \\tilde{\\boldsymbol \\Sigma}_i(\\lambda) = \\lambda\\boldsymbol\\Sigma_i + (1-\\lambda)\\boldsymbol\\Sigma, \\end{equation}\\] gdzie \\(\\boldsymbol \\Sigma_i\\) jest macierzÄ… kowariancji dla \\(i\\)-tej klasy, a \\(\\boldsymbol \\Sigma\\) jest uÅ›rednionÄ… macierzÄ… kowariancji wszystkich klas. Zatem odpowiedni dobÃ³r parametru \\(\\lambda\\) decyduje czy poszukujemy modelu prostszego (\\(\\lambda = 0\\) odpowiada LDA), czy bardziej elastycznego (\\(\\lambda=1\\) oznacza QDA). Dodatkowo metoda RDA pozwala na elastyczny wybÃ³r pomiÄ™dzy postaciami macierzy kowariancji wspÃ³lnej dla wszystkich klas \\(\\boldsymbol\\Sigma\\). MoÅ¼e ona byÄ‡ macierzÄ… jednostkowÄ…, jak w przypadku 8.2.1, co oznacza niezaleÅ¼noÅ›Ä‡ predyktorÃ³w modelu, moÅ¼e teÅ¼ byÄ‡ jak w przypadku 8.2.2, gdzie dopuszcza siÄ™ korelacje miÄ™dzy predyktorami. Dokonuje siÄ™ tego przez odpowiedni dobÃ³r parametru \\(\\gamma\\) \\[\\begin{equation} \\boldsymbol \\Sigma(\\gamma) = \\gamma\\boldsymbol \\Sigma+(1-\\gamma)\\sigma^2I. \\end{equation}\\] PrzykÅ‚ad 8.4 Funkcja rda pakietu klaR jest implementacjÄ… powyÅ¼szej metody. IlustrajÄ… jej dziaÅ‚ania bÄ™dzie klasyfikacja stanÃ³w z poprzedniego przykÅ‚adu. library(klaR) mod.rda &lt;- rda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, dt.ucz) mod.rda ## Call: ## rda(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = dt.ucz) ## ## Regularization parameters: ## gamma lambda ## 0.713621809 0.004013224 ## ## Prior probabilities of groups: ## Down Up ## 0.4909964 0.5090036 ## ## Misclassification rate: ## apparent: 44.058 % ## cross-validated: 46.482 % Model zostaÅ‚ oszacowany z parametrami wyznaczonymi na podstawie sprawdzianu krzyÅ¼owego zastosowanego w funkcji rda. pred.rda &lt;- predict(mod.rda, dt.test) (tab &lt;- table(pred = pred.rda$class, dt.test$Direction)) ## ## pred Down Up ## Down 24 31 ## Up 157 205 sum(diag(prop.table(tab))) ## [1] 0.5491607 JakoÅ›Ä‡ klasyfikacji jest na zbliÅ¼onym poziomie jak przy poprzednich metodach. 8.5 Analiza dyskryminacyjna mieszana Liniowa analiza dyskryminacyjna zakÅ‚adaÅ‚a, Å¼e Å›rednie (centroidy) w klasach sÄ… rÃ³Å¼ne ale macierz kowariancji wszystkich klas jest jednakowa. Analiza dyskryminacyjna mieszana (ang. Mixture Discriminant Analysis) prezentuje jeszcze inne podejÅ›cie poniewaÅ¼ zakÅ‚ada, Å¼e kaÅ¼da klasa moÅ¼e byÄ‡ charakteryzowana przez wiele wielowymiarowych rozkÅ‚adÃ³w normalnych, ktÃ³rych centroidy mogÄ… siÄ™ rÃ³Å¼nic, ale macierze kowariancji nie. WÃ³wczas rozkÅ‚ad dla danej klasy jest mieszaninÄ… rozkÅ‚adÃ³w skÅ‚adowych, a funkcja dyskryminacyjna dla \\(i\\)-tej klasy przyjmuje postaÄ‡ \\[\\begin{equation} g_i(\\boldsymbol x)\\propto \\sum_{k=1}^{L_i}\\phi_{ik}g_{ik}(\\boldsymbol x), \\end{equation}\\] gdzie \\(L_i\\) jest liczbÄ… rozkÅ‚adÃ³w skÅ‚adajÄ…cych siÄ™ na \\(i\\)-tÄ… klasÄ™, a \\(\\phi_{ik}\\) jest wspÃ³Å‚czynnikiem proporcji estymowanych w czasie uczenia modelu. PrzykÅ‚ad 8.5 Funkcja mda pakietu mda (Trevor Hastie et al. 2017) jest implementacjÄ… tej techniki w R. Jej zastosowanie pokaÅ¼emy na przykÅ‚adzie danych gieÅ‚dowych z poprzedniego przykÅ‚adu. UÅ¼yjemy domyÅ›lnych ustawieÅ„ funkcji (trzy rozkÅ‚ady dla kaÅ¼dej klasy). library(mda) mod.mda &lt;- mda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, dt.ucz) mod.mda ## Call: ## mda(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = dt.ucz) ## ## Dimension: 5 ## ## Percent Between-Group Variance Explained: ## v1 v2 v3 v4 v5 ## 63.78 95.81 98.97 99.84 100.00 ## ## Degrees of Freedom (per dimension): 6 ## ## Training Misclassification Error: 0.43337 ( N = 833 ) ## ## Deviance: 1134.288 pred.mda &lt;- predict(mod.mda, dt.test) (tab &lt;- table(pred = pred.mda, dt.test$Direction)) ## ## pred Down Up ## Down 38 46 ## Up 143 190 sum(diag(prop.table(tab))) ## [1] 0.5467626 Kolejny raz model dyskryminacyjny charakteryzuje siÄ™ podobnÄ… jakoÅ›ciÄ… klasyfikacji. 8.6 Elastyczna analiza dyskryminacyjna ZupeÅ‚nie inne podejÅ›cie w stosunku do wczeÅ›niejszych rozwiÄ…zaÅ„, prezentuje elastyczna analiza dyskryminacyjna (ang. Flexible Discriminant Analysis) . KodujÄ…c klasy wynikowe jako zmienne dychotomiczne (dla kaÅ¼dej klasy jest odrÄ™bna zmienna wynikowa) dla kaÅ¼dej z nich budowanych jest \\(k\\) modeli regresji. MogÄ… to byÄ‡ modele regresji penalizowanej, jak regresja grzbietowa lub LASSO, modele regresji wielomianowej albo modele regresji sklejanej (MARS), o ktÃ³rych bÄ™dzie mowa w dalszej czÄ™Å›ci tego opracowania. PrzykÅ‚adowo, jeÅ›li modelem bazowym jest MARS, to funkcja dyskryminacyjna \\(i\\)-tej klasy moÅ¼e byÄ‡ postaci \\[\\begin{equation} g_i(\\boldsymbol x)=\\beta_0+\\beta_1h(1-x_1)+\\beta_2h(x_2-1)+\\beta_3h(1-x_3)+\\beta_4h(x_1-1), \\end{equation}\\] gdzie \\(h\\) sÄ… tzw. funkcjami bazowymi postaci \\[\\begin{equation} h(x)= \\begin{cases} x, &amp; x&gt; 0\\\\ 0, &amp; x\\leq 0. \\end{cases} \\end{equation}\\] Klasyfikacji dokonujemy sprawdzajÄ…c znak funkcji dyskryminacyjnej \\(g_i\\), jeÅ›li jest dodatni, to funkcja przypisuje obiekt do klasy \\(i\\)-tej. W przeciwnym przypadku nie naleÅ¼y do tej klasy. Rysunek 8.9: PrzykÅ‚ad klasyfikacji dwustanowej za pomocÄ… metody FDA PrzykÅ‚ad 8.6 Funkcja fda pakietu mda jest implementacjÄ… techniki FDA w R. Na postawie danych z poprzedniego przykÅ‚adu zostanie przedstawiona zasada dzieÅ‚ania. Przyjmiemy domyÅ›lne ustawienia funkcji, z wyjÄ…tkiem metody estymacji modelu, jako ktÃ³rÄ… przyjmiemy MARS. mod.fda &lt;- fda(Direction ~ Lag1+Lag2, dt.ucz, method = mars) mod.fda ## Call: ## fda(formula = Direction ~ Lag1 + Lag2, data = dt.ucz, method = mars) ## ## Dimension: 1 ## ## Percent Between-Group Variance Explained: ## v1 ## 100 ## ## Training Misclassification Error: 0.44418 ( N = 833 ) PoniewaÅ¼, zmienna wynikowa jest dwustanowa, to powstaÅ‚a tylko jedna funkcja dyskryminacyjna. Parametry modelu sÄ… nastÄ™pujÄ…ce mod.fda$fit$coefficients ## [,1] ## [1,] 0.05846465 ## [2,] -0.17936208 pred.fda &lt;- predict(mod.fda, dt.test) (tab &lt;- table(pred = pred.fda, dt.test$Direction)) ## ## pred Down Up ## Down 40 50 ## Up 141 186 sum(diag(prop.table(tab))) ## [1] 0.5419664 JakoÅ›Ä‡ klasyfikacji jest tylko nieco lepsza niÅ¼ w przypadku poprzednich metod. Bibliografia "],
["bayes.html", "9 Klasyfikatory bayesowskie 9.1 Klasyfikator maximum a posteriori (MAP) 9.2 Klasyfikator najwiÄ™kszej wiarogodnoÅ›ci (ML) 9.3 Naiwny klasyfikator Bayesa (NB)18 9.4 Zalety i wady", " 9 Klasyfikatory bayesowskie CaÅ‚Ä… gamÄ™ klasyfikatorÃ³w opartych na twierdzeniu Bayesa nazywaÄ‡ bÄ™dziemy bayesowskimi. \\[\\begin{equation}\\label{bayes} P(A|B)=\\frac{P(A)P(B|A)}{P(B)}, \\end{equation}\\] gdzie \\(P(B)&gt;0\\). Bayesowskie reguÅ‚y podejmowania decyzji daÅ‚y podstawy takich metod jak: liniowa analiza dyskryminacyjna; kwadratowa analiza dyskryminacyjna; W ustaleniu klasyfikatora bayesowskiego bÄ™dzie nam przyÅ›wiecaÅ‚a caÅ‚y czas ta sama reguÅ‚a: jeÅ›li znam wartoÅ›ci cech charakteryzujÄ…cych badane obiekty oraz klasy do ktÃ³rych naleÅ¼Ä… (w prÃ³bie uczÄ…cej), to na ich podstawie mogÄ™ wyznaczyÄ‡ miary prawdopodobieÅ„stw a posteriori, ktÃ³re pomogÄ… mi w ustaleniu klasy do ktÃ³rej naleÅ¼y nowy testowy element. W dalszej czÄ™Å›ci bÄ™dziemy przyjmowali nastÄ™pujÄ…ce oznaczenia: \\(T\\) - zbiÃ³r danych uczÄ…cych (treningowych), \\(T^j\\) - zbiÃ³r danych uczÄ…cych dla ktÃ³rych przyjÄ™liÅ›my decyzjÄ™ o przynaleÅ¼noÅ›ci do \\(j\\)-tej klasy, \\(T^j_{a_i=v}\\) - zbiÃ³r danych uczÄ…cych o wartoÅ›ci atrybutu \\(a_i\\) rÃ³wnej \\(v\\) i klasy \\(j\\)-tej, \\(\\mathbb{H}\\) - przestrzeÅ„ hipotez, \\(P(h|a_1=v_1, a_2=v_2,\\ldots,a_p=v_p)\\) - prawdopodobieÅ„stwo a posteriori, Å¼e prawdziwa jest hipoteza \\(h\\in \\mathbb{H}\\), jeÅ›li znamy atrybuty obiektu, \\(P(h)\\) - prawdopodobieÅ„stwo a priori zajÅ›cia hipotezy \\(h\\in \\mathbb{H}\\), \\(c\\) - prawdziwy stan obiektu. 9.1 Klasyfikator maximum a posteriori (MAP) Na podstawie wiedzy o atrybutach obiektu \\(x\\) podejmujemy decyzjÄ™ o klasyfikacji tego obiektu zgodnie z hipotezÄ… \\(h_{MAP}\\in \\mathbb{H}\\), ktÃ³ra przyjmuje postaÄ‡ \\[\\begin{align}\\label{MAP} h_{MAP}=&amp;\\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(h|a_1=v_1, a_2=v_2,\\ldots,a_p=v_p)\\\\ =&amp; \\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h)\\cdot P(h), \\end{align}\\] gdzie ostatnia rÃ³wnoÅ›Ä‡ wynika z twierdzenia Bayesa oraz faktu, Å¼e dla konkretnego obiektu \\(x\\) wielkoÅ›ci atrybutÃ³w nie zaleÅ¼Ä… od postawionej hipotezy. 9.2 Klasyfikator najwiÄ™kszej wiarogodnoÅ›ci (ML) Na podstawie wiedzy o atrybutach obiektu \\(x\\) podejmujemy decyzjÄ™ o klasyfikacji tego obiektu zgodnie z hipotezÄ… \\(h_{ML}\\in \\mathbb{H}\\), ktÃ³ra przyjmuje postaÄ‡ \\[\\begin{equation}\\label{ML} h_{ML}=\\operatorname{arg}\\max_{h\\in \\mathbb{H}}P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h). \\end{equation}\\] Uwaga. Obie wspomniane metody wymagajÄ… znajomoÅ›ci prawdopodobieÅ„stwa \\(P(a_1=v_1,a_2=v_2,\\ldots,a_p=v_p|h)\\), ale rÃ³Å¼niÄ… siÄ™ podejÅ›ciem do wiedzy o prawdopodobieÅ„stwach a priori. W metodzie MAP brana pod uwagÄ™ jest wiedza o prawdopodobieÅ„stwie przynaleÅ¼noÅ›ci do poszczegÃ³lnych klas, a w ML nie. Dla klasyfikacji, w ktÃ³rych prawdopodobieÅ„stwa przynaleÅ¼noÅ›ci do klas sÄ… takie same, klasyfikatory MAP i ML sÄ… rÃ³wnowaÅ¼ne. 9.3 Naiwny klasyfikator Bayesa (NB)18 NajwiÄ™kszy problem w wyznaczeniu klasyfikatorÃ³w MAP i ML stanowi wyznaczenie rozkÅ‚adu Å‚Ä…cznego \\(P(a_1=v_1, a_2=v_2,\\ldots,a_p=v_p|h)\\). W naiwnym klasyfikatorze Bayesa zakÅ‚ada siÄ™ niezaleÅ¼noÅ›Ä‡ warunkowÄ… poszczegÃ³lnych atrybutÃ³w wzglÄ™dem klasy do ktÃ³rej ma naleÅ¼eÅ„ wg hipotezy obiekt. ZaÅ‚oÅ¼enie to czÄ™sto nie jest speÅ‚nione i stÄ…d nazwa przymiotnik â€œnaiwnyâ€. Definicja naiwnego klasyfikatora bayesowskiego rÃ³Å¼ni siÄ™ od klasyfikatora MAP tylko podejÅ›ciem do prawdopodobieÅ„stwa a posteriori. \\[\\begin{equation}\\label{naiwny_bayes} h_{NB}=\\operatorname{arg}\\max_{h_j\\in \\mathbb{H}}P(h_j)\\prod_{i=1}^{p}P(a_i=v_i|h_j), \\end{equation}\\] gdzie \\(h_j\\) oznacza hipotezÄ™ (decyzjÄ™), Å¼e badany obiekt naleÅ¼y do \\(j\\)-tej klasy. OczywiÅ›cie zarÃ³wno prawdopodobieÅ„stwo a priori jak i a posteriori sÄ… wyznaczane na podstawie prÃ³by, i tak prawdopodobieÅ„stwo a priori wynosi \\[\\begin{equation}\\label{apriori} P(h_j)=P_T(h_j)=\\frac{|T^j|}{|T|}, \\end{equation}\\] gdzie \\(|A|\\) oznacza moc zbioru \\(A\\). Natomiast prawdopodobieÅ„stwo a posteriori dla \\(i\\)-tego atrybutu wynosi \\[\\begin{equation}\\label{aposteriori} P(a_i=v_i|h_j)=P_{T^j}(a_i=v_i)=\\frac{|T^j_{a_i=v_i}|}{|T^j|}. \\end{equation}\\] Na mocy powyÅ¼szego moÅ¼emy zauwaÅ¼yÄ‡, Å¼e jeÅ¼eli zaÅ‚oÅ¼enie o warunkowej niezaleÅ¼noÅ›ci jest speÅ‚nione, to klasyfikatory NB i MAP sÄ… rÃ³wnowaÅ¼ne. ChcÄ…c przypisaÄ‡ klasÄ™ nowemu obiektowi powstaje problem praktyczny, polegajÄ…cy na tym, Å¼e dla pewnych konfiguracji atrybutÃ³w nie ma odpowiednikÃ³w w nauczonym modelu. Powodem takiego stanu rzeczy jest fakt, Å¼e takie kombinacje nie wystÄ…piÅ‚y w prÃ³bie uczÄ…cej. IstniejÄ… dwa sposoby predykcji w takiej sytuacji: \\[\\begin{equation}\\label{pred1} P(a_i=v_i|h_j)= \\begin{cases} \\frac{|T^j_{a_i=v_i}|}{|T^j|}, &amp; T^j_{a_i=v_i}\\neq \\emptyset\\\\ \\epsilon, &amp; \\text{w przeciwnym przypadku.} \\end{cases} \\end{equation}\\] W tym przypadku przyjmuje siÄ™, Å¼e \\(\\epsilon \\ll 1/|T_j|\\). Drugi sposÃ³b wykorzystuje estymacjÄ™ z poprawkÄ… \\[\\begin{equation}\\label{pred2} P(a_i=v_i|h_j)=\\frac{|T^j_{a_i=v_i}|+mp}{|T^j|+mp}, \\end{equation}\\] gdzie \\(p\\) oznacza prawdopodobieÅ„stwo a priori przyjÄ™cia przez atrybut \\(a\\) wartoÅ›ci \\(v\\) (najczÄ™Å›ciej \\(p=1/|A|\\), \\(A\\) - zbiÃ³r wszystkich moÅ¼liwych wartoÅ›ci atrybutu \\(a\\)), \\(m\\) - waga (najczÄ™Å›ciej \\(m=|A|\\)). W przypadku gdy atrybuty sÄ… mierzone na skali ciÄ…gÅ‚ej najczÄ™Å›ciej stosuje siÄ™ dyskretyzacjÄ™ ich do zmiennych ze skali przedziaÅ‚owej. Inna metoda stosowana w przypadku ciÄ…gÅ‚ych atrybutÃ³w, to uÅ¼ycie gÄ™stoÅ›ci \\(g_i^j\\) o rozkÅ‚adzie normalnym w miejsce \\(P(a_i=v_i|h_j)\\). Przy czym do obliczenia parametrÃ³w rozkÅ‚adu stosujemy wzory \\[\\begin{equation}\\label{sred} m_i^j=\\frac{1}{|T^j|}\\sum_{x\\in T^j}a_i(x), \\end{equation}\\] oraz \\[\\begin{equation}\\label{odch} (s_i^j)^2=\\frac{1}{|T^j|-1}\\sum_{x\\in T^j}(a_i(x)-m_i^j)^2. \\end{equation}\\] ObsÅ‚uga brakÃ³w danych przez naiwny klasyfikator Bayesa jest doÅ›Ä‡ prosta i opiera siÄ™ na liczeniu prawdopodobieÅ„stw a posteriori wyÅ‚Ä…cznie dla obiektÃ³w, ktÃ³rych wartoÅ›ci atrybutÃ³w sÄ… znane. Dlatego prawdopodobieÅ„stwa warunkowe liczy siÄ™ wg wzoru \\[\\begin{equation}\\label{pr_war} P(a_i=v_i|h_j)=\\frac{|T^j_{a_i=v_i}|}{|T^j|-|T^j_{a_i=NA}|}. \\end{equation}\\] JeÅ›li brakujÄ…ce dane nie niosÄ… w sobie istotnych informacji dotyczÄ…cych klasyfikacji obiektÃ³w, to naiwny klasyfikator Bayesa bÄ™dzie dziaÅ‚aÅ‚ poprawnie. Naiwny klasyfikator Bayesa jest implementowany w pakietach e1071 (Meyer et al. 2019) i klaR (Weihs et al. 2005). PrzykÅ‚ad 9.1 Przeprowadzimy klasyfikacjÄ™ dla zbioru Titanic. W przypadku funkcji z pakietu e1071 nie potrzeba zamieniaÄ‡ tabeli na przypadki. W pakiecie klaR istnieje inna funkcja budujÄ…ca klasyfikator Bayesa NaiveBayes, ale w tym przypadku jeÅ›li zbiÃ³r jest w formie tabeli, to naleÅ¼y go zamieniÄ‡ na ramkÄ™ danych z oddzielnymi przypadkami. library(e1071) Titanic ## , , Age = Child, Survived = No ## ## Sex ## Class Male Female ## 1st 0 0 ## 2nd 0 0 ## 3rd 35 17 ## Crew 0 0 ## ## , , Age = Adult, Survived = No ## ## Sex ## Class Male Female ## 1st 118 4 ## 2nd 154 13 ## 3rd 387 89 ## Crew 670 3 ## ## , , Age = Child, Survived = Yes ## ## Sex ## Class Male Female ## 1st 5 1 ## 2nd 11 13 ## 3rd 13 14 ## Crew 0 0 ## ## , , Age = Adult, Survived = Yes ## ## Sex ## Class Male Female ## 1st 57 140 ## 2nd 14 80 ## 3rd 75 76 ## Crew 192 20 nb &lt;- naiveBayes(Survived ~ ., data = Titanic) nb$apriori ## Survived ## No Yes ## 1490 711 PoniÅ¼sze tabele zawierajÄ… warunkowe prawdopodobieÅ„stwa przynaleÅ¼noÅ›ci do poszczegÃ³lnych klas. nb$tables ## $Class ## Class ## Survived 1st 2nd 3rd Crew ## No 0.08187919 0.11208054 0.35436242 0.45167785 ## Yes 0.28551336 0.16596343 0.25035162 0.29817159 ## ## $Sex ## Sex ## Survived Male Female ## No 0.91543624 0.08456376 ## Yes 0.51617440 0.48382560 ## ## $Age ## Age ## Survived Child Adult ## No 0.03489933 0.96510067 ## Yes 0.08016878 0.91983122 dane &lt;- as.data.frame(Titanic) pred &lt;- predict(nb, dane) pred ## [1] Yes No No No Yes Yes Yes Yes No No No No Yes Yes Yes Yes Yes ## [18] No No No Yes Yes Yes Yes No No No No Yes Yes Yes Yes ## Levels: No Yes tab &lt;- table(pred, dane$Survived) tab ## ## pred No Yes ## No 7 7 ## Yes 9 9 sum(diag(prop.table(tab))) ## [1] 0.5 Naiwny klasyfikator spisaÅ‚ siÄ™ bardzo sÅ‚abo, poniewaÅ¼ klasyfikacja na poziomie 0.5 jest taka jak przy rzucie monetÄ…. PrzykÅ‚ad 9.2 Przeprowadzimy klasyfikacjÄ™ gatunkÃ³w irysÃ³w na podstawie szerokoÅ›ci i dÅ‚ugoÅ›ci kielicha i pÅ‚atka. library(klaR) set.seed(2019) uczaca &lt;- sample(1:nrow(iris), 2*nrow(iris)/3) pr.ucz &lt;- iris[uczaca,] pr.test &lt;- iris[-uczaca,] nb2 &lt;- NaiveBayes(Species~., data = pr.ucz) nb2$apriori ## grouping ## setosa versicolor virginica ## 0.35 0.32 0.33 PrawdopodobieÅ„stwa a priori zostaÅ‚y oszacowane na podstawie prÃ³by uczÄ…cej. PoniÅ¼sze tabele zawierajÄ… Å›rednie i odchylenia standardowe zmiennych w poszczegÃ³lnych klasach. nb2$tables ## $Sepal.Length ## [,1] [,2] ## setosa 4.982857 0.3485143 ## versicolor 6.003125 0.5462390 ## virginica 6.463636 0.5808497 ## ## $Sepal.Width ## [,1] [,2] ## setosa 3.408571 0.3616721 ## versicolor 2.750000 0.3537814 ## virginica 2.960606 0.2838787 ## ## $Petal.Length ## [,1] [,2] ## setosa 1.480000 0.1875539 ## versicolor 4.275000 0.4852668 ## virginica 5.460606 0.5225774 ## ## $Petal.Width ## [,1] [,2] ## setosa 0.2657143 0.1109925 ## versicolor 1.3437500 0.2198790 ## virginica 2.0151515 0.3123712 pred &lt;- predict(nb2, newdata = pr.test) tab &lt;- table(pred$class, pr.test$Species) tab ## ## setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 17 0 ## virginica 0 1 17 sum(diag(prop.table(tab))) ## [1] 0.98 Klasyfikacja na podstawie modelu jest bardzo dobra (98%). 9.4 Zalety i wady Zalety: prostota konstrukcji i prosty algorytm; jeÅ›li jest speÅ‚nione zaÅ‚oÅ¼enie warunkowej niezaleÅ¼noÅ›ci, to ten klasyfikator dziaÅ‚a szybciej i czasem lepiej niÅ¼ inne metody klasyfikacji; nie potrzebuje duÅ¼ych zbiorÃ³w danych do estymacji parametrÃ³w; Wady: czÄ™sto nie speÅ‚nione zaÅ‚oÅ¼enie o warunkowej niezaleÅ¼noÅ›ci powoduje obciÄ…Å¼enie wynikÃ³w; brak moÅ¼liwoÅ›ci wprowadzania interakcji efektÃ³w kilku zmiennych; potrzebuje zaÅ‚oÅ¼enia normalnoÅ›ci warunkowych gÄ™stoÅ›ci w przypadku ciÄ…gÅ‚ych atrybutÃ³w; czÄ™sto istniejÄ… lepsze klasyfikatory. Bibliografia "],
["metoda-k-najblizszych-sasiadow.html", "10 Metoda \\(k\\) najbliÅ¼szych sÄ…siadÃ³w", " 10 Metoda \\(k\\) najbliÅ¼szych sÄ…siadÃ³w Technika \\(k\\) najbliÅ¼szych sÄ…siadÃ³w (ang. \\(k\\)-Nearest Neighbors) przewiduje wartoÅ›Ä‡ zmiennej wynikowej na podstawie \\(k\\) najbliÅ¼szych obserwacji zbioru uczÄ…cego. W przeciwieÅ„stwie do wspomnianych wczeÅ›niej modeli liniowych, nie posiada ona jawnej formy i naleÅ¼y do klasy technik nazywanych czarnymi skrzynkami (ang. black box). MoÅ¼e byÄ‡ wykorzystywana, zarÃ³wno do zadaÅ„ klasyfikacyjnych, jak i regresyjnych. W obu przypadkach predykcja dla nowych wartoÅ›ci predyktorÃ³w przebiega podobnie. Niech \\(\\boldsymbol x_0\\) bÄ™dzie obserwacjÄ…, dla ktÃ³rej poszukujemy wartoÅ›ci zmiennej wynikowej \\(y_0\\). Na podstawie zbioru obserwacji \\(\\boldsymbol x\\in T\\) zbioru uczÄ…cego wyznacza siÄ™ \\(k\\) najbliÅ¼szych sÄ…siadÃ³w19, gdzie \\(k\\) jest z gÃ³ry ustalonÄ… wartoÅ›ciÄ…. NastÄ™pnie, jeÅ›li zadanie ma charakter klasyfikacyjny, to \\(y_0\\) przypisuje siÄ™ modÄ™ zmiennej wynikowej obserwacji bÄ™dÄ…cych \\(k\\) najbliÅ¼szymi sÄ…siadami. W przypadku zadaÅ„ regresyjnych \\(y_0\\) przypisuje siÄ™ Å›redniÄ… lub medianÄ™. Olbrzymie znaczenie dla wynikÃ³w predykcji na podstawie metody kNN ma dobÃ³r metryki. Nie istnieje obiektywna technika wyboru najlepszej metryki, dlatego jej doboru dokonujemy metodÄ… prÃ³b i bÅ‚Ä™dÃ³w. NaleÅ¼y dodatkowo pamiÄ™taÄ‡, Å¼e wielkoÅ›ci mierzone \\(\\boldsymbol x\\) mogÄ… siÄ™ rÃ³Å¼niÄ‡ zakresami zmiennoÅ›ci, a co za tym idzie, mogÄ… znaczÄ…co wpÅ‚ynÄ…Ä‡ na mierzone odlegÅ‚oÅ›ci pomiÄ™dzy punktami. Dlatego zaleca siÄ™ standaryzacjÄ™ zmiennych przed zastosowaniem metody kNN. Kolejnym parametrem, ktÃ³ry ma znaczÄ…cy wpÅ‚yw na predykcjÄ™, jest liczba sÄ…siadÃ³w \\(k\\). WybÃ³r zbyt maÅ‚ej liczby \\(k\\) moÅ¼e doprowadziÄ‡ do przeuczenia modelu jak to jest pokazane na rysunku 10.1 Rysunek 10.1: PrzykÅ‚ad klasyfikacji dla \\(k=1\\) Z kolei zbyt duÅ¼a liczba sÄ…siadÃ³w powoduje obciÄ…Å¼enie wynikÃ³w (patrz rysunek 10.2) Rysunek 10.2: PrzykÅ‚ad zastosowania 100 sÄ…siadÃ³w Dopiero dobÃ³r odpowiedniego \\(k\\) daje model o stosunkowo niskiej wariancji i obciÄ…Å¼eniu. NajczÄ™Å›ciej liczby \\(k\\) poszukujemy za pomocÄ… prÃ³bkowania. Rysunek 10.3: Model z optymalnÄ… liczbÄ… sÄ…siadÃ³w PrzykÅ‚ad 10.1 KlasyfikacjÄ™ z wykorzystaniem metody kNN przeprowadzimy na przykÅ‚adzie danych zbioru spam pakietu ElemStatLearn. Metoda kNN ma wiele implementacji R-owych ale na potrzeby przykÅ‚adu wykorzystamy funkcjÄ™ knn3 pakietu caret. Najpierw dokonamy oszacowania optymalnego \\(k\\) library(ElemStatLearn) library(tidyverse) spam.std &lt;- spam %&gt;% mutate_if(is.numeric, scale) set.seed(123) ind &lt;- sample(nrow(spam), size = nrow(spam)*2/3) dt.ucz &lt;- spam.std[ind,] dt.test &lt;- spam.std[-ind,] acc &lt;- function(pred, obs){ tab &lt;- table(pred,obs) acc &lt;- sum(diag(prop.table(tab))) acc } 1:40 %&gt;% map(~knn3(spam~., data = dt.ucz, k = .x)) %&gt;% map(~predict(.x, newdata = dt.test, type = &quot;class&quot;)) %&gt;% map_dbl(~acc(pred = .x, obs = dt.test$spam)) %&gt;% tibble(k = 1:length(.), acc=.) %&gt;% ggplot(aes(k, acc))+ geom_line() Rysunek 10.4: Ocena jakoÅ›ci dopasowania modelu dla rÃ³Å¼nej liczby sÄ…siadÃ³w BiorÄ…c pod uwagÄ™ wykres 10.4 moÅ¼na rozwaÅ¼aÄ‡ 3 lub 5 sÄ…siadÃ³w jako optymalne rozwiÄ…zanie, poniewaÅ¼ wÃ³wczas poprawnoÅ›Ä‡ klasyfikacji jest najwyÅ¼sza. Proponuje unikaÄ‡ rozwiÄ…zania z 1 najbliÅ¼szym sÄ…siadem poniewaÅ¼, bÄ™dzie siÄ™ ono charakteryzowaÅ‚o duÅ¼a zmiennoÅ›ciÄ…. WybÃ³r \\(k=3\\) wydaje siÄ™ byÄ‡ optymalny. mod.knn &lt;- knn3(spam~., data = dt.ucz, k = 3) mod.knn ## 3-nearest neighbor model ## Training set outcome distribution: ## ## email spam ## 1860 1207 Predykcji dokonujemy w ten sam sposÃ³b co w innych modelach klasyfikacyjnych pred.knn.class &lt;- predict(mod.knn, newdata = dt.test, type = &quot;class&quot;) head(pred.knn.class) ## [1] spam spam spam spam spam spam ## Levels: email spam pred.knn &lt;- predict(mod.knn, newdata = dt.test) head(pred.knn) ## email spam ## [1,] 0.0000000 1.0000000 ## [2,] 0.3333333 0.6666667 ## [3,] 0.3333333 0.6666667 ## [4,] 0.0000000 1.0000000 ## [5,] 0.3333333 0.6666667 ## [6,] 0.0000000 1.0000000 (tab &lt;- table(pred.knn.class, dt.test$spam)) ## ## pred.knn.class email spam ## email 869 88 ## spam 59 518 sum(diag(prop.table(tab))) ## [1] 0.9041721 metrykÄ™ moÅ¼na wybieraÄ‡ dowolnie, choÄ‡ najczÄ™Å›ciej jest to metryka euklidesowaâ†© "],
["uogolnione-modele-addytywne.html", "11 UogÃ³lnione modele addytywne 11.1 Przypadek jednowymiarowy 11.2 Przypadek wielowymiarowy 11.3 UogÃ³lnione modele addytywne", " 11 UogÃ³lnione modele addytywne Modele liniowe, jako techniki klasyfikacji i regresji, majÄ… niewÄ…tpliwÄ… zaletÄ™ - jawna postaÄ‡ zaleÅ¼noÅ›ci pomiÄ™dzy predyktorami i zmiennÄ… wynikowÄ…. CzÄ™sto w rzeczywistoÅ›ci tak uproszczony model nie potrafi oddaÄ‡ zÅ‚oÅ¼onoÅ›ci natury badanego zjawiska. Dlatego powstaÅ‚ pomysÅ‚ aby w miejsce kombinacji liniowej predyktorÃ³w wstawiÄ‡ kombinacjÄ™ liniowÄ… ich funkcji, czyli \\[\\begin{equation} \\E(Y|X)=f(X) = \\sum_{i=1}^M\\beta_mh_m(X), \\tag{11.1} \\end{equation}\\] gdzie \\(h_m:\\mathbb{R}^d\\to\\mathbb{R}\\) nazywana czÄ™sto funkcjÄ… bazowÄ… (ang. linear basis expansion). WÃ³wczas w zaleÅ¼noÅ›ci od postaci funkcji bazowej otrzymujemy modele z rÃ³Å¼nymi poziomami elastycznoÅ›ci: gdy \\(h_m(X)=X_m,\\ m=1,\\ldots,M\\), to otrzymujemy model liniowy; gdy \\(h_m(X)=X_j^2\\) lub \\(h_m(X)=X_jX_k\\), to otrzymujemy struktury wielomianowe, charakteryzujÄ…ce siÄ™ wiÄ™kszÄ… elastycznoÅ›ciÄ… modelu; gdy \\(h_m(X)=\\log X_j\\) lub \\(h_m(X)=\\sqrt{X_j}\\), to uzyskujemy nieliniowoÅ›Ä‡ czynnikÃ³w wchodzÄ…cych w skÅ‚ad kombinacji liniowej (11.1); dopuszczalne sÄ… rÃ³wnieÅ¼ kawaÅ‚kami liniowe funkcje postaci \\(h_m(X)= I(l_m\\leq X_k &lt;u_m)\\), gdzie \\(I\\) jest funkcjÄ… charakterystycznÄ… (ang. indicator) przedziaÅ‚u \\([l_m,u_m)\\). Zbiory wszystkich funkcji bazowych definiowanych w ten sposÃ³b tworzy sÅ‚ownik funkcji bazowych \\(\\mathcal{D}\\). Aby kontrolowaÄ‡ zÅ‚oÅ¼onoÅ›Ä‡ modeli, majÄ…c do dyspozycji tak zasobny sÅ‚ownik, wprowadza siÄ™ nastÄ™pujÄ…ce podejÅ›cia: ogranicza siÄ™ klasÄ™ dostÄ™pnych funkcji bazowych \\[\\begin{equation} f(X) = \\sum_{j=1}^df_j(X_j)=\\sum_{j=1}^d\\sum_{m=1}^{M_j}\\beta_{jm}h_{jm}(X_j), \\end{equation}\\] wÅ‚Ä…cza siÄ™ do modelu jedynie te funkcje ze sÅ‚ownika \\(\\mathcal{D}\\), ktÃ³re istotnie poprawiajÄ… dopasowanie modelu, uÅ¼ywa siÄ™ metod penalizowanych, czyli dopuszcza siÄ™ stosowanie wszystkich funkcji bazowych ze sÅ‚ownika \\(\\mathcal{D}\\), ale wspÃ³Å‚czynniki przy nich stojÄ…ce sÄ… ograniczane. 11.1 Przypadek jednowymiarowy Dla uproszczenia rozwaÅ¼aÅ„ przyjmiemy, Å¼e \\(X\\) jest jednowymiarowe. Rysunek 11.1: PrzykÅ‚adowe zastosowanie kilku rodzajÃ³w funkcji bazowych. Wykres w lewym gÃ³rnym rogu powstaÅ‚ ze staÅ‚ych na przedziaÅ‚ach, wykres w gÃ³rnym prawym rogu powstaÅ‚ z liniowych funkcji bazowych na przedzialach, w lewym dolnym rogu model powstaÅ‚ rÃ³wnieÅ¼ z liniowych funkcji bazowych na przedzialach ale z zaÅ‚oÅ¼eniem ciÄ…gÅ‚oÅ›ci, a prawym dolnym rogu powstaÅ‚ z zastosowania funcji bazowej \\(\\max(X-\\xi_1,0)\\) Rysunek 11.2: Kolejne wykresy przedstawiajÄ… coraz bardziej gÅ‚adkie modele bÄ™dÄ…ce efektem dodawania wielomianÃ³w trzeciego stopnia na przedziaÅ‚ach. Na kaÅ¼dym kolejnym modelu mymuszone zostaÅ‚y silniejsze zaÅ‚oÅ¼enia dotyczÄ…ce gÅ‚adkoÅ›ci PrzykÅ‚adowo szeÅ›cienny splajn20 dla dwÃ³ch punkÃ³w wÄ™zÅ‚owych skÅ‚ada siÄ™ z nastÄ™pujÄ…cych funkcji bazowych \\[\\begin{gather} h_1(X)=1,\\quad h_3(X)=X^2,\\quad h_5(X)=(X-\\xi_1)_+^3\\\\ h_2(X)=X,\\quad h_4(X)=X^3,\\quad h_6(X)=(X-\\xi_2)_+^3. \\end{gather}\\] Zachowanie wielomianÃ³w poza punktami wÄ™zÅ‚owymi jest czasami bardzo dziwne. Zdarza siÄ™, Å¼e charakteryzujÄ… siÄ™ tam duÅ¼Ä… zmiennoÅ›ciÄ…. Dlatego wprowadza siÄ™ takie splajny aby w obszarach brzegowych zachowywaÅ‚y siÄ™ przewidywalnie. Naturalny splajn szeÅ›cienny zakÅ‚ada liniowoÅ›Ä‡ modelu poza wÄ™zÅ‚ami brzegowymi. Dla \\(K\\) wÄ™zÅ‚Ã³w naturalny splajn szeÅ›cienny skÅ‚ada siÄ™ z nastÄ™pujÄ…cych funkcji bazowych \\[\\begin{gather} N_1(X)=1,\\quad N_2(X)=X,\\quad N_{k+2}(X)=d_k(X)-d_{K-1}(X), \\end{gather}\\] gdzie \\(d_k(X)=\\frac{(X-\\xi_k)^3_+-(X-\\xi_K)^3_+}{\\xi_K-\\xi_k}.\\) Estymacji parametrÃ³w modelu dokonujemy metodÄ… najmniejszych kwadratÃ³w, minimalizujÄ…c \\[\\begin{equation} RSS(f,\\lambda) = \\sum_{i=1}^N(y_i-f(x_i))^2+\\lambda\\int(f&#39;&#39;(t))^2dt, \\end{equation}\\] gdzie \\(\\lambda\\) jest parametrem wygÅ‚adzania. Pierwsze wyraÅ¼enie po prawej stronie to ocena dopasowania, a drugie to kara za krzywoliniowoÅ›Ä‡. Dla naturalnego splajna \\[\\begin{equation} f(x)=\\sum_{j=1}^NN_j(x)\\beta_j \\end{equation}\\] minimalizujemy \\[\\begin{equation} RSS(\\beta, \\lambda)=(\\boldsymbol y -\\boldsymbol N\\beta)&#39;(\\boldsymbol y-\\boldsymbol N\\beta)+\\lambda\\beta&#39;\\boldsymbol \\Omega \\beta, \\end{equation}\\] gdzie \\(\\{\\boldsymbol N\\}_{ij}= N_j(x_i)\\) i \\(\\{\\boldsymbol \\Omega\\}_{jk}=\\int N&#39;&#39;_j(t)N&#39;&#39;_k(t)dt\\). RozwiÄ…zaniem zaganienia minimalizacji \\(RSS(\\beta,\\lambda)\\) jest \\[\\begin{equation} \\hat{\\beta}=((\\boldsymbol N&#39;\\boldsymbol N)+\\lambda\\boldsymbol \\Omega)^{-1}\\boldsymbol N&#39;\\boldsymbol y. \\end{equation}\\] 11.2 Przypadek wielowymiarowy W przypadku gdy \\(X\\in \\mathbb{R}^d\\) poszukujemy takiej \\(d\\)-wymiarowej regresji \\(f(x)\\), ktÃ³ra bÄ™dzie minimalizowaÅ‚a wyraÅ¼enie \\[\\begin{equation} \\min_f\\sum_{i=1}^N(y_i-f(x_i))^2+\\lambda J(f), \\end{equation}\\] gdzie \\(J\\) jest odpowiedniÄ… funkcjÄ… wyraÅ¼ajÄ…cÄ… krzywoliniowoÅ›Ä‡ modelu. Dla \\(X\\in \\mathbb{R}^2\\) przyjmuje postaÄ‡ \\[\\begin{equation} J(f)=\\iint_{\\mathbb{R}^2}\\left[\\left(\\frac{\\partial^2 f(x)}{\\partial^2 x_1}\\right)^2+2\\left(\\frac{\\partial^2 f(x)}{\\partial x_1\\partial x_2}\\right)^2+ \\left(\\frac{\\partial^2 f(x)}{\\partial^2 x_2}\\right)^2\\right]dx_1dx_2. \\end{equation}\\] RozwiÄ…zanie przyjmuje postaÄ‡ \\[\\begin{equation} f(x) = \\beta_0+\\beta&#39;x+\\sum_{i=1}^N \\alpha_ih_i(x), \\end{equation}\\] gdzie \\(h_i(x)=||x-x_j||^2\\log||x-x_j||\\). 11.3 UogÃ³lnione modele addytywne Przez uogÃ³lnione modele addytywne (ang. Generalized Additive Models) rozumiemy klasÄ™ modeli, ktÃ³re poprzez funkcjÄ™ Å‚Ä…czÄ…cÄ…, opisujÄ… warunkowÄ… wartoÅ›Ä‡ zmiennej wynikowej w nastÄ™pujÄ…cy sposÃ³b \\[\\begin{equation} g(\\E(Y|X))=g(\\mu(X))=\\alpha+f_1(X_1)+\\dots+f_d(X_d), \\end{equation}\\] gdzie \\(g\\) jest funkcjÄ… Å‚Ä…czÄ…cÄ…. NajczÄ™Å›ciej stosowanymi funkcjami Å‚Ä…czÄ…cymi sÄ…: \\(g(\\mu)=\\mu\\) - stosowana w modelach, gdy zmienna wynikowa ma rozkÅ‚ad normalny; \\(g(\\mu)=\\logit\\mu\\) - stosowana, gdy zmienna wynikowa ma rozkÅ‚ad dwumianowy; \\(g(\\mu)=\\probit\\mu\\) - stosowana rÃ³wnieÅ¼ w przypadku gdy zmienna ma rozkÅ‚ad dwumianowy, a \\(\\Phi^{-1}\\) oznacza odwrotnoÅ›Ä‡ dystrybuanty standaryzowanego rozkÅ‚adu normalnego; \\(g(\\mu)=\\log\\mu\\) - stosowana, gdy zmienna wynikowa jest zmiennÄ… typu zliczeniowego (rozkÅ‚ad Poissona). 11.3.1 Algorytm uczenia modelu GAM Algorytm uczenia wstecznego (ang. backfitting) przebiega wg nastÄ™pujÄ…cych krokÃ³w: Ustalamy wstÄ™pne oszacowania na \\(\\alpha=\\bar{y}\\) i \\(\\hat{f}_j=0\\). Dla \\(j=1,\\ldots,d,1,\\ldots,d,1,\\ldots\\) powtarzamy szacowanie \\[\\begin{align} \\hat{f}_j\\leftarrow &amp;\\mathcal{S}_j\\left[(y_i-\\hat{\\alpha}-\\sum_{k\\neq j}\\hat{f}_k(x_{ik}))^N_1\\right],\\\\ \\hat{f}_j\\leftarrow &amp;\\hat{f}_j-\\frac{1}{N}\\sum_{i=1}^N\\hat{f}_j(x_{ij}) \\end{align}\\] dopÃ³ki \\(\\hat{f}_j\\) osiÄ…gnie zbieÅ¼noÅ›Ä‡. Funkcja \\(\\mathcal{S}_j\\left[(y_i-\\hat{\\alpha}-\\sum_{k\\neq j}\\hat{f}_k(x_{ik}))^N_1\\right]\\) jest jednowymiarowym szeÅ›ciennym splajnem o \\(N\\) wÄ™zÅ‚ach. W jej miejsce moÅ¼na przyjÄ…Ä‡ rÃ³wnieÅ¼ inne funkcje, takie jak: jednowymiarowe lokalne regresje wielomianowe (ang. LOESS - locally estimated scatterplot smoothing), regresje liniowe, wielomianowe. PrzykÅ‚ad 11.1 Dla zilustrowania zasady dziaÅ‚ania uogÃ³lnionych modeli addytywnych przeprowadzimy analizÄ™ stÄ™Å¼enia ozonu (\\(O_3\\)) w zaleÅ¼noÅ›ci od wybranych parametrÃ³w meteorologicznych. Do zbudowania modelu GAM wykorzystamy funkcjÄ™ gam pakietu mgcv (Wood 2003). library(faraway) head(ozone) ## O3 vh wind humidity temp ibh dpg ibt vis doy ## 1 3 5710 4 28 40 2693 -25 87 250 33 ## 2 5 5700 3 37 45 590 -24 128 100 34 ## 3 5 5760 3 51 54 1450 25 139 60 35 ## 4 6 5720 4 69 35 1568 15 121 60 36 ## 5 4 5790 6 19 45 2631 -33 123 100 37 ## 6 4 5790 3 25 55 554 -28 182 250 38 library(mgcv) mod.gam &lt;- gam(O3~s(temp, bs = &quot;cr&quot;, m = 2)+s(ibh)+s(ibt), data = ozone) summary(mod.gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## O3 ~ s(temp, bs = &quot;cr&quot;, m = 2) + s(ibh) + s(ibt) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.7758 0.2382 49.44 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(temp) 3.357 4.216 20.758 5.99e-16 *** ## s(ibh) 4.171 5.072 7.344 1.37e-06 *** ## s(ibt) 2.111 2.729 1.403 0.213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.708 Deviance explained = 71.7% ## GCV = 19.348 Scale est. = 18.724 n = 330 W powyÅ¼szym modelu uÅ¼yto splajnÃ³w jako funkcji \\(f_i\\). W przypadku zmiennej temp byÅ‚ to szeÅ›cienny splajn z regularyzacjÄ… w postaci ciÄ…gÅ‚oÅ›ci drugiej pochodnej, a pozostaÅ‚e sÄ… prostymi splajnami. Dopasowanie modelu siÄ™ga 71.7% a wartoÅ›Ä‡ uogÃ³lnionego sprawdzianu krzyÅ¼owego 19.35. PoniÅ¼szy wykres pokazuje rodzaje transformacji uÅ¼yte przy dopasowaniu modelu. par(mfrow = c(1,3)) plot(mod.gam, shade = T, residuals = T) par(mfrow=c(1,1)) Bibliografia "],
["metoda-wektorow-nosnych.html", "12 Metoda wektorÃ³w noÅ›nych 12.1 Wprowadzenie 12.2 Definicja modelu dla klas liniowo separowalnych 12.3 Definicja modelu dla klas nieliniowo separowalnych 12.4 Definicja modelu jÄ…drowego 12.5 Zalety i wady", " 12 Metoda wektorÃ³w noÅ›nych 12.1 Wprowadzenie Metoda wektorÃ³w noÅ›nych21 (ang. Support Vector Machines) to kolejna metoda klasyfikacji obserwacji na podstawie cech (atrybutÃ³w). Jest technikÄ… z nauczycielem tzn., Å¼e w prÃ³bie uczÄ…cej wystÄ™pujÄ… zarÃ³wno cechy charakteryzujÄ…ce badane obiekty jak i ich przynaleÅ¼noÅ›Ä‡ do klasy. Rysunek 12.1: PrzykÅ‚ad prostych separujÄ…cych obiekty obu grup 12.2 Definicja modelu dla klas liniowo separowalnych IstotÄ… tej metody jest znalezienie wektorÃ³w noÅ›nych, definiujÄ…cych hiperpowierzchnie optymalnie separujÄ…ce obiekty w homogeniczne grupy. Niech \\(D\\) bÄ™dzie zbiorem \\(n\\) punktÃ³w w \\(d\\)-wymiarowej przestrzeni okreÅ›lonych nastÄ™pujÄ…co \\((\\vec{x}_i, y_i)\\), \\(i=1,\\ldots, d\\), gdzie \\(y_i\\) przyjmuje wartoÅ›ci -1 lub 1 w zaleÅ¼noÅ›ci od tego do ktÃ³rej grupy naleÅ¼y (zakÅ‚adamy istnienie tylko dwÃ³ch grup). Poszukujemy takiej hiperpÅ‚aszczyzny, ktÃ³ra maksymalizuje margines pomiÄ™dzy punktami obu klas w przestrzeni cech \\(\\vec{x}\\). Rysunek 12.2: PÅ‚aszczyzna najlepiej rozdzielajÄ…ca obiekty obu grup (biaÅ‚e i czarne kropki) wraz z prostymi wyznaczajÄ…cymi maksymalny margines separujÄ…cy obie grupy Margines ten jest okreÅ›lany jako najmniejsza odlegÅ‚oÅ›Ä‡ pomiÄ™dzy hiperpÅ‚aszczyznÄ… i elementami z kaÅ¼dej z grup. Dowolna hiperpÅ‚aszczyzna moÅ¼e byÄ‡ zapisana rÃ³wnaniem \\(\\vec{w}\\vec{x}-b=0\\), gdzie \\(\\vec{w}\\) jest wektorem normalnym do hiperpÅ‚aszczyzny. JeÅ›li dane sÄ… liniowo separowalne to, moÅ¼na wybraÄ‡ takie dwie hiperpÅ‚aszczyzny, Å¼e odlegÅ‚oÅ›Ä‡ pomiÄ™dzy nimi jest najwiÄ™ksza. RÃ³wnania tych hiperpÅ‚aszczyzn dane sÄ… wzorami \\[\\begin{equation} \\vec{w}\\vec{x}-b=1, \\quad \\vec{w}\\vec{x}-b=-1 \\tag{12.1} \\end{equation}\\] OdlegÅ‚oÅ›Ä‡ pomiÄ™dzy tymi hiperpÅ‚aszczyznami wynosi \\(\\tfrac{2}{\\|\\vec{w}\\|}\\). Zatem Å¼eby zmaksymalizowaÄ‡ odlegÅ‚oÅ›Ä‡ pomiÄ™dzy hiperpÅ‚aszczyznami (margines) musimy zminimalizowaÄ‡ \\(\\tfrac{\\|\\vec{w}\\|}{2}\\). Dodatkowo, Å¼eby nie pozwoliÄ‡ aby punkty wpadaÅ‚y do marginesu musimy naÅ‚oÅ¼yÄ‡ dodatkowe ograniczenia \\[\\begin{align} \\vec{w}\\vec{x}_i-b\\geq&amp; 1, \\quad y_i=1\\\\ \\vec{w}\\vec{x}_i-b\\leq&amp; -1, \\quad y_i=-1 \\tag{12.2} \\end{align}\\] Co moÅ¼na zapisaÄ‡ proÅ›ciej \\[\\begin{equation} y_i(\\vec{w}\\vec{x}_i-b)\\geq 1,\\quad 1\\leq i\\leq n. \\tag{12.3} \\end{equation}\\] Zatem \\(\\vec{w}\\) i \\(b\\) minimalizujÄ…ce \\(\\|\\vec{w}\\|\\) przy jednoczesnym speÅ‚nieniu warunku definiujÄ… klasyfikator postaci \\[\\begin{equation} \\vec{x}\\rightarrow \\operatorname{sgn}(\\vec{w}\\vec{x}-b). \\tag{12.4} \\end{equation}\\] Z racji, Å¼e \\(\\|\\vec{w}\\|\\) jest okreÅ›lona jako pierwiastek sumy kwadratÃ³w poszczegÃ³lnych wspÃ³Å‚rzÄ™dnych wektora, to czÄ™Å›ciej w minimalizacji stosuje siÄ™ \\(\\|\\vec{w}\\|^2\\). SformuÅ‚owany powyÅ¼ej problem naleÅ¼y do grupy optymalizacji funkcji kwadratowej przy liniowych ograniczeniach. RozwiÄ…zuje siÄ™ go metodÄ… mnoÅ¼nikÃ³w Lagrangeâ€™a. Minimalizujemy funkcjÄ™ \\[\\begin{equation} L(w, b, \\alpha) = \\frac{1}{2}\\|\\vec{w}\\|^2-\\sum_{i=1}^{n}\\alpha_i\\big(y_i(\\vec{w}\\vec{x}_i-b)-1\\big), \\tag{12.5} \\end{equation}\\] gdzie \\(\\alpha_i\\) sÄ… mnoÅ¼nikami Lagrangeâ€™a. Niestety rozwiÄ…zanie takiego rÃ³wnania rÃ³Å¼niczkujÄ…c po \\(\\vec{w}\\) i \\(b\\) i przyrÃ³wnujÄ…c do zera nie jest Å‚atwe. Dlatego Karush-Kuhn Tucker wprowadzili ograniczenia na mnoÅ¼niki \\(\\alpha_i\\geq 0\\) oraz \\(\\alpha_i\\big(y_i(\\vec{w}\\vec{x}_i-b)-1\\big)=0\\). Co w konsekwencji powoduje, Å¼e \\(\\alpha_i\\) sÄ… niezerowe jedynie dla wektorÃ³w noÅ›nych, a dla pozostaÅ‚ych 0. Dalej jednak poszukiwanie rozwiÄ…zania zagadnienia minimalizacji funkcji \\(L\\) ze wzglÄ™du na tak wiele parametrÃ³w moÅ¼e byÄ‡ uciÄ…Å¼liwe. WÃ³wczas stosuje siÄ™ maksymalizacjÄ™ dualnej wersji22 \\[\\begin{equation} L_D(\\alpha) = \\sum_{i=1}^{n}\\alpha_i+\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j\\vec{x}_i&#39;\\vec{x}_j \\tag{12.6} \\end{equation}\\] przy ograniczeniach \\(\\alpha_i\\geq 0\\) i \\(\\sum_{i=1}^{n}\\alpha_iy_i=0\\). RozwiÄ…zaniem powyÅ¼szego zagadnienia jest \\[\\begin{align} \\vec{w}=&amp;\\sum_{i=1}^{n}\\alpha_iy_i\\vec{x}_i,\\tag{12.7}\\\\ b=&amp;y_i-\\vec{w}\\vec{x}_i, \\tag{12.8} \\end{align}\\] a hiperpÅ‚aszczyzna decyzyjna \\[\\begin{equation} \\sum_{i=1}^{n}\\alpha_iy_i\\vec{x}_i\\vec{x}-b=0, \\tag{12.9} \\end{equation}\\] gdzie \\(\\vec{x}_i\\) sÄ… wektorami noÅ›nymi ze zbioru uczÄ…cego, a \\(\\vec{x}\\) jest nowym wektorem dla ktÃ³rego przeprowadzamy klasyfikacjÄ™. NaleÅ¼y rÃ³wnieÅ¼ zauwaÅ¼yÄ‡, Å¼e im wiÄ™ksza wartoÅ›Ä‡ \\(\\alpha_i\\), tym wiÄ™kszy wpÅ‚yw wektora na granicÄ™ decyzyjnÄ…. 12.3 Definicja modelu dla klas nieliniowo separowalnych Niestety rzadko przestrzeÅ„ atrybutÃ³w jest liniowo separowalna. Stosuje siÄ™ wÃ³wczas modyfikacjÄ™ powyÅ¼szej metody przez wprowadzenie nastÄ™pujÄ…cej funkcji straty \\[\\begin{equation} \\zeta_i=\\max\\big(0,1-y_i(\\vec{w}\\vec{x}_i-b)\\big). \\tag{12.10} \\end{equation}\\] ZauwaÅ¼my, Å¼e \\(\\zeta_i\\) jest najmniejszÄ… liczbÄ… nieujemnÄ… speÅ‚niajÄ…cÄ… nierÃ³wnoÅ›Ä‡ \\[\\begin{equation} y_i(\\vec{w}\\vec{x}_i-b)\\geq 1-\\zeta_i. \\tag{12.11} \\end{equation}\\] MoÅ¼emy jÄ… interpretowaÄ‡ tak, Å¼e jeÅ›li warunek (12.3) jest speÅ‚niony, czyli punkty leÅ¼Ä… na zewnÄ…trz marginesu (po wÅ‚aÅ›ciwych stronach), to funkcja straty przyjmuje wartoÅ›Ä‡ 0. W przeciwnym przypadku wartoÅ›Ä‡ funkcji jest proporcjonalna do odlegÅ‚oÅ›ci od brzegu marginesu. Dlatego wystarczy zminimalizowaÄ‡ wartoÅ›Ä‡ \\[\\begin{equation} \\frac{1}{n}\\sum_{i=1}^{n}\\zeta_i+\\lambda\\|\\vec{w}\\|^2, \\tag{12.12} \\end{equation}\\] przy warunku (12.11) i \\(\\zeta_i\\geq 0\\) oraz gdzie \\(\\lambda\\) jest wagÄ… kompromisu pomiÄ™dzy szerokoÅ›ciÄ… marginesu a zapewnieniem, Å¼e punkty leÅ¼Ä… po wÅ‚aÅ›ciwych stronach marginesu. Przy dostatecznie maÅ‚ych wartoÅ›ciach \\(\\lambda\\) i separowalnoÅ›ci liniowej punktÃ³w przestrzeni atrybutÃ³w powyÅ¼szy klasyfikator bÄ™dzie siÄ™ zachowywaÅ‚ podobnie jak (12.4). RozwiÄ…zanie problemu minimalizacji funkcji straty okreÅ›lonej w (12.12) za pomocÄ… dualnej wersji mnoÅ¼nikÃ³w Lagrangeâ€™a sprowadza siÄ™ do minimalizacji funkcji \\[\\begin{equation} L(\\alpha_i) = \\sum_{i=1}^{n}\\alpha_i+\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j\\vec{x}_i&#39;\\vec{x}_j, \\tag{12.13} \\end{equation}\\] przy warunkach \\[\\begin{equation} \\sum_{i=1}^{n}\\alpha_iy_i=0,\\quad 0\\leq \\alpha_i\\leq \\frac{1}{2n\\lambda}. \\tag{12.14} \\end{equation}\\] Wektor normalny do hiperpÅ‚aszczyzny jest postaci \\[\\begin{equation} \\vec{w}=\\sum_{i=1}^{n}\\alpha_iy_i\\vec{x}_i, \\tag{12.15} \\end{equation}\\] a parametr \\(b\\) taki jak w (12.8). PowyÅ¼szy algorytm zostaÅ‚ przedstawiony przez Vapnika w 1963 roku jako klasyfikator liniowy ale dopiero po wprowadzeniu funkcji jÄ…drowych przeksztaÅ‚cajÄ…cych liniowy brzeg decyzyjny na nieliniowy, metoda ta zyskaÅ‚a w oczach statystykÃ³w. 12.4 Definicja modelu jÄ…drowego W roku 1992 Boser, Guyon i Vapnik wprowadzili pojÄ™cie nieliniowego klasyfikatora opartego na metodzie wektorÃ³w noÅ›nych, ktÃ³ry byÅ‚o uogÃ³lnieniem techniki przedstawionej przez Vapnika w 1963 roku. Pozwala ona na nieliniowy ksztaÅ‚t brzegu obszaru decyzyjnego. Zasada dziaÅ‚ania polega na znalezieniu takiego jÄ…dra przeksztaÅ‚cenia (ang. kernel) \\(\\phi\\), ktÃ³re odwzoruje przestrzeÅ„ \\(d\\)-wymiarowÄ… w \\(d&#39;\\)-wymiarowÄ…, gdzie \\(d&#39;&gt;d\\) takÄ…, Å¼e \\(D_{\\phi}=\\{\\phi(\\vec{x}_i), y_i\\}\\) jest moÅ¼liwie jak najbardziej separowalna. Rysunek 12.3: PrzykÅ‚ad zastosowania takiego przeksztaÅ‚cenia jÄ…drowego aby z sytuacji braku liniowej separowalnoÅ›ci do niej doprowadziÄ‡ Dla funkcji jÄ…drowej okreÅ›lonej wzorem \\(k(\\vec{x}_i,\\vec{x}_j)=\\phi(\\vec{x}_i)\\phi(\\vec{x}_j)\\) przeprowadzamy identyczne rozumowanie jak w przypadku liniowych brzegÃ³w obszarÃ³w decyzyjnych. Minimalizujemy zatem wyraÅ¼enie \\[\\begin{align} L(\\alpha_i) =&amp; \\sum_{i=1}^{n}\\alpha_i+\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j\\phi(\\vec{x}_i)\\phi(\\vec{x}_j)\\\\ =&amp;\\sum_{i=1}^{n}\\alpha_i+\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_jk(\\vec{x}_i,\\vec{x}_j), \\tag{12.16} \\end{align}\\] przy warunkach \\[\\begin{equation} \\sum_{i=1}^{n}\\alpha_iy_i=0,\\quad 0\\leq \\alpha_i\\leq \\frac{1}{2n\\lambda}. \\tag{12.17} \\end{equation}\\] RozwiÄ…zanie powyÅ¼szego problemu sÄ… rÃ³wnieÅ¼ podobne do ich liniowych odpowiednikÃ³w \\[\\begin{equation} \\vec{w}=\\sum_{i=1}^{n}\\alpha_iy_i\\phi(\\vec{x}_i), \\tag{12.18} \\end{equation}\\] a parametr \\(b=\\vec{w}\\phi(\\vec{x}_i)-y_i\\). NajczÄ™Å›ciej stosowanymi funkcjami jÄ…drowymi sÄ…: wielomianowa \\(k(\\vec{x}_i,\\vec{x}_j)=(a\\vec{x}_i&#39;\\vec{x}_j+b)^q\\), gaussowska \\(k(\\vec{x}_i,\\vec{x}_j)=\\exp(-\\gamma\\|\\vec{x}_i-\\vec{x}_j\\|^2)\\), Laplaceâ€™a \\(k(\\vec{x}_i,\\vec{x}_j)=\\exp(-\\gamma\\|\\vec{x}_i-\\vec{x}_j\\|)\\), hiperboliczna \\(k(\\vec{x}_i,\\vec{x}_j)=\\tanh(\\vec{x}_i&#39;\\vec{x}_j+b)\\), sigmoidalna \\(k(\\vec{x}_i,\\vec{x}_j)=\\tanh(a\\vec{x}_i&#39;\\vec{x}_j+b)\\), Besselâ€™a \\(k(\\vec{x}_i,\\vec{x}_j)=\\frac{Bessel^n_{(\\nu+1)}(\\sigma\\|\\vec{x}_i-\\vec{x}_j\\|)}{(\\|\\vec{x}_i-\\vec{x}_j\\|)^{n(\\nu+1)}}\\), ANOVA \\(k(\\vec{x}_i,\\vec{x}_j)=\\left(\\sum_{k=1}^{n}\\exp\\big(-\\sigma(x^k_i-x^k_j)^2\\big)\\right)^d\\), sklejana dla jednowymiarowej przestrzeni \\(k(x_i,x_j)=1+x_ix_j\\min(x_i,x_j)-\\frac{x_i+x_j}{2}\\big(\\min(x_i,x_j)\\big)^2+\\frac{(\\min(x_i,x_j))^3}{3}\\). W przypadku braku wiedzy o danych funkcja gaussowska, Laplaceâ€™a i Besselâ€™a sÄ… zalecane. PrzykÅ‚ady obszarÃ³w zastosowaÅ„: w kategoryzacji tekstu i hipertekstu; klasyfikacji obrazÃ³w - rezultaty eksperymentÃ³w pokazujÄ…, Å¼e SVM daje lepsze rezultaty niÅ¼ inne techniki; rozpoznawanie obiektÃ³w 3D; odnajdowanie wÅ‚amaÅ„ do systemu; rozpoznawanie pisma rÄ™cznego; odkrywanie ukrytych treÅ›ci na zdjÄ™ciach; klasyfikacja protein; odnajdowanie sekwencji kodu genetycznego itpâ€¦ 12.5 Zalety i wady Mocne strony: stopieÅ„ skomplikowania nie jest zaleÅ¼ny od wymiaru przestrzeni atrybutÃ³w; optymalny klasyfikator (znajduje minimum globalne); nie jest czuÅ‚y na przetrenowanie; bardzo duÅ¼a skutecznoÅ›Ä‡ w praktyce. SÅ‚abe strony: przy duÅ¼ej iloÅ›ci danych estymacja modelu moÅ¼e trwaÄ‡ dÅ‚ugo; estymacja poprawnego modelu wymaga pewnej wiedzy; nie ma miejsca na wprowadzenie wÅ‚asnej wiedzy. lub podpierajÄ…cychâ†© w przypadku przestrzeni wypukÅ‚ej oba rozwiÄ…zania siÄ™ pokrywajÄ…â†© "],
["bibliografia.html", "Bibliografia", " Bibliografia "]
]
