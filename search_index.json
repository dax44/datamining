[
["pochodne-drzew-decyzyjnych.html", "5 Pochodne drzew decyzyjnych 5.1 Bagging 5.2 Lasy losowe 5.3 Boosting", " 5 Pochodne drzew decyzyjnych Przykład zastosowania drzew decyzyjnych na zbiorze iris w poprzednich przykładach może skłaniać do przypuszczenia, że drzewa decyzyjne zawsze dobrze radzą sobie z predykcją wartości wynikowej. Niestety w przykładach nieco bardziej skomplikowanych, gdzie chociażby klasy zmiennej wynikowej nie są tak wyraźnie separowalne, drzewa decyzyjne wypadają gorzej w porównaniu z innymi modelami nadzorowanego uczenia maszynowego. I tak u podstaw metod bazujących na prostych drzewach decyzyjnych stał pomysł, że skoro jedno drzewo nie ma wystarczających własności predykcyjnych, to może zastosowanie wielu drzew połączonych w pewien sposób poprawi je. Tak powstały metody bagging, random forest i boosting1. Należy zaznaczyć, że metody znajdują swoje zastosowanie również w innych modelach nadzorowanego uczenia maszynowego. 5.1 Bagging Technika ta została wprowadzona przez Breiman (1996) i ma na celu zmniejszenie wariancji modelu pojedynczego drzewa. Podobnie jak technika bootstrap, w której statystyki są wyliczane na wielu próbach pobranych z tego samego rozkładu (próby), w metodzie bagging losuje się wiele prób ze zbioru uczącego (najczęściej poprzez wielokrotne losowanie próby o rozmiarze zbioru uczącego ze zwracaniem), a następnie dla każdej próby bootstrapowej buduje się drzewo. W ten sposób otrzymujemy \\(B\\) drzew decyzyjnych \\(\\hat{f}^1(x), \\hat{f}^2(x),\\ldots, \\hat{f}^B(x)\\). Na koniec poprzez uśrednienie otrzymujemy model charakteryzujący się większą precyzją \\[\\begin{equation} \\hat{f}_{bag}(x)=\\frac1B\\sum_{b=1}^B\\hat{f}^b(x). \\end{equation}\\] Ponieważ podczas budowy drzew na podstawie prób bootstrapowych nie kontrolujemy złożoności, to w rezultacie każde z drzew może charakteryzować się dużą wariancją. Poprzez uśrednianie wyników pojedynczych drzew otrzymujemy mniejsze obciążenie ale również przy dostatecznie dużej liczbie prób (\\(B\\) często liczy się w setkach, czy tysiącach) zmniejszamy wariancję “średniej” predykcji z drzew. Oczywiście metodę tą trzeba dostosować do zadań klasyfikacyjnych, ponieważ nie istnieje średnia klasyfikacji z wielu drzew. W miejsce średniej stosuje się modę, czyli wartość dominującą. Przyjrzyjmy się jak maszyna losuje obserwacje ze zwracaniem n &lt;- NULL m &lt;- NULL for(i in 1:1000){ x &lt;- sample(1:500, size = 500, replace = T) y &lt;- setdiff(1:500, x) z &lt;- unique(x) n[i] &lt;- length(z) m[i] &lt;- length(y) } mean(n)/500*100 ## [1] 63.148 mean(m)/500*100 ## [1] 36.852 Faktycznie uczenie modelu metodą bagging odbywa się średnio na 2/3 obserwacji zbioru uczącego wylosowanych do prób bootstrapowych, a pozostała 1/3 (ang. out-of-bag) jest wykorzystana do oceny jakości predykcji. Niewątpliwą zaletą drzew decyzyjnych była ich łatwa interpretacja. W przypadku metody bagging jest ona znacznie utrudniona, ponieważ jej wynik składa się z agregacji wielu drzew. Można natomiast ocenić ważność predyktorów (ang. variable importance). I tak, przez obserwację spadku \\(RSS\\) dla baggingu regresyjnego przy zastosowaniu danego predyktora w podziałach drzewa i uśrednieniu wyniku otrzymamy wskaźnik ważności predyktora dużo lepszy niż dla pojedynczego drzewa. W przypadku baggingu klasyfikacyjnego w miejsce \\(RSS\\) stosujemy indeks Gini’ego. Implementacja R-owa metody bagging znajduje się w pakiecie ipred, a funkcja do budowy modelu nazywa się bagging (Peters and Hothorn 2018). Można również stosować funkcję randomForest pakietu randomForest (Liaw and Wiener 2002) - powody takiego działania wyjaśnią się w podrozdziale Lasy losowe. Przykład 5.1 Tym razem cel zadania jest regresyjny i polega na ustaleniu miary tendencji centralnej ceny mieszkań w Bostonie na podstawie zmiennych umieszczonych w zbiorze Boston pakietu MASS (Venables and Ripley 2002). Zmienną zależną będzie mediana cen mieszkań na przedmieściach Bostonu (medv). library(MASS) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 set.seed(2019) boston.train &lt;- Boston %&gt;% sample_frac(size = 2/3) boston.test &lt;- setdiff(Boston, boston.train) Aby móc porównać wyniki predykcji z metody bagging, najpierw zostanie zbudowane jedno drzewo decyzyjne w oparciu o algorytm CART. library(rpart) library(rpart.plot) boston.rpart &lt;- rpart(medv~., data = boston.train) x &lt;- summary(boston.rpart) ## Call: ## rpart(formula = medv ~ ., data = boston.train) ## n= 337 ## ## CP nsplit rel error xerror xstd ## 1 0.43506104 0 1.0000000 1.0037495 0.10496568 ## 2 0.21114710 1 0.5649390 0.6856438 0.07732133 ## 3 0.05641774 2 0.3537919 0.4393220 0.05974589 ## 4 0.04154842 3 0.2973741 0.3726563 0.05716622 ## 5 0.02707678 4 0.2558257 0.3520312 0.05569786 ## 6 0.01489117 5 0.2287489 0.3238915 0.05681943 ## 7 0.01202564 6 0.2138578 0.2922610 0.05311293 ## 8 0.01057622 7 0.2018321 0.2889364 0.05318206 ## 9 0.01031677 8 0.1912559 0.2838433 0.05152251 ## 10 0.01006729 9 0.1809391 0.2838187 0.05152098 ## 11 0.01000000 10 0.1708718 0.2815210 0.05152993 ## ## Variable importance ## lstat nox indus crim tax rm age dis ptratio ## 24 13 13 13 11 10 10 2 2 ## rad black ## 1 1 ## ## Node number 1: 337 observations, complexity param=0.435061 ## mean=22.61157, MSE=79.33004 ## left son=2 (186 obs) right son=3 (151 obs) ## Primary splits: ## lstat &lt; 10.02 to the right, improve=0.4350610, (0 missing) ## rm &lt; 6.8375 to the left, improve=0.4305766, (0 missing) ## indus &lt; 6.66 to the right, improve=0.2914821, (0 missing) ## ptratio &lt; 19.15 to the right, improve=0.2608119, (0 missing) ## nox &lt; 0.5125 to the right, improve=0.2169607, (0 missing) ## Surrogate splits: ## indus &lt; 7.625 to the right, agree=0.846, adj=0.656, (0 split) ## nox &lt; 0.519 to the right, agree=0.828, adj=0.616, (0 split) ## crim &lt; 0.12995 to the right, agree=0.786, adj=0.523, (0 split) ## age &lt; 63.9 to the right, agree=0.777, adj=0.503, (0 split) ## tax &lt; 377 to the right, agree=0.769, adj=0.483, (0 split) ## ## Node number 2: 186 observations, complexity param=0.05641774 ## mean=17.31828, MSE=19.86042 ## left son=4 (58 obs) right son=5 (128 obs) ## Primary splits: ## crim &lt; 5.84803 to the right, improve=0.4083024, (0 missing) ## dis &lt; 2.0754 to the left, improve=0.3684093, (0 missing) ## lstat &lt; 14.405 to the right, improve=0.3516672, (0 missing) ## nox &lt; 0.657 to the right, improve=0.3255969, (0 missing) ## age &lt; 84.9 to the right, improve=0.2247741, (0 missing) ## Surrogate splits: ## rad &lt; 16 to the right, agree=0.855, adj=0.534, (0 split) ## tax &lt; 551.5 to the right, agree=0.839, adj=0.483, (0 split) ## nox &lt; 0.657 to the right, agree=0.828, adj=0.448, (0 split) ## dis &lt; 2.0754 to the left, agree=0.801, adj=0.362, (0 split) ## lstat &lt; 19.055 to the right, agree=0.796, adj=0.345, (0 split) ## ## Node number 3: 151 observations, complexity param=0.2111471 ## mean=29.13179, MSE=75.5574 ## left son=6 (120 obs) right son=7 (31 obs) ## Primary splits: ## rm &lt; 7.127 to the left, improve=0.4947648, (0 missing) ## lstat &lt; 4.495 to the right, improve=0.4054324, (0 missing) ## nox &lt; 0.574 to the left, improve=0.1389706, (0 missing) ## ptratio &lt; 14.75 to the right, improve=0.1349232, (0 missing) ## age &lt; 89.45 to the left, improve=0.1133301, (0 missing) ## Surrogate splits: ## lstat &lt; 3.21 to the right, agree=0.841, adj=0.226, (0 split) ## ptratio &lt; 14.15 to the right, agree=0.828, adj=0.161, (0 split) ## tax &lt; 207 to the right, agree=0.808, adj=0.065, (0 split) ## nox &lt; 0.639 to the left, agree=0.801, adj=0.032, (0 split) ## ## Node number 4: 58 observations ## mean=13.08793, MSE=14.14485 ## ## Node number 5: 128 observations, complexity param=0.01489117 ## mean=19.23516, MSE=10.66681 ## left son=10 (61 obs) right son=11 (67 obs) ## Primary splits: ## lstat &lt; 14.405 to the right, improve=0.2915760, (0 missing) ## dis &lt; 1.99235 to the left, improve=0.2280873, (0 missing) ## age &lt; 84.15 to the right, improve=0.1950219, (0 missing) ## ptratio &lt; 20.95 to the right, improve=0.1349341, (0 missing) ## rm &lt; 5.706 to the left, improve=0.1194638, (0 missing) ## Surrogate splits: ## age &lt; 91.15 to the right, agree=0.758, adj=0.492, (0 split) ## dis &lt; 2.0418 to the left, agree=0.664, adj=0.295, (0 split) ## nox &lt; 0.607 to the right, agree=0.633, adj=0.230, (0 split) ## indus &lt; 18.84 to the right, agree=0.625, adj=0.213, (0 split) ## rm &lt; 5.703 to the left, agree=0.617, adj=0.197, (0 split) ## ## Node number 6: 120 observations, complexity param=0.04154842 ## mean=26.02417, MSE=34.39883 ## left son=12 (98 obs) right son=13 (22 obs) ## Primary splits: ## lstat &lt; 5.145 to the right, improve=0.2690898, (0 missing) ## dis &lt; 2.0891 to the right, improve=0.2163813, (0 missing) ## rm &lt; 6.543 to the left, improve=0.2036454, (0 missing) ## age &lt; 89.45 to the left, improve=0.1796977, (0 missing) ## tax &lt; 548 to the left, improve=0.1751322, (0 missing) ## Surrogate splits: ## zn &lt; 92.5 to the left, agree=0.833, adj=0.091, (0 split) ## nox &lt; 0.4035 to the right, agree=0.833, adj=0.091, (0 split) ## indus &lt; 1.495 to the right, agree=0.825, adj=0.045, (0 split) ## dis &lt; 1.48495 to the right, agree=0.825, adj=0.045, (0 split) ## ## Node number 7: 31 observations, complexity param=0.02707678 ## mean=41.16129, MSE=52.78882 ## left son=14 (11 obs) right son=15 (20 obs) ## Primary splits: ## rm &lt; 7.437 to the left, improve=0.4423448, (0 missing) ## lstat &lt; 5.185 to the right, improve=0.3125696, (0 missing) ## ptratio &lt; 15.05 to the right, improve=0.1896089, (0 missing) ## black &lt; 392.715 to the right, improve=0.1133472, (0 missing) ## age &lt; 37.6 to the right, improve=0.0737298, (0 missing) ## Surrogate splits: ## lstat &lt; 4.635 to the right, agree=0.774, adj=0.364, (0 split) ## indus &lt; 2.32 to the left, agree=0.742, adj=0.273, (0 split) ## dis &lt; 5.9736 to the right, agree=0.710, adj=0.182, (0 split) ## black &lt; 390.095 to the right, agree=0.710, adj=0.182, (0 split) ## crim &lt; 0.10593 to the left, agree=0.677, adj=0.091, (0 split) ## ## Node number 10: 61 observations ## mean=17.38689, MSE=8.122779 ## ## Node number 11: 67 observations ## mean=20.91791, MSE=7.041172 ## ## Node number 12: 98 observations, complexity param=0.01202564 ## mean=24.58265, MSE=20.9745 ## left son=24 (64 obs) right son=25 (34 obs) ## Primary splits: ## rm &lt; 6.543 to the left, improve=0.1564077, (0 missing) ## black &lt; 364.385 to the right, improve=0.1331323, (0 missing) ## age &lt; 89.45 to the left, improve=0.1241124, (0 missing) ## tax &lt; 223.5 to the right, improve=0.1204819, (0 missing) ## dis &lt; 4.46815 to the right, improve=0.1048755, (0 missing) ## Surrogate splits: ## dis &lt; 3.6589 to the right, agree=0.704, adj=0.147, (0 split) ## rad &lt; 6.5 to the left, agree=0.704, adj=0.147, (0 split) ## age &lt; 68.9 to the left, agree=0.694, adj=0.118, (0 split) ## indus &lt; 1.605 to the right, agree=0.673, adj=0.059, (0 split) ## nox &lt; 0.4045 to the right, agree=0.673, adj=0.059, (0 split) ## ## Node number 13: 22 observations, complexity param=0.01031677 ## mean=32.44545, MSE=43.70884 ## left son=26 (15 obs) right son=27 (7 obs) ## Primary splits: ## tax &lt; 364 to the left, improve=0.2868266, (0 missing) ## lstat &lt; 3.855 to the right, improve=0.2413545, (0 missing) ## age &lt; 31.85 to the left, improve=0.1598075, (0 missing) ## dis &lt; 5.4085 to the right, improve=0.1258591, (0 missing) ## black &lt; 381.59 to the right, improve=0.1052855, (0 missing) ## Surrogate splits: ## crim &lt; 2.6956 to the left, agree=0.773, adj=0.286, (0 split) ## indus &lt; 14 to the left, agree=0.773, adj=0.286, (0 split) ## nox &lt; 0.5875 to the left, agree=0.773, adj=0.286, (0 split) ## age &lt; 89.65 to the left, agree=0.773, adj=0.286, (0 split) ## dis &lt; 2.3371 to the right, agree=0.773, adj=0.286, (0 split) ## ## Node number 14: 11 observations ## mean=34.64545, MSE=3.304298 ## ## Node number 15: 20 observations, complexity param=0.01057622 ## mean=44.745, MSE=43.81147 ## left son=30 (12 obs) right son=31 (8 obs) ## Primary splits: ## ptratio &lt; 15.4 to the right, improve=0.3226860, (0 missing) ## rad &lt; 6 to the right, improve=0.2170243, (0 missing) ## tax &lt; 270 to the right, improve=0.1545997, (0 missing) ## age &lt; 71.85 to the right, improve=0.1331209, (0 missing) ## zn &lt; 10 to the left, improve=0.1328727, (0 missing) ## Surrogate splits: ## zn &lt; 10 to the left, agree=0.80, adj=0.500, (0 split) ## nox &lt; 0.541 to the left, agree=0.80, adj=0.500, (0 split) ## age &lt; 86.7 to the left, agree=0.80, adj=0.500, (0 split) ## dis &lt; 2.5813 to the right, agree=0.80, adj=0.500, (0 split) ## crim &lt; 0.45114 to the left, agree=0.75, adj=0.375, (0 split) ## ## Node number 24: 64 observations, complexity param=0.01006729 ## mean=23.2625, MSE=21.96891 ## left son=48 (57 obs) right son=49 (7 obs) ## Primary splits: ## indus &lt; 14.48 to the left, improve=0.19142190, (0 missing) ## crim &lt; 0.841845 to the left, improve=0.17407590, (0 missing) ## black &lt; 374.635 to the right, improve=0.14590640, (0 missing) ## dis &lt; 2.6499 to the right, improve=0.13374910, (0 missing) ## age &lt; 79.85 to the left, improve=0.08856433, (0 missing) ## Surrogate splits: ## crim &lt; 1.163695 to the left, agree=0.984, adj=0.857, (0 split) ## nox &lt; 0.589 to the left, agree=0.984, adj=0.857, (0 split) ## age &lt; 84.35 to the left, agree=0.984, adj=0.857, (0 split) ## dis &lt; 2.28545 to the right, agree=0.969, adj=0.714, (0 split) ## black &lt; 361.635 to the right, agree=0.969, adj=0.714, (0 split) ## ## Node number 25: 34 observations ## mean=27.06765, MSE=9.646894 ## ## Node number 26: 15 observations ## mean=30.02667, MSE=14.56062 ## ## Node number 27: 7 observations ## mean=37.62857, MSE=66.76776 ## ## Node number 30: 12 observations ## mean=41.675, MSE=48.28521 ## ## Node number 31: 8 observations ## mean=49.35, MSE=1.7575 ## ## Node number 48: 57 observations ## mean=22.54386, MSE=10.87053 ## ## Node number 49: 7 observations ## mean=29.11429, MSE=73.89265 rpart.plot(boston.rpart) Rysunek 5.1: Drzewo regresyjne pełne Przycinamy drzewo… printcp(boston.rpart) ## ## Regression tree: ## rpart(formula = medv ~ ., data = boston.train) ## ## Variables actually used in tree construction: ## [1] crim indus lstat ptratio rm tax ## ## Root node error: 26734/337 = 79.33 ## ## n= 337 ## ## CP nsplit rel error xerror xstd ## 1 0.435061 0 1.00000 1.00375 0.104966 ## 2 0.211147 1 0.56494 0.68564 0.077321 ## 3 0.056418 2 0.35379 0.43932 0.059746 ## 4 0.041548 3 0.29737 0.37266 0.057166 ## 5 0.027077 4 0.25583 0.35203 0.055698 ## 6 0.014891 5 0.22875 0.32389 0.056819 ## 7 0.012026 6 0.21386 0.29226 0.053113 ## 8 0.010576 7 0.20183 0.28894 0.053182 ## 9 0.010317 8 0.19126 0.28384 0.051523 ## 10 0.010067 9 0.18094 0.28382 0.051521 ## 11 0.010000 10 0.17087 0.28152 0.051530 plotcp(boston.rpart) boston.rpart2 &lt;- prune(boston.rpart, cp = 0.012026) rpart.plot(boston.rpart2) Rysunek 5.2: Drzewo regresyjne przycięte Predykcja na podstawie drzewa na zbiorze testowym. boston.pred &lt;- predict(boston.rpart2, newdata = boston.test) rmse &lt;- function(pred, obs) sqrt(1/length(pred)*sum((pred-obs)^2)) rmse(boston.pred, boston.test$medv) ## [1] 4.825862 Teraz zbudujemy model metodą bagging. library(randomForest) boston.bag &lt;- randomForest(medv~., data = boston.train, mtry = ncol(boston.train)-1) boston.bag ## ## Call: ## randomForest(formula = medv ~ ., data = boston.train, mtry = ncol(boston.train) - 1) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 13 ## ## Mean of squared residuals: 13.06701 ## % Var explained: 83.53 Predykcja na podstawie modelu boston.pred2 &lt;- predict(boston.bag, newdata = boston.test) rmse(boston.pred2, boston.test$medv) ## [1] 3.039308 Zatem predykcja na podstawie modelu bagging jest nico lepsza niż z pojedynczego drzewa. Dodatkowo możemy ocenić ważność zmiennych użytych w budowie drzew. varImpPlot(boston.bag) Rysunek 5.3: Wykres ważności predyktorów importance(boston.bag) ## IncNodePurity ## crim 1200.11828 ## zn 24.17836 ## indus 262.33396 ## chas 22.27133 ## nox 417.32236 ## rm 9102.58339 ## age 416.48170 ## dis 1494.79734 ## rad 171.92103 ## tax 403.66309 ## ptratio 411.88528 ## black 331.58495 ## lstat 12137.38999 x$variable.importance ## lstat nox indus crim tax rm ## 15197.8587 8683.8225 8325.2431 8074.7200 6991.0756 6768.5423 ## age dis ptratio rad black zn ## 6538.5039 1305.3786 1193.2073 853.4309 323.8576 242.3521 W porównaniu do ważności zmiennych dla pojedynczego drzewa widać pewne różnice. 5.2 Lasy losowe Lasy losowe są uogólnieniem metody bagging, polegającą na losowaniu dla każdego drzewa wchodzącego w skład lasu \\(m\\) predyktorów spośród \\(p\\) dostępnych, a następnie budowaniu drzew z wykorzystaniem tylko tych predyktorów (Ho 1995). Dzięki temu za każdy razem drzewo jest budowane w oparciu o nowy zestaw cech (najczęściej przyjmujemy \\(m=\\sqrt{p}\\)). W przypadku modeli bagging za każdym razem najsilniejszy predyktor wchodził w skład zbioru uczącego, a co za tym idzie również uczestniczył w tworzeniu reguł podziału. Wówczas wiele drzew zawierało reguły stosujące dany atrybut, a wtedy predykcje otrzymywane za pomocą drzew były skorelowane. Dlatego nawet duża liczba prób bootstrapowych nie zapewniała poprawy precyzji. Implementacja tej metody znajduje się w pakiecie randomForest. Przykład 5.2 Kontynuując poprzedni przykład 5.1 możemy zbudować las losowy aby przekonać się czy nastąpi poprawa predykcji zmiennej wynikowej. boston.rf &lt;- randomForest(medv~., data = boston.train) boston.rf ## ## Call: ## randomForest(formula = medv ~ ., data = boston.train) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 4 ## ## Mean of squared residuals: 13.09902 ## % Var explained: 83.49 Porównanie MSE na próbach uczących pomiędzy lasem losowym i modelem bagging wypada nieco na korzyść bagging. boston.pred3 &lt;- predict(boston.rf, newdata = boston.test) rmse(boston.pred3, boston.test$medv) ## [1] 3.418302 Ważność zmiennych również się nieco różni. varImpPlot(boston.rf) 5.3 Boosting Rozważania na temat metody boosting zaczęły się od pytań postawionych w publikacji Kearns and Valiant (1989), czy da się na podstawie na podstawie zbioru słabych modeli stworzyć jeden dobry? Odpowiedzi pozytywnej na nie udzielili, najpierw Schapire (1990), a potem Breiman (1998). W metodzie boosting nie stosuje się prób bootstrapowych ale odpowiednio modyfikuje się drzewo wyjściowe w kolejnych krokach na tym samym zbiorze uczącym. Algorytm dla drzewa regresyjnego jest następujący: Ustal \\(\\hat{f}(x)=0\\) i \\(r_i=y_i\\) dla każdego \\(i\\) w zbiorze uczącym. Dla \\(b=1,2,\\ldots, B\\) powtarzaj: naucz drzewo \\(\\hat{f}^b\\) o \\(d\\) regułach podziału (czyli \\(d+1\\) liściach) na zbiorze \\((X_i, r_i)\\), zaktualizuj drzewo do nowej “skurczonej” wersji \\[\\begin{equation} \\hat{f}(x)\\leftarrow \\hat{f}(x)+\\lambda\\hat{h}^b(x), \\end{equation}\\] zaktualizuj reszty \\[\\begin{equation} r_i\\leftarrow r_i-\\lambda\\hat{f}^b(x_i). \\end{equation}\\] Wyznacz boosted model \\[\\begin{equation} \\hat{f}(x) = \\sum_{b=1}^B\\lambda\\hat{f}^b(x) \\end{equation}\\] Uczenie drzew klasyfikacyjnego metoda boosting przebiega w podobny sposób. Wynik uczenia drzew metodą boosting zależy od trzech parametrów: Liczby drzew \\(B\\). W przeciwieństwie do metody bagging i lasów losowych, zbyt duże \\(B\\) może doprowadzić do przeuczenia modelu. \\(B\\) ustala się najczęściej na podstawie walidacji krzyżowej. Parametru “kurczenia” (ang. shrinkage) \\(\\lambda\\). Kontroluje on szybkość uczenia się kolejnych drzew. Typowe wartości \\(\\lambda\\) to 0.01 lub 0.001. Bardzo małe \\(\\lambda\\) może wymagać dobrania większego \\(B\\), aby zapewnić dobrą jakość predykcyjną modelu. Liczby podziałów w drzewach \\(d\\), która decyduje o złożoności drzewa. Bywa, że nawet \\(d=1\\) daje dobre rezultaty, ponieważ model wówczas uczy się powoli. Implementację metody boosting można znaleźć w pakiecie gbm (Greenwell et al. 2019) Przykład 5.3 Metodę boosting zastosujemy do zadania predykcji ceny mieszkań na przedmieściach Bostonu. Dobór parametrów modelu będzie arbitralny, więc niekoniecznie model będzie najlepiej dopasowany. library(gbm) ## Loaded gbm 2.1.5 boston.boost &lt;- gbm(medv~., data = boston.train, distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 2, shrinkage = 0.01) boston.boost ## gbm(formula = medv ~ ., distribution = &quot;gaussian&quot;, data = boston.train, ## n.trees = 5000, interaction.depth = 2, shrinkage = 0.01) ## A gradient boosted model with gaussian loss function. ## 5000 iterations were performed. ## There were 13 predictors of which 13 had non-zero influence. summary(boston.boost) ## var rel.inf ## lstat lstat 37.72235740 ## rm rm 28.25340805 ## dis dis 9.04378958 ## crim crim 6.95484787 ## nox nox 3.97210067 ## black black 3.16250916 ## ptratio ptratio 3.03202030 ## age age 2.35790500 ## chas chas 1.97366108 ## tax tax 1.67544858 ## indus indus 1.22537648 ## rad rad 0.57329299 ## zn zn 0.05328283 Predykcja na podstawie metody boosting boston.pred4 &lt;- predict(boston.boost, newdata = boston.test, n.trees = 5000) rmse(boston.pred4, boston.test$medv) ## [1] 3.06509 \\(RMSE\\) jest w tym przypadku mniejsze niż w lasach losowych ale nieco większe niż w metodzie bagging. Wszystkie metody wzmacnianych drzew dają wyniki lepsze niż pojedyncze drzewa. Bibliografia "]
]
