<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>7 Klasyfikatory bayesowskie | Eksploracja danych</title>
  <meta name="description" content="Książka stanowi materiał źródłowy do przeprowadzenia przedmiotu Eksploracja Danych.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="7 Klasyfikatory bayesowskie | Eksploracja danych" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Książka stanowi materiał źródłowy do przeprowadzenia przedmiotu Eksploracja Danych." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Klasyfikatory bayesowskie | Eksploracja danych" />
  
  <meta name="twitter:description" content="Książka stanowi materiał źródłowy do przeprowadzenia przedmiotu Eksploracja Danych." />
  



<meta name="date" content="2019-03-22">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="klasyfikatory-liniowe.html">
<link rel="next" href="bibliografia.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
        P: '{\\mathrm{P}}',
        E: '{\\mathrm{E}}',
        Var: '{\\mathrm{Var}}',
        Cor: '{\\mathrm{Cor}}',
        Cov: '{\\mathrm{Cov}}',
        Tr: '{\\mathrm{Tr}}'
    },
}
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Eksploracja Danych</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Wstęp</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#o-ksiazce"><i class="fa fa-check"></i>O książce</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#zakres-przedmiotu"><i class="fa fa-check"></i>Zakres przedmiotu</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#zakres-technik-stosowanych-w-data-mining"><i class="fa fa-check"></i>Zakres technik stosowanych w data mining</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#etapy-eksploracji-danych"><i class="fa fa-check"></i>Etapy eksploracji danych</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="roz1.html"><a href="roz1.html"><i class="fa fa-check"></i><b>1</b> Import danych</a></li>
<li class="chapter" data-level="2" data-path="przygotowanie-danych.html"><a href="przygotowanie-danych.html"><i class="fa fa-check"></i><b>2</b> Przygotowanie danych</a><ul>
<li class="chapter" data-level="2.1" data-path="przygotowanie-danych.html"><a href="przygotowanie-danych.html#identyfikacja-brakow-danych"><i class="fa fa-check"></i><b>2.1</b> Identyfikacja braków danych</a></li>
<li class="chapter" data-level="2.2" data-path="przygotowanie-danych.html"><a href="przygotowanie-danych.html#zastepowanie-brakow-danych"><i class="fa fa-check"></i><b>2.2</b> Zastępowanie braków danych</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html"><i class="fa fa-check"></i><b>3</b> Podział metod data mining</a><ul>
<li class="chapter" data-level="3.1" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#rodzaje-wnioskowania"><i class="fa fa-check"></i><b>3.1</b> Rodzaje wnioskowania</a><ul>
<li class="chapter" data-level="3.1.1" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#dziedzina"><i class="fa fa-check"></i><b>3.1.1</b> Dziedzina</a></li>
<li class="chapter" data-level="3.1.2" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#obserwacja"><i class="fa fa-check"></i><b>3.1.2</b> Obserwacja</a></li>
<li class="chapter" data-level="3.1.3" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#atrybuty-obserwacji"><i class="fa fa-check"></i><b>3.1.3</b> Atrybuty obserwacji</a></li>
<li class="chapter" data-level="3.1.4" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#zbior-uczacy"><i class="fa fa-check"></i><b>3.1.4</b> Zbiór uczący</a></li>
<li class="chapter" data-level="3.1.5" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#zbior-testowy"><i class="fa fa-check"></i><b>3.1.5</b> Zbiór testowy</a></li>
<li class="chapter" data-level="3.1.6" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#model"><i class="fa fa-check"></i><b>3.1.6</b> Model</a></li>
<li class="chapter" data-level="3.1.7" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#jakosc-dopasowania-modelu"><i class="fa fa-check"></i><b>3.1.7</b> Jakość dopasowania modelu</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#modele-regresyjne"><i class="fa fa-check"></i><b>3.2</b> Modele regresyjne</a></li>
<li class="chapter" data-level="3.3" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#modele-klasyfikacyjne"><i class="fa fa-check"></i><b>3.3</b> Modele klasyfikacyjne</a></li>
<li class="chapter" data-level="3.4" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#modele-grupujace"><i class="fa fa-check"></i><b>3.4</b> Modele grupujące</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html"><i class="fa fa-check"></i><b>4</b> Drzewa decyzyjne</a><ul>
<li class="chapter" data-level="4.1" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#wezy-i-gaezie"><i class="fa fa-check"></i><b>4.1</b> Węzły i gałęzie</a></li>
<li class="chapter" data-level="4.2" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#rodzaje-regu-podziau"><i class="fa fa-check"></i><b>4.2</b> Rodzaje reguł podziału</a><ul>
<li class="chapter" data-level="4.2.1" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#podziay-dla-atrybutow-ze-skali-nominalnej"><i class="fa fa-check"></i><b>4.2.1</b> Podziały dla atrybutów ze skali nominalnej</a></li>
<li class="chapter" data-level="4.2.2" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#podziay-dla-atrybutow-ze-skali-ciagej"><i class="fa fa-check"></i><b>4.2.2</b> Podziały dla atrybutów ze skali ciągłej</a></li>
<li class="chapter" data-level="4.2.3" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#podziay-dla-atrybutow-ze-skali-porzadkowej"><i class="fa fa-check"></i><b>4.2.3</b> Podziały dla atrybutów ze skali porządkowej</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#algorytm-budowy-drzewa"><i class="fa fa-check"></i><b>4.3</b> Algorytm budowy drzewa</a></li>
<li class="chapter" data-level="4.4" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#kryteria-zatrzymania"><i class="fa fa-check"></i><b>4.4</b> Kryteria zatrzymania</a></li>
<li class="chapter" data-level="4.5" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#reguy-podziau"><i class="fa fa-check"></i><b>4.5</b> Reguły podziału</a></li>
<li class="chapter" data-level="4.6" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#przycinanie-drzewa-decyzyjnego"><i class="fa fa-check"></i><b>4.6</b> Przycinanie drzewa decyzyjnego</a><ul>
<li class="chapter" data-level="4.6.1" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#przycinanie-redukujace-bad"><i class="fa fa-check"></i><b>4.6.1</b> Przycinanie redukujące błąd</a></li>
<li class="chapter" data-level="4.6.2" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#przycinanie-minimalizujace-bad"><i class="fa fa-check"></i><b>4.6.2</b> Przycinanie minimalizujące błąd</a></li>
<li class="chapter" data-level="4.6.3" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#przycinanie-ze-wzgledu-na-wspoczynnik-zozonosci-drzewa"><i class="fa fa-check"></i><b>4.6.3</b> Przycinanie ze względu na współczynnik złożoności drzewa</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#obsuga-brakow-danych"><i class="fa fa-check"></i><b>4.7</b> Obsługa braków danych</a></li>
<li class="chapter" data-level="4.8" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#zalety-i-wady"><i class="fa fa-check"></i><b>4.8</b> Zalety i wady</a><ul>
<li class="chapter" data-level="4.8.1" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#zalety"><i class="fa fa-check"></i><b>4.8.1</b> Zalety</a></li>
<li class="chapter" data-level="4.8.2" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#wady"><i class="fa fa-check"></i><b>4.8.2</b> Wady</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#inne-algorytmy-budowy-drzew-decyzyjnych-implementowane-w-r"><i class="fa fa-check"></i><b>4.9</b> Inne algorytmy budowy drzew decyzyjnych implementowane w <strong>R</strong></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pochodne-drzew-decyzyjnych.html"><a href="pochodne-drzew-decyzyjnych.html"><i class="fa fa-check"></i><b>5</b> Pochodne drzew decyzyjnych</a><ul>
<li class="chapter" data-level="5.1" data-path="pochodne-drzew-decyzyjnych.html"><a href="pochodne-drzew-decyzyjnych.html#bagging"><i class="fa fa-check"></i><b>5.1</b> Bagging</a></li>
<li class="chapter" data-level="5.2" data-path="pochodne-drzew-decyzyjnych.html"><a href="pochodne-drzew-decyzyjnych.html#lasy-losowe"><i class="fa fa-check"></i><b>5.2</b> Lasy losowe</a></li>
<li class="chapter" data-level="5.3" data-path="pochodne-drzew-decyzyjnych.html"><a href="pochodne-drzew-decyzyjnych.html#boosting"><i class="fa fa-check"></i><b>5.3</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="klasyfikatory-liniowe.html"><a href="klasyfikatory-liniowe.html"><i class="fa fa-check"></i><b>6</b> Klasyfikatory liniowe</a><ul>
<li class="chapter" data-level="6.1" data-path="klasyfikatory-liniowe.html"><a href="klasyfikatory-liniowe.html#reprezentacja-progowa"><i class="fa fa-check"></i><b>6.1</b> Reprezentacja progowa</a></li>
<li class="chapter" data-level="6.2" data-path="klasyfikatory-liniowe.html"><a href="klasyfikatory-liniowe.html#reprezentacja-logitowa"><i class="fa fa-check"></i><b>6.2</b> Reprezentacja logitowa</a></li>
<li class="chapter" data-level="6.3" data-path="klasyfikatory-liniowe.html"><a href="klasyfikatory-liniowe.html#wady-klasyfikatorow-liniowych"><i class="fa fa-check"></i><b>6.3</b> Wady klasyfikatorów liniowych</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html"><i class="fa fa-check"></i><b>7</b> Klasyfikatory bayesowskie</a><ul>
<li class="chapter" data-level="7.1" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html#klasyfikator-maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.1</b> Klasyfikator maximum a posteriori (MAP)</a></li>
<li class="chapter" data-level="7.2" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html#klasyfikator-najwiekszej-warogodnosci-ml"><i class="fa fa-check"></i><b>7.2</b> Klasyfikator największej warogodności (ML)</a></li>
<li class="chapter" data-level="7.3" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html#naiwny-klasyfikator-bayesa-nb"><i class="fa fa-check"></i><b>7.3</b> Naiwny klasyfikator Bayesa (NB)</a></li>
<li class="chapter" data-level="7.4" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html#zalety-i-wady-1"><i class="fa fa-check"></i><b>7.4</b> Zalety i wady</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Eksploracja danych</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="klasyfikatory-bayesowskie" class="section level1">
<h1><span class="header-section-number">7</span> Klasyfikatory bayesowskie</h1>
<p>Całą gamę klasyfikatorów opartych na twierdzeniu Bayesa nazywać będziemy bayesowskimi.
<span class="math display">\[\begin{equation}\label{bayes}
        P(A|B)=\frac{P(A)P(B|A)}{P(B)},
\end{equation}\]</span>
gdzie <span class="math inline">\(P(B)&gt;0\)</span>.</p>
<p>Bayesowskie reguły podejmowania decyzji dały podstawy takich metod jak:</p>
<ul>
<li>liniowa analiza dyskryminacyjna;</li>
<li>kwadratowa analiza dyskryminacyjna;</li>
</ul>
<p>W ustaleniu klasyfikatora bayesowskiego będzie nam przyświecała cały czas ta sama reguła: <em>jeśli znam wartości cech charakteryzujących badane obiekty oraz klasy do których należą (w próbie uczącej), to na ich podstawie mogę wyznaczyć miary prawdopodobieństw a posteriori, które pomogą mi w ustaleniu klasy do której należy nowy testowy element.</em></p>
<p>W dalszej części będziemy przyjmowali następujące oznaczenia:</p>
<ul>
<li><span class="math inline">\(T\)</span> - zbiór danych uczących (treningowych),</li>
<li><span class="math inline">\(T^j\)</span> - zbiór danych uczących dla których przyjęliśmy decyzję o przynależności do <span class="math inline">\(j\)</span>-tej klasy,</li>
<li><span class="math inline">\(T^j_{a_i=v}\)</span> - zbiór danych uczących o wartości atrybutu <span class="math inline">\(a_i\)</span> równej <span class="math inline">\(v\)</span> i klasy <span class="math inline">\(j\)</span>-tej,</li>
<li><span class="math inline">\(\mathbb{H}\)</span> - przestrzeń hipotez,</li>
<li><span class="math inline">\(P(h|a_1=v_1, a_2=v_2,\ldots,a_p=v_p)\)</span> - prawdopodobieństwo a posteriori, że prawdziwa jest hipoteza <span class="math inline">\(h\in \mathbb{H}\)</span>, jeśli znamy atrybuty obiektu,</li>
<li><span class="math inline">\(P(h)\)</span> - prawdopodobieństwo a priori zajścia hipotezy <span class="math inline">\(h\in \mathbb{H}\)</span>,</li>
<li><span class="math inline">\(c\)</span> - prawdziwy stan obiektu.</li>
</ul>
<div id="klasyfikator-maximum-a-posteriori-map" class="section level2">
<h2><span class="header-section-number">7.1</span> Klasyfikator maximum a posteriori (MAP)</h2>
<p>Na podstawie wiedzy o atrybutach obiektu <span class="math inline">\(x\)</span> podejmujemy decyzję o klasyfikacji tego obiektu zgodnie z hipotezą <span class="math inline">\(h_{MAP}\in \mathbb{H}\)</span>, która przyjmuje postać
<span class="math display">\[\begin{align}\label{MAP}
        h_{MAP}=&amp;\operatorname{arg}\max_{h\in \mathbb{H}}P(h|a_1=v_1, a_2=v_2,\ldots,a_p=v_p)\\
            =&amp; \operatorname{arg}\max_{h\in \mathbb{H}}P(a_1=v_1, a_2=v_2,\ldots,a_p=v_p|h)\cdot P(h),
\end{align}\]</span>
gdzie ostatnia równość wynika z twierdzenia Bayesa oraz faktu, że dla konkretnego obiektu <span class="math inline">\(x\)</span> wielkości atrybutów nie zależą od postawionej hipotezy.</p>
</div>
<div id="klasyfikator-najwiekszej-warogodnosci-ml" class="section level2">
<h2><span class="header-section-number">7.2</span> Klasyfikator największej warogodności (ML)</h2>
<p>Na podstawie wiedzy o atrybutach obiektu <span class="math inline">\(x\)</span> podejmujemy decyzję o klasyfikacji tego obiektu zgodnie z hipotezą <span class="math inline">\(h_{ML}\in \mathbb{H}\)</span>, która przyjmuje postać
<span class="math display">\[\begin{equation}\label{ML}
        h_{ML}=\operatorname{arg}\max_{h\in \mathbb{H}}P(a_1=v_1, a_2=v_2,\ldots,a_p=v_p|h).
\end{equation}\]</span></p>

<div class="remark">
 <span class="remark"><em>Uwaga. </em></span> Obie wspomniane metody wymagają znajomości prawdopodobieństwa <span class="math inline">\(P(a_1=v_1,a_2=v_2,\ldots,a_p=v_p|h)\)</span>, ale różnią się podejściem do wiedzy o prawdopodobieństwach a priori. W metodzie MAP brana pod uwagę jest wiedza o prawdopodobieństwie przynależności do poszczególnych klas, a w ML nie. Dla klasyfikacji, w których prawdopodobieństwa przynależności do klas są takie same, klasyfikatory MAP i ML są równoważne.
</div>

</div>
<div id="naiwny-klasyfikator-bayesa-nb" class="section level2">
<h2><span class="header-section-number">7.3</span> Naiwny klasyfikator Bayesa (NB)<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a></h2>
<p>Największy problem w wyznaczeniu klasyfikatorów MAP i ML stanowi wyznaczenie rozkładu łącznego <span class="math inline">\(P(a_1=v_1, a_2=v_2,\ldots,a_p=v_p|h)\)</span>. W naiwnym klasyfikatorze Bayesa zakłada się niezależność warunkową poszczególnych atrybutów względem klasy do której ma należeń wg hipotezy obiekt. Założenie to często nie jest spełnione i stąd nazwa przymiotnik “naiwny”.</p>
<p>Definicja naiwnego klasyfikatora bayesowskiego różni się od klasyfikatora MAP tylko podejściem do prawdopodobieństwa a posteriori.
<span class="math display">\[\begin{equation}\label{naiwny_bayes}
        h_{NB}=\operatorname{arg}\max_{h_j\in \mathbb{H}}P(h_j)\prod_{i=1}^{p}P(a_i=v_i|h_j),
\end{equation}\]</span>
gdzie <span class="math inline">\(h_j\)</span> oznacza hipotezę (decyzję), że badany obiekt należy do <span class="math inline">\(j\)</span>-tej klasy.</p>
<p>Oczywiście zarówno prawdopodobieństwo a priori jak i a posteriori są wyznaczane na podstawie próby, i tak prawdopodobieństwo a priori wynosi
<span class="math display">\[\begin{equation}\label{apriori}
        P(h_j)=P_T(h_j)=\frac{|T^j|}{|T|}, 
\end{equation}\]</span>
gdzie <span class="math inline">\(|A|\)</span> oznacza moc zbioru <span class="math inline">\(A\)</span>.</p>
<p>Natomiast prawdopodobieństwo a posteriori dla <span class="math inline">\(i\)</span>-tego atrybutu wynosi
<span class="math display">\[\begin{equation}\label{aposteriori}
        P(a_i=v_i|h_j)=P_{T^j}(a_i=v_i)=\frac{|T^j_{a_i=v_i}|}{|T^j|}.
\end{equation}\]</span>
Na mocy powyższego możemy zauważyć, że jeżeli założenie o warunkowej niezależności jest spełnione, to klasyfikatory NB i MAP są równoważne.</p>
<p>Chcąc przypisać klasę nowemu obiektowi powstaje problem praktyczny, polegający na tym, że dla pewnych konfiguracji atrybutów nie ma odpowiedników w nauczonym modelu. Powodem takiego stanu rzeczy jest fakt, że takie kombinacje nie wystąpiły w próbie uczącej.</p>
<p>Istnieją dwa sposoby predykcji w takiej sytuacji:</p>
<ol style="list-style-type: decimal">
<li><span class="math display">\[\begin{equation}\label{pred1}
         P(a_i=v_i|h_j)=
         \begin{cases}
             \frac{|T^j_{a_i=v_i}|}{|T^j|}, &amp; T^j_{a_i=v_i}\neq \emptyset\\
             \epsilon, &amp; \text{w przeciwnym przypadku.}
         \end{cases}
 \end{equation}\]</span>
W tym przypadku przyjmuje się, że <span class="math inline">\(\epsilon \ll 1/|T_j|\)</span>.</li>
<li>Drugi sposób wykorzystuje estymację z poprawką
<span class="math display">\[\begin{equation}\label{pred2}
     P(a_i=v_i|h_j)=\frac{|T^j_{a_i=v_i}|+mp}{|T^j|+mp},
\end{equation}\]</span>
gdzie <span class="math inline">\(p\)</span> oznacza prawdopodobieństwo a priori przyjęcia przez atrybut <span class="math inline">\(a\)</span> wartości <span class="math inline">\(v\)</span> (najczęściej <span class="math inline">\(p=1/|A|\)</span>, <span class="math inline">\(A\)</span> - zbiór wszystkich możliwych wartości atrybutu <span class="math inline">\(a\)</span>), <span class="math inline">\(m\)</span> - waga (najczęściej <span class="math inline">\(m=|A|\)</span>).</li>
</ol>
<p>W przypadku gdy atrybuty są mierzone na skali ciągłej najczęściej stosuje się dyskretyzację ich do zmiennych ze skali przedziałowej. Inna metoda stosowana w przypadku ciągłych atrybutów, to użycie gęstości <span class="math inline">\(g_i^j\)</span> o rozkładzie normalnym w miejsce <span class="math inline">\(P(a_i=v_i|h_j)\)</span>. Przy czym do obliczenia parametrów rozkładu stosujemy wzory
<span class="math display">\[\begin{equation}\label{sred}
        m_i^j=\frac{1}{|T^j|}\sum_{x\in T^j}a_i(x),
\end{equation}\]</span>
oraz
<span class="math display">\[\begin{equation}\label{odch}
        (s_i^j)^2=\frac{1}{|T^j|-1}\sum_{x\in T^j}(a_i(x)-m_i^j)^2.
\end{equation}\]</span></p>
<p>Obsługa braków danych przez naiwny klasyfikator Bayesa jest dość prosta i opiera się na liczeniu prawdopodobieństw a posteriori wyłącznie dla obiektów, których wartości atrybutów są znane. Dlatego prawdopodobieństwa warunkowe liczy się wg wzoru
<span class="math display">\[\begin{equation}\label{pr_war}
        P(a_i=v_i|h_j)=\frac{|T^j_{a_i=v_i}|}{|T^j|-|T^j_{a_i=NA}|}.
\end{equation}\]</span>
Jeśli brakujące dane nie niosą w sobie istotnych informacji dotyczących klasyfikacji obiektów, to naiwny klasyfikator Bayesa będzie działał poprawnie.</p>
<p>Naiwny klasyfikator Bayesa jest implementowany w pakietach <strong>e1071</strong> <span class="citation">(Meyer et al. <a href="#ref-R-e1071">2019</a>)</span> i <strong>klaR</strong> <span class="citation">(Weihs et al. <a href="#ref-R-klaR">2005</a>)</span>.</p>

<div class="example">
<span id="exm:unnamed-chunk-46" class="example"><strong>Przykład 7.1  </strong></span>Przeprowadzimy klasyfikację dla zbioru <code>Titanic</code>. W przypadku funkcji z pakietu <code>e1071</code> nie potrzeba zamieniać tabeli na przypadki. W pakiecie <code>klaR</code> istnieje inna funkcja budująca klasyfikator Bayesa <code>NaiveBayes</code>, ale w tym przypadku jeśli zbiór jest w formie tabeli, to należy go zamienić na ramkę danych z oddzielnymi przypadkami.
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
Titanic</code></pre>
<pre><code>## , , Age = Child, Survived = No
## 
##       Sex
## Class  Male Female
##   1st     0      0
##   2nd     0      0
##   3rd    35     17
##   Crew    0      0
## 
## , , Age = Adult, Survived = No
## 
##       Sex
## Class  Male Female
##   1st   118      4
##   2nd   154     13
##   3rd   387     89
##   Crew  670      3
## 
## , , Age = Child, Survived = Yes
## 
##       Sex
## Class  Male Female
##   1st     5      1
##   2nd    11     13
##   3rd    13     14
##   Crew    0      0
## 
## , , Age = Adult, Survived = Yes
## 
##       Sex
## Class  Male Female
##   1st    57    140
##   2nd    14     80
##   3rd    75     76
##   Crew  192     20</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">nb &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(Survived <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Titanic)
nb<span class="op">$</span>apriori</code></pre>
<pre><code>## Survived
##   No  Yes 
## 1490  711</code></pre>
<p>Poniższe tabele zawierają warunkowe prawdopodobieństwa przynależności do poszczólnych klas.</p>
<pre class="sourceCode r"><code class="sourceCode r">nb<span class="op">$</span>tables</code></pre>
<pre><code>## $Class
##         Class
## Survived        1st        2nd        3rd       Crew
##      No  0.08187919 0.11208054 0.35436242 0.45167785
##      Yes 0.28551336 0.16596343 0.25035162 0.29817159
## 
## $Sex
##         Sex
## Survived       Male     Female
##      No  0.91543624 0.08456376
##      Yes 0.51617440 0.48382560
## 
## $Age
##         Age
## Survived      Child      Adult
##      No  0.03489933 0.96510067
##      Yes 0.08016878 0.91983122</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">dane &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(Titanic)
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(nb, dane)
pred</code></pre>
<pre><code>##  [1] Yes No  No  No  Yes Yes Yes Yes No  No  No  No  Yes Yes Yes Yes Yes
## [18] No  No  No  Yes Yes Yes Yes No  No  No  No  Yes Yes Yes Yes
## Levels: No Yes</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tab &lt;-<span class="st"> </span><span class="kw">table</span>(pred, dane<span class="op">$</span>Survived)
tab</code></pre>
<pre><code>##      
## pred  No Yes
##   No   7   7
##   Yes  9   9</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(<span class="kw">diag</span>(<span class="kw">prop.table</span>(tab)))</code></pre>
<pre><code>## [1] 0.5</code></pre>
<p>Naiwny klasyfikator spisał się bardzo słabo, ponieważ klasyfikacja na poziomie 0.5 jest taka jak przy rzucie monetą.</p>

<div class="example">
<span id="exm:unnamed-chunk-50" class="example"><strong>Przykład 7.2  </strong></span>Przeprowadzimy klasyfikację gatunków irysów na podstawie szerokości i długości kielicha i płatka.
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(klaR)
<span class="kw">set.seed</span>(<span class="dv">2019</span>)
uczaca &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(iris), <span class="dv">2</span><span class="op">*</span><span class="kw">nrow</span>(iris)<span class="op">/</span><span class="dv">3</span>)
pr.ucz &lt;-<span class="st"> </span>iris[uczaca,]
pr.test &lt;-<span class="st"> </span>iris[<span class="op">-</span>uczaca,]
nb2 &lt;-<span class="st"> </span><span class="kw">NaiveBayes</span>(Species<span class="op">~</span>., <span class="dt">data =</span> pr.ucz)
nb2<span class="op">$</span>apriori</code></pre>
<pre><code>## grouping
##     setosa versicolor  virginica 
##       0.36       0.31       0.33</code></pre>
<p>Prawdopodobieństwa a priori zostały oszacowane na podstawie próby uczącej. Poniższe tabele zawierają średnie i odchylenia standardowe zmiennych w poszczególnych klasach.</p>
<pre class="sourceCode r"><code class="sourceCode r">nb2<span class="op">$</span>tables</code></pre>
<pre><code>## $Sepal.Length
##                [,1]      [,2]
## setosa     4.994444 0.3438807
## versicolor 5.977419 0.5613731
## virginica  6.603030 0.7359029
## 
## $Sepal.Width
##                [,1]      [,2]
## setosa     3.436111 0.3586903
## versicolor 2.838710 0.2996414
## virginica  2.942424 0.3211603
## 
## $Petal.Length
##                [,1]      [,2]
## setosa     1.472222 0.1782632
## versicolor 4.316129 0.4576001
## virginica  5.606061 0.6269666
## 
## $Petal.Width
##                 [,1]       [,2]
## setosa     0.2444444 0.09394358
## versicolor 1.3354839 0.18537959
## virginica  2.0000000 0.25860201</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(nb2, <span class="dt">newdata =</span> pr.test)
tab &lt;-<span class="st"> </span><span class="kw">table</span>(pred<span class="op">$</span>class, pr.test<span class="op">$</span>Species)
tab</code></pre>
<pre><code>##             
##              setosa versicolor virginica
##   setosa         14          0         0
##   versicolor      0         18         1
##   virginica       0          1        16</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(<span class="kw">diag</span>(<span class="kw">prop.table</span>(tab)))</code></pre>
<pre><code>## [1] 0.96</code></pre>
<p>Klasyfikacja na podstawie modelu jest badzo do dobra (96%).</p>
</div>
<div id="zalety-i-wady-1" class="section level2">
<h2><span class="header-section-number">7.4</span> Zalety i wady</h2>
<ul>
<li>Zalety:
<ul>
<li>prostota konstrukcji i prosty algorytm;</li>
<li>jeśli jest spełnione założenie warunkowej niezależności, to ten klasyfikator działa szybciej i czasem lepiej niż inne metody klasyfikacji;</li>
<li>nie potrzebuje dużych zbiorów danych do estymacji parametrów;</li>
</ul></li>
<li>Wady:
<ul>
<li>często nie spełnione założenie o warunkowej niezależności powoduje obciążenie wyników;</li>
<li>brak możliwości wprowadzania interakcji efektów kilku zmiennych;</li>
<li>potrzebuje założenia normalności warunkowych gęstości w przypadku ciągłych atrybutów;</li>
<li>często istnieją lepsze klasyfikatory.</li>
</ul></li>
</ul>

</div>
</div>
<h3>Bibliografia</h3>
<div id="refs" class="references">
<div id="ref-R-e1071">
<p>Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2019. <em>E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), Tu Wien</em>. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>.</p>
</div>
<div id="ref-R-klaR">
<p>Weihs, Claus, Uwe Ligges, Karsten Luebke, and Nils Raabe. 2005. “KlaR Analyzing German Business Cycles.” In <em>Data Analysis and Decision Support</em>, edited by D. Baier, R. Decker, and L. Schmidt-Thieme, 335–43. Berlin: Springer-Verlag.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="18">
<li id="fn18"><p>ang. <em>Naive Bayes Classifier</em><a href="klasyfikatory-bayesowskie.html#fnref18" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="klasyfikatory-liniowe.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografia.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["EksploracjaDanych.pdf", "EksploracjaDanych.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
