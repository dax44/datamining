\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Eksploracja danych},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Eksploracja danych}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{true}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-03-12}

%\usepackage[cp1250]{inputenc}
%\usepackage{amsmath}
\usepackage[MeX]{polski}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{array}
\usepackage{ragged2e}
\usepackage{caption}
\usepackage{tikzsymbols}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage{tabu}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{fontspec}


\newcommand{\zb}[1]{\buildrel #1 \over \longrightarrow}
\newcommand{\PP}{\mathrm{P}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cor}{\operatorname{Cor}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\row}[1]{\buildrel \text{#1} \over =}
\newcommand{\pp}{\text{p.p.}}
\newcommand{\wtt}{wtedy i tylko wtedy, gdy }
\renewcommand{\arraystretch}{1.4}

\newcommand{\btwocol}{\begin{multicols}{2}}
\newcommand{\etwocol}{\end{multicols}}

%definicje twierdze
\theoremstyle{plain}
\newtheorem{tw}{Twierdzenie}[section]
\newtheorem{lm}[tw]{Lemat}
\newtheorem{uw}[tw]{Uwaga}
\newtheorem{wn}[tw]{Wniosek}

\theoremstyle{definition}
\newtheorem{df}[tw]{Definicja}
\newtheorem{prz}[tw]{Przykad}
\let\proof\uuundefined
\let\endproof\uuundefined
\newenvironment{proof}[1][Dow贸d]{\textbf{#1} }{\ \rule{0.5em}{0.5em}}

\usepackage{makeidx}
\makeindex

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{wstep}{%
\chapter*{Wstp}\label{wstep}}
\addcontentsline{toc}{chapter}{Wstp}

\hypertarget{o-ksiazce}{%
\section*{O ksi偶ce}\label{o-ksiazce}}
\addcontentsline{toc}{section}{O ksi偶ce}

Niniejsza ksi偶ka powstaa na bazie dowiadcze autora, a g贸wnym jej celem jest przybli偶enie czytelnikowi podstaw z dziedziny \emph{Data mining} studentom kierunku \emph{Matematyka} Politechniki Lubelskiej. Bdzie czy w sobie zar贸wno treci teoretyczne zwizane z przedstawianymi etapami eksploracji danych i budow modeli, jak i praktyczne wskaz贸wki dotczce budowy modeli w rodowisku \textbf{R} \citep{R-base}. Podane zostan r贸wnie偶 wskaz贸wki, jak raportowa wyniki analiz i jak dokona waciwych ilustracji wynik贸w. Bardzo u偶yteczny w napisaniu ksi偶ki byy pakiety programu R: \textbf{bookdown} \citep{R-bookdown}, \textbf{knitr} \citep{R-knitr} oraz pakiet \textbf{rmarkdown} \citep{R-rmarkdown}.

\hypertarget{zakres-przedmiotu}{%
\section*{Zakres przedmiotu}\label{zakres-przedmiotu}}
\addcontentsline{toc}{section}{Zakres przedmiotu}

Przedmiot \emph{Eksploracja danych} bdzie obejmowa swoim zakresem eksploracj i wizualizacj danych oraz uczenie maszynowe. Eksploracja danych ma na celu pozyskiwanie i systematyzacj wiedzy pochodzcej z danych. Odbywa si ona g贸wnie przy u偶yciu technik statystycznych, rachunku prawdopodobiestwa i metod z zakresu baz danych. Natomiast uczenie maszynowe, to ga藕 nauki (obejmuje nie tylko statystyk, cho to na niej si g贸wnie opiera) dotyczcej budowy modeli zdolnych do rozpoznawania wzorc贸w, przewidywania wartoci i klasyfikacji obiekt贸w. Data mining to szybko rosnaca grupa metod analizy danych rozwijana nie tylko przez statystyk贸w ale r贸wnie偶 przez biolog贸w, genetyk贸w, cybernetyk贸w, informatyk贸w, ekonomist贸w, osoby pracujace nad rozpoznawaniem obraz贸w i wiele innych grup zawodowych. W dzisiejszych czasch trudno sobie wyobrazi 偶ycie bez sztucznej inteligencji. Towarzyszy ona nam w codziennym, 偶yciu kiedy korzystamy z telefon贸w kom贸rkowych, wyszukiwarek internetowych, robot贸w sprztajcych, automatycznych samochod贸w, nawigacji czy gier komputerowych. Lista ta jest niepena i stale si wydu偶a.

href=``\url{https://twitter.com/i/status/1091069356367200256}''\textgreater{}January 31, 2019

\hypertarget{zakres-technik-stosowanych-w-data-mining}{%
\section*{Zakres technik stosowanych w data mining}\label{zakres-technik-stosowanych-w-data-mining}}
\addcontentsline{toc}{section}{Zakres technik stosowanych w data mining}

\begin{itemize}
\tightlist
\item
  statystyka opisowa
\item
  wielowymiarowa analiza danych
\item
  analiza szereg贸w czasowych
\item
  analiza danych przestrzennych
\item
  reguy asocjacji
\item
  uczenie maszynowe\footnote{ang. \emph{machine learning}}, w tym:

  \begin{itemize}
  \tightlist
  \item
    klasyfikacja
  \item
    predykcja
  \item
    analiza skupie
  \item
    \emph{text mining}
  \end{itemize}
\item
  i wiele innych
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/cluster1.jpg}
\caption{\label{fig:cluster1}Przykad nienadzorowanego uczenia maszynowego.~ \emph{殴r贸do:}\url{https://analyticstraining.com/cluster-analysis-for-business/}}
\end{figure}

href=``\url{https://twitter.com/i/status/1097199751072690176}''\textgreater{}Ferbruary 17, 2019

\hypertarget{etapy-eksploracji-danych}{%
\section*{Etapy eksploracji danych}\label{etapy-eksploracji-danych}}
\addcontentsline{toc}{section}{Etapy eksploracji danych}

\begin{figure}
\centering
\includegraphics{images/dm_stages.jpg}
\caption{\label{fig:unnamed-chunk-2}Etapy eksploracji danych \citep{KAVAKIOTIS2017}}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Czyszczenie danych - polega na usuwaniu brak贸w danych, usuwaniu staych zmiennych, imputacji brak贸w danych oraz przygotowaniu danych do dalszych analiz.
\item
  Integracja danych - czenie danych pochodzcych z r贸偶nych 藕r贸de.
\item
  Selekcja danych - wyb贸r z bazy tych danych, kt贸re s potrzebne do dalszych analiz.
\item
  Transformacja danych - przeksztacenie i konsolidacja danych do postaci przydatnej do eksploracji.
\item
  Eksploracja danych - zastosowanie technik wymienionych wczeniej w celu odnalezienia wzorc贸w\footnote{ang. \emph{patterns}} i zale偶noci.
\item
  Ewaluacja modeli - ocena poprawnoci modeli oraz wzorc贸w z nich uzyskanych.
\item
  Wizualizacja wynik贸w - graficzne przedstawienie odkrytych wzorc贸w.
\item
  Wdra偶anie modeli - zastosowanie wyznaczonych wzorc贸w.
\end{enumerate}

\hypertarget{roz1}{%
\chapter{Import danych}\label{roz1}}

rodowisko \textbf{R} pozwala na import i export plik贸w o r贸偶nych rozszerzeniach (\texttt{txt,\ csv,\ xls,\ xlsx,\ sav,\ xpt,\ dta}, itd.)\footnote{p贸ki co nie jest mi znana funkcja pozwalajca na import plik贸w programu Statistica}. W tym celu czasami trzeba zainstalowa pakiety rozszerzajce podstawowe mo偶liwoci R-a. Najnowsza\footnote{na dzie 19.02.2019} wersja programu \href{https://www.rstudio.com}{RStudio} (v. 1.1.463)\footnote{istniej rownie偶 nowsze wersje deweloperskie} pozwala na wczytanie danych z popularnych 藕r贸de za pomoc GUI.

\begin{figure}
\centering
\includegraphics{images/import.JPG}
\caption{\label{fig:import1}Narzdzie do importu plik贸w programu RStudio}
\end{figure}

Jeli dane s zapisane w trybie tekstowym (np. \texttt{txt}, \texttt{csv}), to wczytujemy je w nastpujcy spos贸b

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane1 <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"data/dane1.txt"}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ T)}
\KeywordTok{head}\NormalTok{(dane1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane2 <-}\StringTok{ }\KeywordTok{read.csv2}\NormalTok{(}\StringTok{"data/dane1.csv"}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ T)}
\KeywordTok{head}\NormalTok{(dane2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# funkcja pakietu readr wczytuje plik jako ramk danych w formacie tibble}
\CommentTok{# pakiet readr jest czsi wikszego pakietu tidyverse, }
\CommentTok{# kt贸ry zosta wczytany wczsniej}
\NormalTok{dane3 <-}\StringTok{ }\KeywordTok{read_csv2}\NormalTok{(}\StringTok{"data/dane1.csv"}\NormalTok{)}
\NormalTok{dane3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 150 x 5
##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
##           <dbl>       <dbl>        <dbl>       <dbl> <chr>  
##  1          5.1         3.5          1.4         0.2 setosa 
##  2          4.9         3            1.4         0.2 setosa 
##  3          4.7         3.2          1.3         0.2 setosa 
##  4          4.6         3.1          1.5         0.2 setosa 
##  5          5           3.6          1.4         0.2 setosa 
##  6          5.4         3.9          1.7         0.4 setosa 
##  7          4.6         3.4          1.4         0.3 setosa 
##  8          5           3.4          1.5         0.2 setosa 
##  9          4.4         2.9          1.4         0.2 setosa 
## 10          4.9         3.1          1.5         0.1 setosa 
## # ... with 140 more rows
\end{verbatim}

Jeli dane s przechowywane w pliku Excel (np. \texttt{xlsx}), to importujemy je za pomoc funkcji \texttt{read\_excel} pakietu \texttt{readxl}. Domylnie jest wczytywany arkusz pierwszy ale jeli zachodzi taka potrzeba, to mo偶na ustali, kt贸ry arkusz pliku Excel ma by wczytany za pomoc paramteru \texttt{sheet}, np. \texttt{sheet=3}, co oznacza, 偶e zostanie wczytany trzeci arkusz pliku.

\begin{figure}
\centering
\includegraphics{images/excel.jpg}
\caption{\label{fig:excel}Fragment pliku Excel}
\end{figure}

Poniewa偶 w pliku \texttt{dane1.xlsx} braki danych zostay zakodowane znakami \texttt{BD} oraz \texttt{-}, to nale偶y ten fakt przekaza funkcji, aby poprawnie wczyta braki danych. W przeciwnym przypadku zmienne zawierajce braki tak kodowane, bd wczytane jako zmienne znakowe.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readxl)}
\NormalTok{dane4 <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(}\StringTok{"data/dane1.xlsx"}\NormalTok{, }\DataTypeTok{na =} \KeywordTok{c}\NormalTok{(}\StringTok{"BD"}\NormalTok{, }\StringTok{"-"}\NormalTok{))}
\NormalTok{dane4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 150 x 5
##    `Dugo kielic~ `Szeroko kiel~ `Dugo patka` `Szeroko pat~
##               <dbl>            <dbl>            <dbl>            <dbl>
##  1              5.1              3.5              1.4              0.2
##  2              4.9              3                1.4              0.2
##  3              4.7              3.2              1.3              0.2
##  4              4.6              3.1              1.5              0.2
##  5              5                3.6              1.4              0.2
##  6              5.4              3.9              1.7              0.4
##  7             NA               NA                1.4              0.3
##  8              5                3.4              1.5              0.2
##  9              4.4              2.9              1.4              0.2
## 10              4.9              3.1              1.5              0.1
## # ... with 140 more rows, and 1 more variable: Gatunki <chr>
\end{verbatim}

Istniej oczywicie jeszcze wiele innych fomat贸w danych, charakterystycznych dla program贸w, w kt贸rych s traktowane jako domylne.\footnote{do ich wczytywania stosujemy funkcje pakietu \texttt{foreign}} W szczeg贸lny spos贸b nale偶y zwr贸ci uwag na pliki o rozszerzeniu \texttt{RData} lub \texttt{rda}\footnote{oznaczaj to samo} oraz pliki \texttt{rds}. Pliki \texttt{rda} su偶 do przechowywania obiekt贸w programu \texttt{R}. Mog to by pliki danych ale r贸wnie偶 obiekty graficzne (typu wyniki funkcji \texttt{ggplot}), modele (np. wynik funkcji \texttt{lm()}), zdefiniowane funkcje i wszystkie inne obiekty, kt贸re da si zapisa w rodowisku \texttt{R}. Ponadto pliki \texttt{rda} pozawalaj na zapisanie wilu obiekt贸w w jednym pliku. Pliki o rozszerzeniu \texttt{rds} maj podobn funkcj z tym, 偶e pozwalaj na przechowywanie tylko jednego obiektu.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# wszystkie wczytane wczeniej pliki zapisuje w jednym pliku}
\KeywordTok{save}\NormalTok{(dane1, dane2, dane3, dane4, }\DataTypeTok{file =} \StringTok{"data/dane.rda"}\NormalTok{)}
\CommentTok{# plik rda zosta zapisany}
\KeywordTok{list.files}\NormalTok{(}\DataTypeTok{path =} \StringTok{"data/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "algae.csv"    "Analysis.txt" "dane.rda"     "dane1.csv"   
## [5] "dane1.txt"    "dane1.xlsx"   "dane4.rds"    "dane4.sav"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# usuwam dane ze rodowiska R}
\KeywordTok{rm}\NormalTok{(dane1, dane2, dane3, dane4)}
\CommentTok{# sprawdzam co jest wczytane do R}
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## character(0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# wczytuj plik rda}
\KeywordTok{load}\NormalTok{(}\StringTok{"data/dane.rda"}\NormalTok{)}
\CommentTok{# jeszcze raz sprawdzam co jest wczytane do R}
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "dane1" "dane2" "dane3" "dane4"
\end{verbatim}

Zapisujc obiekty jako oddzielne pliki, mo偶na przy wczytywaniu nadawa im nazwy.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rm}\NormalTok{(dane1, dane2, dane3)}
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "dane4"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{saveRDS}\NormalTok{(dane4, }\DataTypeTok{file =} \StringTok{"data/dane4.rds"}\NormalTok{)}
\NormalTok{nowe_dane <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\StringTok{"data/dane4.rds"}\NormalTok{)}
\NormalTok{nowe_dane}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 150 x 5
##    `Dugo kielic~ `Szeroko kiel~ `Dugo patka` `Szeroko pat~
##               <dbl>            <dbl>            <dbl>            <dbl>
##  1              5.1              3.5              1.4              0.2
##  2              4.9              3                1.4              0.2
##  3              4.7              3.2              1.3              0.2
##  4              4.6              3.1              1.5              0.2
##  5              5                3.6              1.4              0.2
##  6              5.4              3.9              1.7              0.4
##  7             NA               NA                1.4              0.3
##  8              5                3.4              1.5              0.2
##  9              4.4              2.9              1.4              0.2
## 10              4.9              3.1              1.5              0.1
## # ... with 140 more rows, and 1 more variable: Gatunki <chr>
\end{verbatim}

Opr贸cz wielu zalet takiego sposobu importu i eksportu danych jest jedna powa偶na wada, pliki te mo偶na odczyta jedynie za pomoc \texttt{R}. Osobicie polecam stosowa do importu i eksportu danych plik贸w w takich formatach, kt贸re mog przeczyta wszyscy. Jak dotd wida do importu r贸偶nych format贸w danych potrzebujemy r贸偶nych funkcji, czasami nawet z r贸偶nych pakiet贸w. Istnieje rozwizanie tego poroblemu 

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rio)}
\NormalTok{dane1 <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"data/dane1.txt"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(dane1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane2 <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"data/dane1.csv"}\NormalTok{, }\DataTypeTok{dec =} \StringTok{","}\NormalTok{)}
\CommentTok{# dane1.csv miay , jako znak rozdzielajcy cech i mantys liczb}
\CommentTok{# dlatego wczamy parametr dec}
\KeywordTok{head}\NormalTok{(dane2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane3 <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"data/dane1.xlsx"}\NormalTok{, }\DataTypeTok{na=}\KeywordTok{c}\NormalTok{(}\StringTok{"BD"}\NormalTok{,}\StringTok{"-"}\NormalTok{))}
\KeywordTok{head}\NormalTok{(dane3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Dugo kielicha Szeroko kielicha Dugo patka Szeroko patka
## 1              5.1                3.5            1.4              0.2
## 2              4.9                3.0            1.4              0.2
## 3              4.7                3.2            1.3              0.2
## 4              4.6                3.1            1.5              0.2
## 5              5.0                3.6            1.4              0.2
## 6              5.4                3.9            1.7              0.4
##   Gatunki
## 1  setosa
## 2  setosa
## 3  setosa
## 4  setosa
## 5  setosa
## 6  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane4 <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"data/dane4.rds"}\NormalTok{)}
\NormalTok{dane4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 150 x 5
##    `Dugo kielic~ `Szeroko kiel~ `Dugo patka` `Szeroko pat~
##               <dbl>            <dbl>            <dbl>            <dbl>
##  1              5.1              3.5              1.4              0.2
##  2              4.9              3                1.4              0.2
##  3              4.7              3.2              1.3              0.2
##  4              4.6              3.1              1.5              0.2
##  5              5                3.6              1.4              0.2
##  6              5.4              3.9              1.7              0.4
##  7             NA               NA                1.4              0.3
##  8              5                3.4              1.5              0.2
##  9              4.4              2.9              1.4              0.2
## 10              4.9              3.1              1.5              0.1
## # ... with 140 more rows, and 1 more variable: Gatunki <chr>
\end{verbatim}

Lista mo偶liwoci jak daje nam pakiet \texttt{rio} \citep{R-rio} jest niemal nieograniczona:\footnote{fragment pliku \texttt{help} funkcji \texttt{import}}

\begin{itemize}
\tightlist
\item
  Comma-separated data (.csv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE
\item
  Pipe-separated data (.psv), using fread or, if fread = FALSE, read.table with sep = `\textbar{}', row.names = FALSE and stringsAsFactors = FALSE
\item
  Tab-separated data (.tsv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE
\item
  SAS (.sas7bdat), using read\_sas.
\item
  SAS XPORT (.xpt), using read\_xpt or, if haven = FALSE, read.xport.
\item
  SPSS (.sav), using read\_sav. If haven = FALSE, read.spss can be used.
\item
  Stata (.dta), using read\_dta. If haven = FALSE, read.dta can be used.
\item
  SAS XPORT (.xpt), using read.xport.
\item
  SPSS Portable Files (.por), using read\_por.
\item
  Excel (.xls and .xlsx), using read\_excel. Use which to specify a sheet number. For .xlsx files, it is possible to set readxl = FALSE, so that read.xlsx can be used instead of readxl (the default).
\item
  R syntax object (.R), using dget
\item
  Saved R objects (.RData,.rda), using load for single-object .Rdata files. Use which to specify an object name for multi-object .Rdata files. This can be any R object (not just a data frame).
\item
  Serialized R objects (.rds), using readRDS. This can be any R object (not just a data frame).
\item
  Epiinfo (.rec), using read.epiinfo
\item
  Minitab (.mtp), using read.mtp
\item
  Systat (.syd), using read.systat
\item
  ``XBASE'' database files (.dbf), using read.dbf
\item
  Weka Attribute-Relation File Format (.arff), using read.arff
\item
  Data Interchange Format (.dif), using read.DIF
\item
  Fortran data (no recognized extension), using read.fortran
\item
  Fixed-width format data (.fwf), using a faster version of read.fwf that requires a widths argument and by default in rio has stringsAsFactors = FALSE. If readr = TRUE, import will be performed using read\_fwf, where widths should be: NULL, a vector of column widths, or the output of fwf\_empty, fwf\_widths, or fwf\_positions.
\item
  gzip comma-separated data (.csv.gz), using read.table with row.names = FALSE and stringsAsFactors = FALSE
\item
  CSVY (CSV with a YAML metadata header) using read\_csvy.
\item
  Feather R/Python interchange format (.feather), using read\_feather
\item
  Fast storage (.fst), using read.fst
\item
  JSON (.json), using fromJSON
\item
  Matlab (.mat), using read.mat
\item
  EViews (.wf1), using readEViews
\item
  OpenDocument Spreadsheet (.ods), using read\_ods. Use which to specify a sheet number.
\item
  Single-table HTML documents (.html), using read\_html. The data structure will only be read correctly if the HTML file can be converted to a list via as\_list.
\item
  Shallow XML documents (.xml), using read\_xml. The data structure will only be read correctly if the XML file can be converted to a list via as\_list.
\item
  YAML (.yml), using yaml.load
\item
  Clipboard import (on Windows and Mac OS), using read.table with row.names = FALSE
\item
  Google Sheets, as Comma-separated data (.csv)
\end{itemize}

\hypertarget{przyk1}{%
\section{Przykad}\label{przyk1}}

Poni偶sza ilustracja przedstawia fragment pliku danych \texttt{Analysis.txt} zawierajcego pewne bdy, kt贸re nale偶y naprawi na etapie importu danych. Po pierwsze brakuje w nim nazw zmiennych\footnote{cho nie wida tego na rysunku}. Poszczeg贸lne kolumny nazywaj si nastpujco: \texttt{season}, \texttt{size}, \texttt{speed}, \texttt{mxPH}, \texttt{mnO2}, \texttt{Cl}, \texttt{NO3}, \texttt{NH4}, \texttt{oPO4}, \texttt{PO4}, \texttt{Chla}, \texttt{a1}, \texttt{a2}, \texttt{a3}, \texttt{a4}, \texttt{a5}, \texttt{a6}, \texttt{a7}. Naszym zadaniem jest import tego pliku z jednoczesn obsug brak贸w\footnote{braki danych s zakodowane przez XXXXXXX} oraz nadaniem nag贸wk贸w kolumn. Plik \texttt{Analisis.txt} jest umieszczony w kagalogu \texttt{data/}. Z racji, 偶e plik dotyczy glon贸w, to dane zapiszemy pod nazw \texttt{algae}.

\begin{figure}
\includegraphics[width=10.67in]{images/analalysi_foto} \caption{Fragment pliku danych Analisis.txt}\label{fig:foto}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{'data/Analysis.txt'}\NormalTok{, }\DataTypeTok{header=}\NormalTok{F, }
                \DataTypeTok{dec=}\StringTok{'.'}\NormalTok{, }
                \DataTypeTok{col.names=}\KeywordTok{c}\NormalTok{(}\StringTok{'season'}\NormalTok{,}\StringTok{'size'}\NormalTok{,}\StringTok{'speed'}\NormalTok{,}\StringTok{'mxPH'}\NormalTok{,}\StringTok{'mnO2'}\NormalTok{,}\StringTok{'Cl'}\NormalTok{,}
                            \StringTok{'NO3'}\NormalTok{,}\StringTok{'NH4'}\NormalTok{,}\StringTok{'oPO4'}\NormalTok{,}\StringTok{'PO4'}\NormalTok{,}\StringTok{'Chla'}\NormalTok{,}\StringTok{'a1'}\NormalTok{,}\StringTok{'a2'}\NormalTok{,}
                            \StringTok{'a3'}\NormalTok{,}\StringTok{'a4'}\NormalTok{,}\StringTok{'a5'}\NormalTok{,}\StringTok{'a6'}\NormalTok{,}\StringTok{'a7'}\NormalTok{),}
                \DataTypeTok{na.strings=}\KeywordTok{c}\NormalTok{(}\StringTok{'XXXXXXX'}\NormalTok{))}
\KeywordTok{head}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   season  size  speed mxPH mnO2     Cl    NO3     NH4    oPO4     PO4 Chla
## 1 winter small medium 8.00  9.8 60.800  6.238 578.000 105.000 170.000 50.0
## 2 spring small medium 8.35  8.0 57.750  1.288 370.000 428.750 558.750  1.3
## 3 autumn small medium 8.10 11.4 40.020  5.330 346.667 125.667 187.057 15.6
## 4 spring small medium 8.07  4.8 77.364  2.302  98.182  61.182 138.700  1.4
## 5 autumn small medium 8.06  9.0 55.350 10.416 233.700  58.222  97.580 10.5
## 6 winter small   high 8.25 13.1 65.750  9.248 430.000  18.250  56.667 28.4
##     a1   a2   a3  a4   a5   a6  a7
## 1  0.0  0.0  0.0 0.0 34.2  8.3 0.0
## 2  1.4  7.6  4.8 1.9  6.7  0.0 2.1
## 3  3.3 53.6  1.9 0.0  0.0  0.0 9.7
## 4  3.1 41.0 18.9 0.0  1.4  0.0 1.4
## 5  9.2  2.9  7.5 0.0  7.5  4.1 1.0
## 6 15.1 14.6  1.4 0.0 22.5 12.6 2.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     season              size              speed                mxPH      
##  Length:200         Length:200         Length:200         Min.   :5.600  
##  Class :character   Class :character   Class :character   1st Qu.:7.700  
##  Mode  :character   Mode  :character   Mode  :character   Median :8.060  
##                                                           Mean   :8.012  
##                                                           3rd Qu.:8.400  
##                                                           Max.   :9.700  
##                                                           NA's   :1      
##       mnO2              Cl               NO3              NH4          
##  Min.   : 1.500   Min.   :  0.222   Min.   : 0.050   Min.   :    5.00  
##  1st Qu.: 7.725   1st Qu.: 10.981   1st Qu.: 1.296   1st Qu.:   38.33  
##  Median : 9.800   Median : 32.730   Median : 2.675   Median :  103.17  
##  Mean   : 9.118   Mean   : 43.636   Mean   : 3.282   Mean   :  501.30  
##  3rd Qu.:10.800   3rd Qu.: 57.824   3rd Qu.: 4.446   3rd Qu.:  226.95  
##  Max.   :13.400   Max.   :391.500   Max.   :45.650   Max.   :24064.00  
##  NA's   :2        NA's   :10        NA's   :2        NA's   :2         
##       oPO4             PO4              Chla               a1       
##  Min.   :  1.00   Min.   :  1.00   Min.   :  0.200   Min.   : 0.00  
##  1st Qu.: 15.70   1st Qu.: 41.38   1st Qu.:  2.000   1st Qu.: 1.50  
##  Median : 40.15   Median :103.29   Median :  5.475   Median : 6.95  
##  Mean   : 73.59   Mean   :137.88   Mean   : 13.971   Mean   :16.92  
##  3rd Qu.: 99.33   3rd Qu.:213.75   3rd Qu.: 18.308   3rd Qu.:24.80  
##  Max.   :564.60   Max.   :771.60   Max.   :110.456   Max.   :89.80  
##  NA's   :2        NA's   :2        NA's   :12                       
##        a2               a3               a4               a5        
##  Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000  
##  Median : 3.000   Median : 1.550   Median : 0.000   Median : 1.900  
##  Mean   : 7.458   Mean   : 4.309   Mean   : 1.992   Mean   : 5.064  
##  3rd Qu.:11.375   3rd Qu.: 4.925   3rd Qu.: 2.400   3rd Qu.: 7.500  
##  Max.   :72.600   Max.   :42.800   Max.   :44.600   Max.   :44.400  
##                                                                     
##        a6               a7        
##  Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 0.000   1st Qu.: 0.000  
##  Median : 0.000   Median : 1.000  
##  Mean   : 5.964   Mean   : 2.495  
##  3rd Qu.: 6.925   3rd Qu.: 2.400  
##  Max.   :77.600   Max.   :31.600  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{export}\NormalTok{(algae, }\DataTypeTok{file =} \StringTok{"data/algae.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{przygotowanie-danych}{%
\chapter{Przygotowanie danych}\label{przygotowanie-danych}}

\hypertarget{korekta-zbioru-danych}{%
\section{Korekta zbioru danych}\label{korekta-zbioru-danych}}

Dane, kt贸re importujemy z zewntrznego 藕r贸da najczciej nie speniaj format贸w obowizujcych w \textbf{R}. Czsto zmienne zawieraj niedopuszczalne znaki szczeg贸lne, odstpy w nazwach, powt贸rzone nazwy kolumn, nazwy zmiennych zaczynajce si od liczby, czy puste wiersze lub kolumny. Przed przystpieniem do analizy zbioru nale偶y rozwa偶y ewentualne poprawki nazw zmiennych, czy usunicie pustych kolumn i wierszy. Niekt贸rych czynnoci mo偶na dokona ju偶 na etapie importu danych, stosujc pewne pakiety oraz nowe funkcjonalnoci rodowiska \textbf{RStudio}. W wikszoci przypadk贸w uchroni nas to od 偶mudnego przeksztacania typ贸w zmiennych. Oczywicie wszystkie te czynnoci czyszczenia danych mo偶na r贸wnie偶 dokona ju偶 po imporcie danych, za pomoc odpowiednich komend \textbf{R}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## przykadowe niepo偶dane nazwy zmiennych}
\NormalTok{test_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{18}\NormalTok{),}\DataTypeTok{ncol =} \DecValTok{6}\NormalTok{))}
\KeywordTok{names}\NormalTok{(test_df) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"hIgHlo"}\NormalTok{, }\StringTok{"REPEAT VALUE"}\NormalTok{, }\StringTok{"REPEAT VALUE"}\NormalTok{,}
                    \StringTok{"% successful (2009)"}\NormalTok{,  }\StringTok{"abc@!*"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\NormalTok{test_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       hIgHlo REPEAT VALUE REPEAT VALUE % successful (2009)     abc@!*
## 1  0.5766562   -1.0119405   -2.2283192           0.9983100  0.4401545
## 2 -1.0209955   -1.9974551    0.5367048          -0.2359354 -0.7866444
## 3 -1.1346090    0.9599928    0.5118476          -1.1698186 -0.1328684
##             
## 1  1.2451586
## 2 -0.2748401
## 3 -1.0657506
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## do poprawy nazw zmiennych u偶yjemy funkcji make.names}
\KeywordTok{names}\NormalTok{(test_df) <-}\StringTok{ }\KeywordTok{make.names}\NormalTok{(}\KeywordTok{names}\NormalTok{(test_df))}
\NormalTok{test_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       hIgHlo REPEAT.VALUE REPEAT.VALUE X..successful..2009.     abc...
## 1  0.5766562   -1.0119405   -2.2283192            0.9983100  0.4401545
## 2 -1.0209955   -1.9974551    0.5367048           -0.2359354 -0.7866444
## 3 -1.1346090    0.9599928    0.5118476           -1.1698186 -0.1328684
##            X
## 1  1.2451586
## 2 -0.2748401
## 3 -1.0657506
\end{verbatim}

Efekt kocowy cho skuteczny to nie jest zadowalajcy. Czyszczenia nazw zmiennych mo偶na te偶 dokona stosujc funkcj \texttt{clean\_names} pakietu \textbf{janitor} \citep{R-janitor}. Pozwala on r贸wnie偶 na usuwanie pustych wierszy i kolumn, znajdowanie zduplikowanych rekord贸w, itp.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(janitor)}
\NormalTok{test_df }\OperatorTok{%>%}\StringTok{ }\CommentTok{# aby na stae zmieni nazwy zmiennych trzeba podstawienia}
\StringTok{    }\KeywordTok{clean_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     h_ig_hlo repeat_value repeat_value_2 x_successful_2009        abc
## 1  0.5766562   -1.0119405     -2.2283192         0.9983100  0.4401545
## 2 -1.0209955   -1.9974551      0.5367048        -0.2359354 -0.7866444
## 3 -1.1346090    0.9599928      0.5118476        -1.1698186 -0.1328684
##            x
## 1  1.2451586
## 2 -0.2748401
## 3 -1.0657506
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# przykadowe dane}
\NormalTok{x <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{w1=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\OtherTok{NA}\NormalTok{),}\DataTypeTok{w2=}\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\OtherTok{NA}\NormalTok{), }\DataTypeTok{w3=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{1}\NormalTok{,}\OtherTok{NA}\NormalTok{))}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   w1 w2 w3
## 1  1 NA  1
## 2  4  2 NA
## 3  2  3  1
## 4 NA NA NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{remove_empty}\NormalTok{(}\StringTok{"rows"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   w1 w2 w3
## 1  1 NA  1
## 2  4  2 NA
## 3  2  3  1
\end{verbatim}

\hypertarget{identyfikacja-brakow-danych}{%
\subsection{Identyfikacja brak贸w danych}\label{identyfikacja-brakow-danych}}

Zanim usuniemy jakiekolwiek braki w zbiorze, powinnimy je najpierw zidentyfikowa, okreli ich charakter, a dopiero potem ewentualnie podj decyzj o uzupenianiu brak贸w.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae <-}\StringTok{ }\NormalTok{rio}\OperatorTok{::}\KeywordTok{import}\NormalTok{(}\StringTok{"data/algae.csv"}\NormalTok{)}

\CommentTok{# najprociej jest wywoa summary}
\KeywordTok{summary}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     season              size              speed                mxPH      
##  Length:200         Length:200         Length:200         Min.   :5.600  
##  Class :character   Class :character   Class :character   1st Qu.:7.700  
##  Mode  :character   Mode  :character   Mode  :character   Median :8.060  
##                                                           Mean   :8.012  
##                                                           3rd Qu.:8.400  
##                                                           Max.   :9.700  
##                                                           NA's   :1      
##       mnO2              Cl               NO3              NH4          
##  Min.   : 1.500   Min.   :  0.222   Min.   : 0.050   Min.   :    5.00  
##  1st Qu.: 7.725   1st Qu.: 10.981   1st Qu.: 1.296   1st Qu.:   38.33  
##  Median : 9.800   Median : 32.730   Median : 2.675   Median :  103.17  
##  Mean   : 9.118   Mean   : 43.636   Mean   : 3.282   Mean   :  501.30  
##  3rd Qu.:10.800   3rd Qu.: 57.824   3rd Qu.: 4.446   3rd Qu.:  226.95  
##  Max.   :13.400   Max.   :391.500   Max.   :45.650   Max.   :24064.00  
##  NA's   :2        NA's   :10        NA's   :2        NA's   :2         
##       oPO4             PO4              Chla               a1       
##  Min.   :  1.00   Min.   :  1.00   Min.   :  0.200   Min.   : 0.00  
##  1st Qu.: 15.70   1st Qu.: 41.38   1st Qu.:  2.000   1st Qu.: 1.50  
##  Median : 40.15   Median :103.29   Median :  5.475   Median : 6.95  
##  Mean   : 73.59   Mean   :137.88   Mean   : 13.971   Mean   :16.92  
##  3rd Qu.: 99.33   3rd Qu.:213.75   3rd Qu.: 18.308   3rd Qu.:24.80  
##  Max.   :564.60   Max.   :771.60   Max.   :110.456   Max.   :89.80  
##  NA's   :2        NA's   :2        NA's   :12                       
##        a2               a3               a4               a5        
##  Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000  
##  Median : 3.000   Median : 1.550   Median : 0.000   Median : 1.900  
##  Mean   : 7.458   Mean   : 4.309   Mean   : 1.992   Mean   : 5.064  
##  3rd Qu.:11.375   3rd Qu.: 4.925   3rd Qu.: 2.400   3rd Qu.: 7.500  
##  Max.   :72.600   Max.   :42.800   Max.   :44.600   Max.   :44.400  
##                                                                     
##        a6               a7        
##  Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 0.000   1st Qu.: 0.000  
##  Median : 0.000   Median : 1.000  
##  Mean   : 5.964   Mean   : 2.495  
##  3rd Qu.: 6.925   3rd Qu.: 2.400  
##  Max.   :77.600   Max.   :31.600  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## wywietl niekompletne wiersze}
\NormalTok{algae[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae),] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size  speed mxPH mnO2   Cl   NO3 NH4 oPO4 PO4 Chla   a1  a2  a3
## 28 autumn small   high  6.8 11.1 9.00 0.630  20  4.0  NA  2.7 30.3 1.9 0.0
## 38 spring small   high  8.0   NA 1.45 0.810  10  2.5 3.0  0.3 75.8 0.0 0.0
## 48 winter small    low   NA 12.6 9.00 0.230  10  5.0 6.0  1.1 35.5 0.0 0.0
## 55 winter small   high  6.6 10.8   NA 3.245  10  1.0 6.5   NA 24.3 0.0 0.0
## 56 spring small medium  5.6 11.8   NA 2.220   5  1.0 1.0   NA 82.7 0.0 0.0
## 57 autumn small medium  5.7 10.8   NA 2.550  10  1.0 4.0   NA 16.8 4.6 3.9
##      a4  a5  a6  a7
## 28  0.0 2.1 1.4 2.1
## 38  0.0 0.0 0.0 0.0
## 48  0.0 0.0 0.0 0.0
## 55  0.0 0.0 0.0 0.0
## 56  0.0 0.0 0.0 0.0
## 57 11.5 0.0 0.0 0.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## policz niekompletne wiersze}
\KeywordTok{nrow}\NormalTok{(algae[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## sprawdzenie liczby brak贸w w wierszach}
\KeywordTok{apply}\NormalTok{(algae, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
##  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36 
##   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0 
##  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54 
##   0   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0 
##  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72 
##   2   2   2   2   2   2   2   6   1   0   0   0   0   0   0   0   0   0 
##  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
##  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 
##   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0 
## 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0 
## 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 
##   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## 199 200 
##   6   0
\end{verbatim}

Wiele ciekawych funkcji do eksploracji danych znajduje si w pakiecie \textbf{DMwR} \citep{R-DMwR}, kt贸ry zosta przygotowany przy okazji publikacji ksi偶ki \emph{Data Mining with R}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## poszukiwanie wierszy zawierajcych wiele brak贸w}
\CommentTok{## w tym przypadku pr贸g wywietlania ustawiony jest na 0.2}
\CommentTok{## czyli 20% wszystkich kolumn}
\KeywordTok{library}\NormalTok{(DMwR)}
\KeywordTok{manyNAs}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  62 199 
##  62 199
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## tworzenie zbioru pozbawionego wierszy zawierajcych wiele brak贸w}
\NormalTok{algae2 <-}\StringTok{ }\NormalTok{algae[}\OperatorTok{-}\KeywordTok{manyNAs}\NormalTok{(algae), ]}

\CommentTok{## sprawdzamy liczb wybrakowanych wierszy kt贸re pozostay}
\KeywordTok{nrow}\NormalTok{(algae2[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae2),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## usuwamy wszystkie wiersze z brakami}
\NormalTok{algae3 <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(algae)}
    
\CommentTok{## wywietl wiersze z brakami}
\NormalTok{algae3[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae3),] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] season size   speed  mxPH   mnO2   Cl     NO3    NH4    oPO4   PO4   
## [11] Chla   a1     a2     a3     a4     a5     a6     a7    
## <0 rows> (or 0-length row.names)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## liczba pozostaych wybrakowanych wierszy}
\KeywordTok{nrow}\NormalTok{(algae3[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae3),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## mo偶na oczywicie te偶 rcznie usuwa wiersze (nie polecam)}
\NormalTok{algae4 <-}\StringTok{ }\NormalTok{algae[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{62}\NormalTok{,}\DecValTok{199}\NormalTok{),]}
\end{Highlighting}
\end{Shaded}

Mo偶na te偶 zbudowa funkcj, kt贸ra bdzie usuwaa braki danych wg naszego upodobania.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## najpierw budujemy funkcj i j kompilujemy aby R m贸g ja stosowa}
\CommentTok{## parametr prog ustala pr贸g odcicia wierszy}
\NormalTok{czysc.dane <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(dt, }\DataTypeTok{prog =} \DecValTok{0}\NormalTok{)\{}
\NormalTok{    licz.braki <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(dt, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\NormalTok{    czyste.dt <-}\StringTok{ }\NormalTok{dt[}\OperatorTok{!}\NormalTok{(licz.braki}\OperatorTok{/}\KeywordTok{ncol}\NormalTok{(dt)}\OperatorTok{>}\NormalTok{prog), ]}
    \KeywordTok{return}\NormalTok{(czyste.dt)}
\NormalTok{\}}
    
\CommentTok{## potem j mo偶emy stosowa}
\NormalTok{algae4 <-}\StringTok{ }\KeywordTok{czysc.dane}\NormalTok{(algae)}
\KeywordTok{nrow}\NormalTok{(algae4[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae4),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## czycimy wiersze, kt贸rych liczba brak贸w przekracza 20% wszystkich kolumn}
\NormalTok{algae5 <-}\StringTok{ }\KeywordTok{czysc.dane}\NormalTok{(algae, }\DataTypeTok{prog =} \FloatTok{0.2}\NormalTok{)}
\KeywordTok{nrow}\NormalTok{(algae5[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae5),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14
\end{verbatim}

Bardzo ciekawym narzdziem do znajdowania brak贸w danych jest funkcja \texttt{md.pattern} pakietu \textbf{mice} \citep{R-mice}. Wskazuje on ile brak贸w wystpuje w ramach ka偶dej zmiennej.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mice)}
\KeywordTok{md.pattern}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/mice1-1.pdf}
\caption{\label{fig:mice1}Na czerwono zaznaczone s zmienne, kt贸re zwieraj braki danych. Liczba w wierszu po lewej stronie wykresu wskazuje ile wierszy w bazie ma dan charakterystyk, a liczba po prawej oznacza ile zmiennych byo \emph{wybrakowanych}}
\end{figure}

\begin{verbatim}
##     season size speed a1 a2 a3 a4 a5 a6 a7 mxPH mnO2 NO3 NH4 oPO4 PO4 Cl
## 184      1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   1  1
## 3        1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   1  1
## 1        1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   1  0
## 7        1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   1  0
## 1        1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   0  1
## 1        1    1     1  1  1  1  1  1  1  1    1    1   0   0    0   0  0
## 1        1    1     1  1  1  1  1  1  1  1    1    0   1   1    1   1  1
## 1        1    1     1  1  1  1  1  1  1  1    1    0   0   0    0   1  0
## 1        1    1     1  1  1  1  1  1  1  1    0    1   1   1    1   1  1
##          0    0     0  0  0  0  0  0  0  0    1    2   2   2    2   2 10
##     Chla   
## 184    1  0
## 3      0  1
## 1      1  1
## 7      0  2
## 1      1  1
## 1      0  6
## 1      1  1
## 1      0  6
## 1      1  1
##       12 33
\end{verbatim}

\hypertarget{zastepowanie-brakow-danych}{%
\subsection{Zastpowanie brak贸w danych}\label{zastepowanie-brakow-danych}}

Zastpowanie brak贸w danych (zwane tak偶e \emph{imputacj danych}) jest kolejnym etapem procesu przygotowania danych do analiz. Nie mo偶na jednak wyr贸偶ni uniwersalnego sposobu zastpowania brak贸w dla wszystkich mo偶liwych sytuacji. Wr贸d statystyk贸w panuje przekonanie, 偶e w przypadku wystpienia brak贸w danych mo偶na zastosowa trzy strategie:

\begin{itemize}
\tightlist
\item
  nic nie robi z brakami - co wydaje si niedorzeczne ale wcale takie nie jest, poniewa偶 istnieje wiele modeli statystycznych (np. drzewa decyzyjne), kt贸re wietnie radz sobie w sytuacji brak贸w danych. Niestety nie jest to spos贸b, kt贸ry mo偶na stosowa zawsze, poniewa偶 s r贸wnie偶 modele wymagajce kompletnoci danych jak na przykad sieci neuronowe.
\item
  usuwa braki wierszami\footnote{polega na usuwaniu wierszy zawierajcych braki} - to metoda, kt贸ra jest stosowana domylnie w przypadku kiedy tw贸rca modelu nie zadecyduje o innym sposobie obsugi luk. Metoda ta ma swoj niewtpliw zalet w postaci jasnej i prostej procedury, ale szczeg贸lnie w przypadku niewielkich zbior贸w mo偶e skutkowa obci偶eniem estymator贸w. Nie wiemy bowiem jaka warto faktycznie jest przypisana danej cesze. Jeli jest to warto bliska np. redniej, to nie wpynie znaczco na obci偶enie estymatora wartoci oczekiwanej. W przypadku, gdy r贸偶ni si ona znacznie od redniej tej cechy, to estymator mo偶e ju偶 wykazywa obci偶enie. Jego wielko zale偶y r贸wnie偶 od liczby usunitych element贸w. Nie jest zalecane usuwanie wielu wierszy ze zbioru danych i na podstawie okrojonego zbioru wyciganie wniosk贸w o populacji, poniewa偶 pr贸ba jest w贸wczas znaczco inna ni偶 populacja. Dodatkowo jeli estymatory s wyznaczane na podstawie zbioru wyra藕nie mniej licznego, to precyzja estymator贸w wyra偶ona wariancj spada. Reasumujc, jeli liczba wierszy z brakujcymi danymi jest niewielka w stosunku do caego zbioru, to usuwanie wierszy jest sensownym rozwizaniem.
\item
  uzupenianie brak贸w - to procedura polegajca na zastpowaniu brak贸w r贸偶nymi technikami. Jej niewtpliw zalet jest fakt posiadania kompletnych danych bez koniecznoci usuwania wierszy. Niestety wi偶e si to r贸wnie偶 z pewnymi wadami. Zbi贸r posiadajcy wiele brak贸w uzupenianych nawet bardzo wyrafinowanymi metodami mo偶e cechowa si zani偶on wariancj poszczeg贸lnych cech oraz tzw. przeuczeniem\footnote{wicej o zjawisku przeuczenia w dalszej czci ksi偶ki}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Uzupenianie redni - braki w zakresie danej zmiennej uzupeniamy redni tej zmiennej przypadk贸w uzupenionych.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\KeywordTok{is.na}\NormalTok{(algae}\OperatorTok{$}\NormalTok{mxPH), ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size speed mxPH mnO2 Cl  NO3 NH4 oPO4 PO4 Chla   a1 a2 a3 a4 a5
## 48 winter small   low   NA 12.6  9 0.23  10    5   6  1.1 35.5  0  0  0  0
##    a6 a7
## 48  0  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(algae}\OperatorTok{$}\NormalTok{mxPH, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\NormalTok{algae[}\KeywordTok{is.na}\NormalTok{(algae}\OperatorTok{$}\NormalTok{mxPH), }\StringTok{"mxPH"}\NormalTok{] <-}\StringTok{ }\NormalTok{m}
\NormalTok{algae[}\KeywordTok{is.na}\NormalTok{(algae}\OperatorTok{$}\NormalTok{mxPH), ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] season size   speed  mxPH   mnO2   Cl     NO3    NH4    oPO4   PO4   
## [11] Chla   a1     a2     a3     a4     a5     a6     a7    
## <0 rows> (or 0-length row.names)
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Uzupenianie median - braki w zakresie danej zmiennej uzupeniamy median tej zmiennej przypadk贸w uzupenionych.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(Chla)) }\OperatorTok{%>%}\StringTok{ }\NormalTok{head}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   season  size  speed mxPH mnO2 Cl   NO3 NH4 oPO4  PO4 Chla   a1  a2  a3
## 1 winter small   high  6.6 10.8 NA 3.245  10    1  6.5   NA 24.3 0.0 0.0
## 2 spring small medium  5.6 11.8 NA 2.220   5    1  1.0   NA 82.7 0.0 0.0
## 3 autumn small medium  5.7 10.8 NA 2.550  10    1  4.0   NA 16.8 4.6 3.9
## 4 spring small   high  6.6  9.5 NA 1.320  20    1  6.0   NA 46.8 0.0 0.0
## 5 summer small   high  6.6 10.8 NA 2.640  10    2 11.0   NA 46.9 0.0 0.0
## 6 autumn small medium  6.6 11.3 NA 4.170  10    1  6.0   NA 47.1 0.0 0.0
##     a4 a5  a6 a7
## 1  0.0  0 0.0  0
## 2  0.0  0 0.0  0
## 3 11.5  0 0.0  0
## 4 28.8  0 0.0  0
## 5 13.4  0 0.0  0
## 6  0.0  0 1.2  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\KeywordTok{is.na}\NormalTok{(algae}\OperatorTok{$}\NormalTok{Chla), }\StringTok{"Chla"}\NormalTok{] <-}\StringTok{ }\KeywordTok{median}\NormalTok{(algae}\OperatorTok{$}\NormalTok{Chla, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Wypenianie zmiennych typu wyliczeniowego, logicznego lub znakowego odbywa si najczciej przez dobranie w miejsce brakujcej wartoci, elementu powtarzajcego si najczciej wr贸d obiekt贸w obserwowanych. W pakiecie \textbf{DMwR} istnieje funkcja \texttt{centralImputation}, kt贸ra wypenia braki wartoci centraln (w przypadku zmiennych typu liczbowego - median, a dla wartoci logicznych, wyliczeniowych lub tekstowych - mod).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\DecValTok{48}\NormalTok{, }\StringTok{"season"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "winter"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\DecValTok{48}\NormalTok{, }\StringTok{"season"}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\NormalTok{algae.uzup <-}\StringTok{ }\KeywordTok{centralImputation}\NormalTok{(algae)}
\NormalTok{algae.uzup[}\DecValTok{48}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size speed     mxPH mnO2 Cl  NO3 NH4 oPO4 PO4 Chla   a1 a2 a3
## 48 winter small   low 8.011734 12.6  9 0.23  10    5   6  1.1 35.5  0  0
##    a4 a5 a6 a7
## 48  0  0  0  0
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Jeszcze innym sposobem imputacji danych s algorytmy oparte o metod \(k\)-najbli偶szych ssiad贸w. Algorytm opiera si na prostej zasadzie, uzupeniania brakujcych wartoci median (w przypadku zmiennych ilociowych) lub mod (w przypadku zmiennych jakociowych) element贸w, kt贸re s \(k\)-tymi najbli偶szymi ssiadami w metryce
  \begin{equation}\label{knn}
   d(x,y)=\sqrt{\sum_{i=1}^{p}\delta_i(x_i,y_i)},
  \end{equation}
  gdzie \(\delta_i\) jest odlegoci pomidzy dwoma elementami ze wzgldu na \(i\)-t cech, okrelon nastpujco
  \begin{equation}\label{metryka}
   \delta_i(v_1, v_2)=\begin{cases}
       1,& \text{jeli zmienna jest jakociowa i }v_1\neq v_2\\
       0,& \text{jeli zmienna jest jakociowa i }v_1=v_2\\
       (v_1-v_2)^2,& \text{jeli zmienna jest ilociowa.}
   \end{cases}
  \end{equation}
  Odlegoci s mierzone dla zmiennych standaryzowanych. Istnieje te偶 odmiana z wagami, kt贸re malej wraz ze wzrostem odlegoci pomidzy ssiadem a uzupenianym elementem (np. \(w(d)=\exp(d)\)).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\DecValTok{48}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size speed     mxPH mnO2 Cl  NO3 NH4 oPO4 PO4 Chla   a1 a2 a3
## 48   <NA> small   low 8.011734 12.6  9 0.23  10    5   6  1.1 35.5  0  0
##    a4 a5 a6 a7
## 48  0  0  0  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae <-}\StringTok{ }\NormalTok{algae }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.character, as.factor)}
\NormalTok{algae.uzup <-}\StringTok{ }\KeywordTok{knnImputation}\NormalTok{(algae, }\DataTypeTok{k =} \DecValTok{5}\NormalTok{, }\DataTypeTok{scale =}\NormalTok{ F, }\DataTypeTok{meth =} \StringTok{"median"}\NormalTok{)}
\NormalTok{algae.uzup[}\DecValTok{48}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size speed     mxPH mnO2 Cl  NO3 NH4 oPO4 PO4 Chla   a1 a2 a3
## 48 summer small   low 8.011734 12.6  9 0.23  10    5   6  1.1 35.5  0  0
##    a4 a5 a6 a7
## 48  0  0  0  0
\end{verbatim}

Istniej r贸wnie偶 du偶o bardziej zo偶one algorytmy imputacji danych oparte na bardziej wyrafinowanych technikach, takich jak: predykcja modelami liniowymi, nieliniowymi, analiza dyskryminacyjna, drzewa klasyfikacyjne. Dwa najbardziej znane pakiety zawierajce funkcje do imputacji w spos贸b zo偶ony, to \textbf{Amelia} i \textbf{mice}.

Imputacja danych z zastosowaniem pakietu \textbf{mice} wymaga podjcia kilku decyzji przed przystpieniem do uzupeniania danych:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Czy dane s MAR (ang. \emph{Missing At Random}) czy MNAR (ang. \emph{Missing Not At Random}), co oznacza, 偶e musimy si zastanowi jakie mogy by 藕r贸da brak贸w danych, przypadkowe czy systematyczne?
\item
  Nale偶y si zdecydowa na form imputacji, okrelajc struktur zale偶noci pomidzy cechami oraz rozkad bdu danej cechy?
\item
  Wybra zbi贸r danych, kt贸ry posu偶y nam za predyktory w imputacji (nie mog zawiera brak贸w).
\item
  Okrelenie, kt贸re niepene zmienne s funkcjami innych wybrakowanych zmiennych.
\item
  Okreli w jakiej kolejnoci dane bd imputowane.
\item
  Okreli parametry startowe imputacji (liczb iteracji, warunek zbie偶noci).
\item
  Okreli licz imputowanych zbior贸w.
\end{enumerate}

Ad 1. Wyr贸偶niamy nastpujce rodzaje brak贸w danych:

\begin{itemize}
\tightlist
\item
  MCAR (ang. \emph{Missing Completely At Random}) - z definicji to braki, kt贸rych pojawienie si jest kompletnie losowe. Przykadowo gdy osoba poproszona o wypenienie wieku w ankiecie bdzie rzuca monet czy wypeni t zmienn.
\item
  MAR - oznacza, 偶e obserwowane wartoci i wybrakowane maj inne rozkady ale da si je oszacowa na podstawie danych obserwowanych. Przykadowo cinienie ttnicze u os贸b, kt贸re nie wypeniy tej wartoci jest wy偶sze ni偶 u os贸b, kt贸re wpisay swoje cinienie. Okazuje si, 偶e osoby starsze z nadcinieniem nie wypeniay ankiety w tym punkcie.
\item
  MNAR - jeli nie jest speniony warunek MCAR i MAR, w贸wczas brak ma charakter nielosowy. Przykadowo respondenci osigajcy wy偶sze zarobki sukcesywnie nie wypeniaj pola ``zarobki'' i dodatkowo nie ma w ankiecie zmiennych, kt贸re pozwoliyby nam ustali, jakie to osoby.
\end{itemize}

Ad 2. Decyzja o algorytmie imputacji wynika bezporednio ze skali w jakiej jest mierzona dana zmienna. Ze wzgldu na rodzaj cechy u偶ywa bdziemy nastpujcych metod:

\begin{table}[t]

\caption{\label{tab:methods}Zestaw metod imputacji danych stosowanych w pakiecie **mice**}
\centering
\begin{tabular}{lll}
\toprule
method & type & description\\
\midrule
pmm & any & Predictive.mean.matching\\
midastouch & any & Weighted predictive mean matching\\
sample & any & Random sample from observed values\\
cart & any & Classification and regression trees\\
rf & any & Random forest imputations\\
\addlinespace
mean & numeric & Unconditional mean imputation\\
norm & numeric & Bayesian linear regression\\
norm.nob & numeric & Linear regression ignoring model error\\
norm.boot & numeric & Linear regression using bootstrap\\
norm.predict & numeric & Linear regression, predicted values\\
\addlinespace
quadratic & numeric & Imputation of quadratic terms\\
ri & numeric & Random indicator for nonignorable data\\
logreg & binary & Logistic regression\\
logreg.boot & binary & Logistic regression with bootstrap\\
polr & ordered & Proportional odds model\\
\addlinespace
polyreg & unordered & Polytomous logistic regression\\
lda & unordered & Linear discriminant analysis\\
2l.norm & numeric & Level-1 normal heteroscedastic\\
2l.lmer & numeric & Level-1 normal homoscedastic,
                                lmer\\
2l.pan & numeric & Level-1 normal homoscedastic, pan\\
\addlinespace
2l.bin & binary & Level-1 logistic, glmer\\
2lonly.mean & numeric & Level-2 class mean\\
2lonly.norm & numeric & Level-2 class normal\\
2lonly.pmm & any & Level-2 class predictive mean matching\\
\bottomrule
\end{tabular}
\end{table}

Ka偶dy z czterech typ贸w danych ma sw贸j domylny algorytm przeznaczony do imputacji:

\begin{itemize}
\tightlist
\item
  zmienna ilociowa - \texttt{pmm}
\item
  zmienna dychotomiczna (stany 0 lub 1) - \texttt{logreg}
\item
  zmienna typu wyliczeniowego (nieuporzdkowana) - \texttt{polyreg}
\item
  zmienna typu wyliczeniowego (uporzdkowana) - \texttt{polr}
\end{itemize}

Niewtpliw zalet metody \texttt{pmm} jest to, 偶e wartoci imputowane s ograniczone jedynie do obserwowanych wartoci. Metody \texttt{norm} i \texttt{norm.nob} uzupeniaj brakujce wartoci w oparciu o model liniowy. S one szybkie i efektywne w przypadku gdy reszty modelu s zbli偶one rozkadem do normalnoci. Druga z tych technik nie bierze pod uwag niepewnoci zwizanej z modelem imputujcym. Metoda \texttt{2L.norm} opiera si na dwupoziomowym heterogenicznym modelu liniowym (skupienia s wczone jako efekt do modelu). Technika \texttt{polyreg} korzysta z funkcji \texttt{multinom} pakietu \textbf{nnet} tworzcej model wielomianowy. \texttt{polr} opiera si o proporcjonalny model logitowy z pakietu \textbf{MASS}. \texttt{lda} to model dyskryminacyjny klasyfikujcy obiekty na podstawie prawdopodobiestw \emph{a posteriori}. Metoda \texttt{sample} zastpuje braki losowa wybranymi wartociami spor贸d wartoci obserwowanych.

Ad 3. Do ustalenia predyktor贸w w modelu \texttt{mice} su偶y funkcja \texttt{predictorMatrix}. Po pierwsze wywietla ona domylny ukad predyktor贸w wczanych do modelu. Mo偶na go dowolnie zmieni i podstawi do modelu imputujcego dane parametrem \texttt{predictorMatrix}. Zera wystpujce w kolejnych wierszach macierzy predyktor贸w oznaczaj pominicie tej zmiennej przy imputacji innej zmiennej. Jeli dodatkowo chcemy by jaka zmienna nie bya imputowana, to opr贸cz usunicia jej z listy predyktor贸w, nale偶y wymaza j z listy metod predykcji (\texttt{method}).

Og贸lne zalecenia co do tego jakie zmienne stosowa jako predyktory jest takie, 偶eby bra ich jak najwicej. Spowoduje to, 偶e bardziej prawdopodobny staje si brak typu MAR a nie MNAR. Z drugiej jednak strony, nierzadko zbiory zawieraj olbrzymi liczb zmiennych i wczanie ich wszystkich do modelu imputujcego nie bdzie miao sensu.

Zalecenia doboru zmiennych s nastpujce:

\begin{itemize}
\tightlist
\item
  we藕 wszystkie te zmienne, kt贸re s wczane do modelu waciwego, czyli tego za pomoc kt贸rego chcesz pozna struktur zale偶noci;
\item
  czasem do modelu imputujcego nale偶y te偶 wczy interakcje zmiennych z modelu waciwego;
\item
  dodaj zmienne, kt贸re mog mie wpyw na wybrakowane cechy;
\item
  wcz zmienne istotnie podnoszce poziom wyjanionej wariancji modelu;
\item
  na koniec usu te zmienne spor贸d predyktor贸w, kt贸re same zawieraj zbyt wiele brak贸w.
\end{itemize}

Ad 4-7. Decyzje podejmowane w tych punktach zale偶 istotnie od analizowanego zbioru i bd przedmiotem oddzielnych analiz w kontekcie rozwa偶anych zbior贸w i zada.

\hypertarget{przyk21}{%
\section{Przykad}\label{przyk21}}

Dokonamy imputacji zbioru \texttt{airquality} z wykorzystaniem pakiet贸w \textbf{mice} i \textbf{VIM} \citep{R-VIM}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{airquality}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Ozone           Solar.R           Wind             Temp      
##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  
##  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  
##  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  
##  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  
##  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  
##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  
##  NA's   :37       NA's   :7                                       
##      Month            Day      
##  Min.   :5.000   Min.   : 1.0  
##  1st Qu.:6.000   1st Qu.: 8.0  
##  Median :7.000   Median :16.0  
##  Mean   :6.993   Mean   :15.8  
##  3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :9.000   Max.   :31.0  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tworzymy dodatkowe braki danych}
\NormalTok{data[}\DecValTok{4}\OperatorTok{:}\DecValTok{10}\NormalTok{,}\DecValTok{3}\NormalTok{] <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{7}\NormalTok{)}
\NormalTok{data[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{4}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Ozone           Solar.R           Wind             Temp      
##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :57.00  
##  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:73.00  
##  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  
##  Mean   : 42.13   Mean   :185.9   Mean   : 9.806   Mean   :78.28  
##  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  
##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  
##  NA's   :37       NA's   :7       NA's   :7        NA's   :5      
##      Month            Day      
##  Min.   :5.000   Min.   : 1.0  
##  1st Qu.:6.000   1st Qu.: 8.0  
##  Median :7.000   Median :16.0  
##  Mean   :6.993   Mean   :15.8  
##  3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :9.000   Max.   :31.0  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{md.pattern}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-3-1.pdf}

\begin{verbatim}
##     Month Day Temp Solar.R Wind Ozone   
## 104     1   1    1       1    1     1  0
## 34      1   1    1       1    1     0  1
## 3       1   1    1       1    0     1  1
## 1       1   1    1       1    0     0  2
## 4       1   1    1       0    1     1  1
## 1       1   1    1       0    1     0  2
## 1       1   1    1       0    0     1  2
## 3       1   1    0       1    1     1  1
## 1       1   1    0       1    0     1  2
## 1       1   1    0       0    0     0  4
##         0   0    5       7    7    37 56
\end{verbatim}

Do ilustracji brak贸w danych mo偶na zastosowa funkcje pakietu \textbf{VIM}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(VIM)}
\KeywordTok{aggr}\NormalTok{(data, }\DataTypeTok{numbers=}\OtherTok{TRUE}\NormalTok{, }
     \DataTypeTok{sortVars=}\OtherTok{TRUE}\NormalTok{, }
     \DataTypeTok{labels=}\KeywordTok{names}\NormalTok{(data), }
     \DataTypeTok{cex.axis=}\NormalTok{.}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{verbatim}
## 
##  Variables sorted by number of missings: 
##  Variable      Count
##     Ozone 0.24183007
##   Solar.R 0.04575163
##      Wind 0.04575163
##      Temp 0.03267974
##     Month 0.00000000
##       Day 0.00000000
\end{verbatim}

Tak przedstawia si wykres rozrzutu zmiennych \texttt{Ozone} i \texttt{Solar.R} z uwzgldnieniem poo偶enia brak贸w danych.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{marginplot}\NormalTok{(data[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-5-1.pdf}

Dokonamy imputacji metod \texttt{pmm}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tempData <-}\StringTok{ }\KeywordTok{mice}\NormalTok{(data, }
                 \DataTypeTok{maxit=}\DecValTok{50}\NormalTok{, }
                 \DataTypeTok{meth=}\StringTok{'pmm'}\NormalTok{, }
                 \DataTypeTok{seed=}\DecValTok{44}\NormalTok{, }
                 \DataTypeTok{printFlag =}\NormalTok{ F)}
\KeywordTok{summary}\NormalTok{(tempData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Class: mids
## Number of multiple imputations:  5 
## Imputation methods:
##   Ozone Solar.R    Wind    Temp   Month     Day 
##   "pmm"   "pmm"   "pmm"   "pmm"      ""      "" 
## PredictorMatrix:
##         Ozone Solar.R Wind Temp Month Day
## Ozone       0       1    1    1     1   1
## Solar.R     1       0    1    1     1   1
## Wind        1       1    0    1     1   1
## Temp        1       1    1    0     1   1
## Month       1       1    1    1     0   1
## Day         1       1    1    1     1   0
\end{verbatim}

Poniewa偶, funkcja \texttt{mice} domylnie dokonuje 5 kompletnych imputacji, mo偶emy si przekona jak bardzo r贸偶ni si poszczeg贸lne imputacje i zdecydowa si na jedn z nich.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(tempData}\OperatorTok{$}\NormalTok{imp}\OperatorTok{$}\NormalTok{Ozone)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     1  2  3  4  5
## 5  21 20  7 36 13
## 10 21 16 44 22 21
## 25 14 14 14  6  8
## 26 23 18  8 19 14
## 27 37 23 21  7  9
## 32 63 23  7 52 39
\end{verbatim}

Ostatecznie imputacji dokonujemy wybierajc jeden z zestaw贸w danych uzupeniajcych (np. pierwszy).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{completedData <-}\StringTok{ }\NormalTok{mice}\OperatorTok{::}\KeywordTok{complete}\NormalTok{(tempData, }\DecValTok{1}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(completedData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Ozone          Solar.R           Wind             Temp      
##  Min.   :  1.0   Min.   :  7.0   Min.   : 1.700   Min.   :57.00  
##  1st Qu.: 20.0   1st Qu.:115.0   1st Qu.: 7.400   1st Qu.:73.00  
##  Median : 32.0   Median :212.0   Median : 9.700   Median :79.00  
##  Mean   : 42.5   Mean   :187.9   Mean   : 9.931   Mean   :78.14  
##  3rd Qu.: 59.0   3rd Qu.:259.0   3rd Qu.:11.500   3rd Qu.:85.00  
##  Max.   :168.0   Max.   :334.0   Max.   :20.700   Max.   :97.00  
##      Month            Day      
##  Min.   :5.000   Min.   : 1.0  
##  1st Qu.:6.000   1st Qu.: 8.0  
##  Median :7.000   Median :16.0  
##  Mean   :6.993   Mean   :15.8  
##  3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :9.000   Max.   :31.0
\end{verbatim}

Za pomoc funkcji pakietu \texttt{mice} mo偶emy r贸wnie偶 przedstawi graficznie gdzie i jak zostay uzupenione dane.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{densityplot}\NormalTok{(tempData, }\OperatorTok{~}\NormalTok{Ozone}\OperatorTok{+}\NormalTok{Solar.R}\OperatorTok{+}\NormalTok{Wind}\OperatorTok{+}\NormalTok{Temp)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-9-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{stripplot}\NormalTok{(tempData, Ozone}\OperatorTok{+}\NormalTok{Solar.R}\OperatorTok{+}\NormalTok{Wind}\OperatorTok{+}\NormalTok{Temp}\OperatorTok{~}\NormalTok{.imp, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{1.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-9-2.pdf}

\hypertarget{podzia-metod-data-mining}{%
\chapter{Podzia metod data mining}\label{podzia-metod-data-mining}}

\hypertarget{rodzaje-wnioskowania}{%
\section{Rodzaje wnioskowania}\label{rodzaje-wnioskowania}}

\emph{Data mining} to zestaw metod pozyskiwania wiedzy na podstawie danych. Ow wiedz zdobywamy w procesie wnioskowania na podstawie modeli. Wnioskowanie mo偶emy podzieli na dedukcyjne i indukcyjne. I tak z wnioskowaniem dedukcyjnym mamy do czynienia w贸wczas, gdy na podstawie obecnego stanu wiedzy potrafimy odpowiedzie na postawione pytanie dotyczce nowej wiedzy, stosujc reguy wnioskowania. O wnioskowaniem indukcyjnym powiemy, 偶e jest to metoda pozyskiwania wiedzy na podstawie informacji ze zbioru uczcego. Znajduje ono szerokie zastosowanie w data mining i charakteryzuje si omylnoci, poniewa偶 nawet najlepiej nauczony model na zbiorze uczcym nie zapewnia nam prawdziwoci odpowiedzi w przypadku nowych danych, a jedynie je uprawdopodabnia. Esencj wnioskowania indukcyjnego w zakresie data mining, jest poszukiwanie na podstawie danych uczcych modelu charakteryzujcego si najlepszymi waciwociami predykcyjnymi i dajcego si zastosowa do zupenie nowego zbioru danych.

Ka偶dy proces uczenia z wykorzystaniem wnioskowania indukcyjnego skada si z nastpujcych element贸w.

\hypertarget{dziedzina}{%
\subsection{Dziedzina}\label{dziedzina}}

\emph{Dziedzina} to zbi贸r wszystkich obiekt贸w pozostajcych w zainteresowaniu badacza, bdcych przedmiotem wnioskowania, oznaczana najczciej przez \(X\). Przykadowo mog to by zbiory os贸b, transakcji, urzdze, instytucji, itp.

\hypertarget{obserwacja}{%
\subsection{Obserwacja}\label{obserwacja}}

Ka偶dy element dziedziny \(x\in X\) nazywamy obserwacj. Obserwacj nazywa bdziemy zar贸wno rekordy danych ze zbioru uczcego, jak i ze zbioru testowego.

\hypertarget{atrybuty-obserwacji}{%
\subsection{Atrybuty obserwacji}\label{atrybuty-obserwacji}}

Ka偶dy obiekt z dziedziny \(x\in X\) mo偶na opisa zestawem cech (atrybut贸w), kt贸re w notacji matematycznej oznaczymy przez \(a:X\to A\), gdzie \(A\) jest przestrzeni wartoci atrybut贸w. Ka偶da obserwacja \(x\) posiadajca \(k\) cech da si wyrazi wektorowo jako \((a_1(x), a_2(x), \ldots, a_k(x))\). Dla wikszoci algorytm贸w uczenia maszynowego wyr贸偶nia si trzy typy atrybut贸w:

\begin{itemize}
\tightlist
\item
  \emph{nominalne} - posiadajce skoczon liczb stan贸w, kt贸re posiadaj porzdku;
\item
  \emph{porzdkowe} - posiadajce skoczon liczb stan贸w z zachowaniem porzdku;
\item
  \emph{cige} - przyjmujce wartoci numeryczne.
\end{itemize}

Czsto jeden z atrybut贸w spenia specjaln rol, poniewa偶 stanowi realizacj cechy, kt贸r traktujemy jako wyjciow (ang. \emph{target value attribute}). W tym przypadku powiemy o \textbf{nadzorowanym uczeniu maszynowym}. Jeli zmiennej wyjciowej nie ma dziedzinie, to m贸wimy o \textbf{nienadzorowanym uczeniu maszynowym}.

\hypertarget{zbior-uczacy}{%
\subsection{Zbi贸r uczcy}\label{zbior-uczacy}}

Zbiorem uczcym \(T\) (ang. \emph{training set}) nazywamy podzbi贸r \(D\) dziedziny \(X\) (czyli \(T\subseteq D\subseteq X\)), gdzie zbi贸r \(D\) stanowi og贸 dostpnych obserwacji z dziedziny \(X\). Zbi贸r uczcy zawiera informacje dotyczce badanego zjawiska, na podstawie kt贸rych, dokonuje si doboru modelu, selekcji cech istotnych z punktu widzenia wasnoci predykcyjnych lub jakoci klasyfikacji, budowy modelu oraz optymalizacji jego parametr贸w. W przypadku uczenia z nauczycielem (nadzorowanego) zbi贸r \(T\) zawiera informacj o wartociach atrybut贸w zmiennej wynikowej.

\hypertarget{zbior-testowy}{%
\subsection{Zbi贸r testowy}\label{zbior-testowy}}

Zbi贸r testowy \(T'\) (ang. \emph{test set}) bdcy dopenieniem zbioru uczcego do zbioru \(D\), czyli \(T'=D\setminus T\), stanowi zestaw danych su偶cy do oceny poprawnoci modelu nadzorowanego. W przypadku metod nienadzorowanych raczej nie stosuje si zbior贸w testowych.

\hypertarget{model}{%
\subsection{Model}\label{model}}

Model to narzdzie pozyskiwania wiedzy na podstawie zbioru uczcego. Nauczony model jest zbiorem regu \(f\), kt贸rego zadaniem jest oszacowanie wielkoci wartoci wynikowej lub odpowiednia klasyfikacja obiekt贸w. W zadaniu grupowania obiekt贸w (ang. \emph{clustering task}), celem modelu jest podanie grup mo偶liwie najbardziej jednorodnych przy zadanym zestawie zmiennych oraz ustalonej liczbie skupie (czasami wyznaczenie liczby skupie jest r贸wnie偶 czci zadania stawianego przed modelem).

\hypertarget{jakosc-dopasowania-modelu}{%
\subsection{Jako dopasowania modelu}\label{jakosc-dopasowania-modelu}}

Do oceny jakoci dopasowania modelu wykorzystuje si, w zale偶noci od zadania, wiele wsp贸czynnik贸w (np. dla zada regresyjnych s to bd rednio-kwadratowy - ang. \emph{Mean Square Error} a dla zada klasyfikacyjnych - trafno - ang. \emph{Accuracy}). Mo偶emy m贸wi dw贸ch rodzajach dopasowania modeli:

\begin{itemize}
\tightlist
\item
  poziom dopasowania na zbiorze uczcym
\item
  poziom dopasowania na zbiorze testowym (oczywicie z punktu widzenia utylitarnoci modelu ten wsp贸czynnik jest wa偶niejszy).
\end{itemize}

W sytuacji, w kt贸rej model wykazuje dobre charakterystyki jakoci dopasowania na zbiorze uczcym ale sabe na testowym, m贸wimy o zjawisku przeuczenia modelu (ang. \emph{overfitting}). Oznacza to, 偶e model wskazuje predykcj poprawnie jedynie dla zbioru treningowego ale ma saba wasnoci generalizacyjne nowe przypadki danych. Takie model nie przedstawiaj znaczcej wartoci w odkrywaniu wiedzy w spos贸b indukcyjny.

Z drugiej strony parametry dopasowania modelu mog pokazywa sabe dopasowanie, zar贸wno na zbiorze uczcym, jak i testowym. W贸wczas r贸wnie偶 model nie jest u偶yteczny w pozyskiwaniu wiedzy na temat badanego zjawiska, a sytuacj tak nazywamy niedouczeniem (ang. \emph{underfitting}).

\begin{figure}
\centering
\includegraphics{images/unde_over_fitting.JPG}
\caption{\label{fig:unnamed-chunk-10}Przykady niedoucznia (wykresy 1 i 4), poprawego modelu (2 i 5) i przeuczenia (3 i 6). Pierwszy wiersz wykres贸w pokazuje klasyfikacj na podstawie modelu na zbiorze uczcym, a drugi na zbiorze testowym. Wykres na dole pokazuje zwizek pomidzy zo偶onoci modelu a wielkoci bdu predykcji. \emph{殴r贸do}: \url{https://cambridgecoding.wordpress.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/}}
\end{figure}

\hypertarget{modele-regresyjne}{%
\section{Modele regresyjne}\label{modele-regresyjne}}

Jednym z rodzaj贸w zada bazujcym na wnioskowaniu indukcyjnym jest model regresyjny. Nale偶y on do grupy metod nadzorowanych, kt贸rych celem jest oszacowanie wartoci cechy wyjciowej (kt贸ra jest ilociowa) na podstawie zestawu predyktor贸w, kt贸re mog by ilociowe i jakociowe. Uczenie takich modeli odbywa si poprzez optymalizacj funkcji celu (np. \(MSE\)) na podstawie zbioru uczcego.

\hypertarget{modele-klasyfikacyjne}{%
\section{Modele klasyfikacyjne}\label{modele-klasyfikacyjne}}

Podobnie jak modele regresyjne, modele klasyfikacyjne nale偶 do grupy metod nadzorowanego uczenia maszynowego. Ich zadaniem jest waciwa klasyfikacja obiekt贸w na podstawie wielkoci predyktor贸w. Odpowiedzi modelu jest zawsze cecha typu jakociowego, natomiast predyktory mog mie dowolny typ. Wyr贸偶nia si klasyfikacj dwu i wielostanow. Lista modeli realizujcych klasyfikacj binarn jest nieco du偶sza ni偶 w przypadku modeli z wielostanow cech wynikow. Proces uczenia modelu klasyfikacyjnego r贸wnie偶 opiera si na optymalizacji funkcji celu. Tym razem s to zupenie inne miary jakoci dopasowania (np. trafno, czyli odsetek poprawnych klasyfikacji).

\hypertarget{modele-grupujace}{%
\section{Modele grupujce}\label{modele-grupujace}}

Bardzo szerok gam modeli nienadzorowanych stanowi metody analizy skupie. Ich zadaniem jest grupowanie obiekt贸w w mo偶liwie najbardziej jednorodne grupy, na podstawie wartoci atrybut贸w poddanych analizie. Poniewa偶 s to metody ``bez nauczyciela'', to ocena ich przydatnoci ma nieco inny charakter i cho istniej r贸偶ne wska藕niki jakoci grupowania, to trudno tu o obiektywne wskazanie najlepszego rozwizania.

\hypertarget{drzewa-decyzyjne}{%
\chapter{Drzewa decyzyjne}\label{drzewa-decyzyjne}}

\emph{Drzewo decyzyjne}\footnote{wygldem przypomina odwr贸cone drzewo, std nazwa} jest struktur hierarchiczn przedstawiajc model klasyfikacyjny lub regresyjny. Stosowane s szczeg贸lnie czsto w贸wczas, gdy funkcyjna posta zwizku pomidzy predyktorami a zmienn wynikow jest nieznana lub ci偶ka do ustalenia.
Ka偶de drzewo decyzyjne skada si z korzenia (ang. \emph{root}), wz贸w (ang. \emph{nodes}) i lici (ang. \emph{leaves}). Korzeniem nazywamy pocztkowy wze drzewa, z kt贸rego poprzez podziay (ang. \emph{splits}) powstaj kolejne wzy potomne. Kocowe wzy, kt贸re nie podlegaj podziaom nazywamy limi, a linie czce wzy nazywamy gaziami (ang. \emph{branches}).

Jeli drzewo su偶y do zada klasyfikacyjnych, to licie zawieraj informacj o tym, kt贸ra klasa w danym cigu podzia贸w jest najbardziej prawdopodobna. Natomiast jeli drzewo jest regresyjne, to licie zawieraj warunkowe miary tendencji centralnej (najczciej redni) wartoci zmiennej wynikowej. Warunek stanowi szereg podzia贸w doprowadzajcy do danego wza terminalnego (licia). W obu przypadkach (klasyfikacji i regresji) drzewo ``d偶y'' do takiego podziau by kolejne wzy, a co za tym idzie r贸wnie偶 licie, byy ja najbardziej jednorodne ze wzgldu na zmienn wynikow.

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-11-1.pdf}
\caption{\label{fig:unnamed-chunk-11}Przykad dziaania drzewa regresyjnego. Wykes w lewym g贸rnym rogu pokazuje prawdziw zale偶no, wyres po prawej stronie jest ilustracj drzewa decyzyjnego, a wykres w lewym dolnym rogu pokazuje dyskretyzacj przestrzeni dokonan przez drzewo, a za razem spos贸b jego dziaania.}
\end{figure}

\hypertarget{wezy-i-gaezie}{%
\section{Wzy i gazie}\label{wezy-i-gaezie}}

Ka偶dy podzia rozdziela dziedzin \(X\) na dwa lub wicej podobszar贸w dziedziny i w贸wczas ka偶da obserwacja wza nadrzdnego jest przyporzdkowana wzom potomnym. Ka偶dy odchodzcy wze potomny jest poczony gazi, kt贸ra to wi偶e si cile z mo偶liwymi wynikami podziau. Ka偶dy \(\mathbf{n}\)-ty wze mo偶na opisa jako podzbi贸r dziedziny w nastpujcy spos贸b
\begin{equation}
    X_{\mathbf{n}}=\{x\in X|t_1(x)=r_1,t_2(x)=r_2,\ldots,t_k(x)=r_k\},
\end{equation}
gdzie \(t_1,t_2,\ldots,t_k\) s podziaami, kt贸re przeprowadzaj \(x\) w obszary \(r_1, r_2,\ldots, r_k\). Przez
\begin{equation}
    S_{\mathbf{n}, t=r}=\{x\in S|t(x)=r\}
\end{equation}
rozumiemy, 偶e dokonano takiego cigu podzia贸w zbioru \(S\), 偶e jego wartoci znalazy si w \(\mathbf{n}\)-tym w藕le.

\hypertarget{rodzaje-regu-podziau}{%
\section{Rodzaje regu podziau}\label{rodzaje-regu-podziau}}

Najczciej wystpujce reguy podziau w drzewach decyzyjnych s jednowymiarowe, czyli warunek podziau jest generowany na podstawie jednego atrybutu. Istniej podziay wielowymiarowe ale ze wzgldu na zo偶ono obliczeniow s rzadziej stosowane.

\hypertarget{podziay-dla-atrybutow-ze-skali-nominalnej}{%
\subsection{Podziay dla atrybut贸w ze skali nominalnej}\label{podziay-dla-atrybutow-ze-skali-nominalnej}}

Istniej dwa typy regu podziau dla skali nominalnej:

\begin{itemize}
\tightlist
\item
  oparte na wartoci atrybutu (ang. \emph{value based}) - w贸wczas funkcja testowa przyjmuje posta \(t(x)=a(x)\), czyli podzia generuj wartoci atrybutu;
\item
  oparte na r贸wnoci (ang. \emph{equality based}) - gdzie funkcja testowa jest zdefiniowana jako
  \begin{equation}
    t(x)= \begin{cases}
        1, &\text{ gdy } a(x)=\nu\\
        0, & \text{ w przeciwnym przypadku},
    \end{cases}
  \end{equation}
  gdzie \(\nu\in A\) i \(A\) jest zbiorem mo偶liwych wartoci \(a\). W tym przypadku podzia jest dychotomiczny, albo obiekt ma warto atrybutu r贸wn \(\nu\), albo go nie ma.
\end{itemize}

\hypertarget{podziay-dla-atrybutow-ze-skali-ciagej}{%
\subsection{Podziay dla atrybut贸w ze skali cigej}\label{podziay-dla-atrybutow-ze-skali-ciagej}}

Reguy podziau stosowane do skali cigej, to:

\begin{itemize}
\tightlist
\item
  oparta na nier贸wnociach (ang. \emph{inequality based}) - zdefiniowana jako
  \begin{equation}
  t(x) = \begin{cases}
    1, &\text{ gdy }a(x)\leq \nu\\
    0, & \text{w przeciwnym przypadku}
    \end{cases}
  \end{equation}
  gdzie \(\nu\in A\);
\item
  przedziaowa (ang. \emph{interval based}) - zdefiniowana jako
  \begin{equation}
    t(x) = \begin{cases}
        1, &\text{ gdy }a(x) \in I_1\\
        2, &\text{ gdy }a(x) \in I_2\\
        \vdots & \\
        k, &\text{ gdy }a(x) \in I_k\\
    \end{cases}
  \end{equation}
  gdzie \(I_1,I_2,\ldots,I_k\subset A\) stanowi rozczny podzia (przedziaami) przeciwdziedziny \(A\).
\end{itemize}

\hypertarget{podziay-dla-atrybutow-ze-skali-porzadkowej}{%
\subsection{Podziay dla atrybut贸w ze skali porzdkowej}\label{podziay-dla-atrybutow-ze-skali-porzadkowej}}

Podziay te mog wykorzystywa oba wczeniej wspomniane typy, w zale偶noci od potrzeb.

\hypertarget{algorytm-budowy-drzewa}{%
\section{Algorytm budowy drzewa}\label{algorytm-budowy-drzewa}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  stw贸rz pocztkowy wze (korze) i oznacz go jako \emph{otwarty};
\item
  przypisz wszystkie mo偶liwe rekordy do wza pocztkowego;
\item
  \textbf{dop贸ki} istniej otwarte wzy \textbf{wykonuj}:

  \begin{itemize}
  \tightlist
  \item
    wybierz wze \(\mathbf{n}\), wyznacz potrzebne statystyki opisowe zmiennej zale偶nej dla tego wza i przypisz warto docelow;
  \item
    \textbf{jeli} kryterium zatrzymania podziau jest spenione dla wza \(n\), \textbf{to} oznacz go za \textbf{zamknity};
  \item
    \textbf{w przeciwnym przypadku} wybierz podzia \(r\) element贸w wza \(\mathbf{n}\), i dla ka偶dego podzbioru podziau stw贸rz wze ni偶szego rzdu (potomka) \(\mathbf{n}_r\) oraz oznacz go jako \emph{otwarty};
  \item
    nastpnie przypisz wszystkie przypadki generowane podziaem \(r\) do odpowiednich wz贸w potomk贸w \(\mathbf{n}_r\);
  \item
    oznacza wze \(\mathbf{n}\) jako \emph{zamknity}.
  \end{itemize}
\end{enumerate}

Spos贸b przypisywania wartoci docelowej wi偶e si cile z rodzajem drzewa. W drzewach regresyjnych chodzi o wyliczenie redniej lub mediany dla obserwacji ujtych w danym w藕le. Natomiast w przypadku drzewa klasyfikacyjnego, wyznacza si wartoci prawdopodobiestw przynale偶noci obserwacji znajdujcej si w danym w藕le do poszczeg贸lnych klas
\begin{equation}
    \P(d|\mathbf{n})=\P_{T_\mathbf{n}}(d)=\frac{|T_\mathbf{n}^d|}{|T_\mathbf{n}|},
\end{equation}
gdzie \(T_\mathbf{n}\) oznaczaj obserwacje zbioru uczcego znajdujce si w w藕le \(\mathbf{n}\), a \(T_\mathbf{n}^d\) oznacza dodatkowo podzbi贸r zbioru uczcego w \(\mathbf{n}\) w藕le, kt贸re nale偶 do klasy \(d\). Oczywicie klasyfikacja na podstawie otrzymanych prawdopodobiestw w danym w藕le jest dokonana przez wyb贸r klasy charakteryzujcej si najwy偶szym prawdopodobiestwem.

\hypertarget{kryteria-zatrzymania}{%
\section{Kryteria zatrzymania}\label{kryteria-zatrzymania}}

Kryterium zatrzymania jest warunkiem, kt贸ry decyduje o tym, 偶e dany wze uznajemy za zamknity i nie dokonujemy dalszego jego podziau. Wyr贸偶niamy nastpujce kryteria zatrzymania:

\begin{itemize}
\tightlist
\item
  jednorodno wza - w przypadku drzewa klasyfikacyjnego mo偶e zdarzy si sytuacja, 偶e wszystkie obserwacje wza bd pochodziy z jednej klasy. W贸wczas nie ma sensu dokonywa dalszego podziau wza;
\item
  wze jest pusty - zbi贸r przypisanych obserwacji zbioru uczcego do \(\mathbf{n}\)-tego wza jest pusty;
\item
  brak regu podziau - wszystkie reguy podziau zostay wykorzystane, zatem nie da si stworzy potomnych wz贸w, kt贸re charakteryzowayby si wiksz homogenicznoci;
\end{itemize}

Warunki ujte w pierwszych dw贸ch kryteriach mog by nieco zagodzone, poprzez zatrzymanie podzia贸w w贸wczas, gdy prawdopodobiestwo przynale偶enia do pewnej klasy przekroczy ustalony pr贸g lub gdy liczebno wza spadnie poni偶ej ustalonej wartoci.

W literaturze tematu istnieje jeszcze jedno czsto stosowane kryterium zatrzymania oparte na wielkoci drzewa. Wze potomny ustala si jako zamknity, gdy dugo cie偶ki dojcia do nie go przekroczy ustalon warto.

\hypertarget{reguy-podziau}{%
\section{Reguy podziau}\label{reguy-podziau}}

Wa偶nym elementem algorytmu tworzenia drzewa regresyjnego jest \emph{regua podziau}. Dobierana jest w taki spos贸b aby zmaksymalizowa zdolnoci generalizacyjne drzewa. Zo偶ono drzewa mierzona jest najczciej przecitn liczb podzia贸w potrzebnych do dotarcia do licia zaczynajc od korzenia. Licie s najczciej tworzone w贸wczas gdy dyspersja wartoci wynikowej jest stosunkowo maa lub wze zawiera w miar homogeniczne obserwacje ze wzgldu na przynale偶no do klasy zmiennej wynikowej. W przypadku drzew regresyjnych zmienno na poziomie wz贸w jest dobr miar su偶c do definiowania podziau w w藕le. I tak, jeli pewien podzia generuje nam stosunkowo mae dyspersje wartoci docelowych w wzach potomnych, to mo偶na ten podzia uzna za waciwy. Jeli \(T_n\) oznacza zbi贸r rekord贸w nale偶cych do wza \(n\), a \(T_{n,t=r}\) s podzbiorami generowanymi przez podzia \(r\) w wzach potomnych dla \(n\), to dyspersj wartoci docelowej \(f\) bdziemy oznaczali nastpujco
\begin{equation}\label{dyspersja}
     \operatorname{disp}_{T_{n,t=r}}(f).
\end{equation}

Regu podziau mo偶emy okrela poprzez minimalizacj redniej wa偶onej dyspersji wartoci docelowej nastpujcej postaci
\begin{equation}\label{reg_podz}
        \operatorname{disp}_n(f|t)=\sum_{r\in R_t}\frac{|T_{n,t=r}|}{|T_n|}\operatorname{disp}_{T_{n,t=r}}(f),
\end{equation}
gdzie \(|\  |\) oznacza moc zbioru, a \(R_t\) zbi贸r wszystkich mo偶liwych wartoci reguy podziau. Czasami wygodniej bdzie maksymalizowa przyrost dyspersji (lub spadek)
\begin{equation}\label{przyrost}
        \bigtriangleup \operatorname{disp}_n(f|t)=\operatorname{disp}_n(f)-\sum_{r\in R_t}\frac{|T_{n,t=r}|}{|T_n|}\operatorname{disp}_{T_{n,t=r}}(f).
\end{equation}

Miar heterogenicznoci wz贸w ze wzgldu na zmienn wynikow (ang. \emph{impurity}) w drzewach klasyfikacyjnych, kt贸ra pozwala na tworzenie kolejnych podzia贸w wza, s najczciej wska藕nik Gini'ego i entropia \citep{Breiman1984}.

Entropi podzbioru uczcego w w藕le \(\mathbf{n}\), wyznaczamy wg wzoru
\begin{equation}
E_{T_{\mathbf{n}}}(c|t) = \sum_{x\in R_t} \frac{|T_{\mathbf{n}, t=r}|}{|T_{\mathbf{n}}|}E_{T_{\mathbf{n}, t=r}}(c),
\end{equation}
gdzie \(t\) jest podziaem (kandydatem), \(r\) potencjalnym wynikiem podziau \(t\), \(c\) jest oznaczeniem klasy zmiennej wynikowej, a
\begin{equation}
    E_{T_{\mathbf{n}, t=r}}(c) = \sum_{d\in C}-\P_{T_{\mathbf{n}, t=r}}(c=d)\log\P_{T_{\mathbf{n}, t=r}}(c=d),
\end{equation}
przy czym
\begin{equation}
    \P_{T_{\mathbf{n}, t=r}}(c=d)= \P_{T_{\mathbf{n}}}(c=d|t=r).
\end{equation}

Podobnie definiuje si indeks Gini'ego
\begin{equation}
Gi_{T_{\mathbf{n}}}(c|t) = \sum_{x\in R_t} \frac{|T_{\mathbf{n}, t=r}|}{|T_{\mathbf{n}}|}Gi_{T_{\mathbf{n}, t=r}}(c),
\end{equation}
gdzie
\begin{equation}
    Gi_{T_{\mathbf{n}, t=r}}(c) = \sum_{d\in C}\P_{T_{\mathbf{n}, t=r}}(c=d)\cdot(1-\P_{T_{\mathbf{n}, t=r}}(c=d))= 1-\sum_{d\in C}\P^2_{T_{\mathbf{n}, t=r}}(c=d).
\end{equation}
Dla tak zdefiniowanych miar ``nieczystoci'' wz贸w, podziau dokonujemy w taki spos贸b, aby zminimalizowa wsp贸czynnik Gini'ego lub entropi. Im ni偶sze miary nieczystoci, tym bardziej obserwacje znajdujce si w w藕le s monokultur\footnote{prawie wszystkie s w jednej klasie}. Nierzadko korzysta si r贸wnie偶 z wsp贸czynnika przyrostu informacji (ang. \emph{information gain})
\begin{equation}
    \Delta E_{T_{\mathbf{n}}}(c|t)=E_{T_{\mathbf{n}}}(c)-E_{T_{\mathbf{n}}}(c|t).
\end{equation}
Istnieje r贸wnie偶 jego odpowiednik dla indeksu Gini'ego. W obu przypadkach optymalnego podziau szukamy poprzez maksymalizacj przyrostu informacji.

\hypertarget{przycinanie-drzewa-decyzyjnego}{%
\section{Przycinanie drzewa decyzyjnego}\label{przycinanie-drzewa-decyzyjnego}}

Uczenie drzewa decyzyjnego wi偶e si z ryzykiem przeuczenia modelu (podobnie jak to si ma w przypadku innych modeli predykcyjnych). Wczeniej przytoczone reguy zatrzymania (np. gboko drzewa czy zatrzymanie przy osigniciu jednorodnoci na zadanym poziomie) pomagaj kontrolowa poziom generalizacji drzewa ale czasami bdzie dodatkowo potrzebne przycicie drzewa, czyli usunicie pewnych podzia贸w, a co za tym idzie, r贸wnie偶 lici (wz贸w).

\hypertarget{przycinanie-redukujace-bad}{%
\subsection{Przycinanie redukujce bd}\label{przycinanie-redukujace-bad}}

Jedn ze strategii przycinania drzewa jest przycinanie redukujce bd (ang. \emph{reduced error pruning}). Polega ono na por贸wnaniu bd贸w (najczciej u偶ywana jest miara odsetka bdnych klasyfikacji lub MSE) licia \(\mathbf{l}\) i wza do kt贸rego drzewo przycinamy \(\mathbf{n}\) na cakiem nowym zbiorze uczcym \(R\). Niech \(e_R(\mathbf{l})\) i \(e_R(\mathbf{n})\) oznaczaj odpowiednio bdy na zbiorze \(R\) licia i wza. Przez bd wza rozumiemy bd pod-drzewa o korzeniu \(\mathbf{n}\). W贸wczas jeli zachodzi warunek
\begin{equation}
    e_R(\mathbf{l})\leq e_R(\mathbf{n}), 
\end{equation}
to zaleca si zastpi wze \(\mathbf{n}\) liciem \(\mathbf{l}\).

\hypertarget{przycinanie-minimalizujace-bad}{%
\subsection{Przycinanie minimalizujce bd}\label{przycinanie-minimalizujace-bad}}

Przycinanie minimalizujce bd opiera si na spostrze偶eniu, 偶e bd drzewa przycitego charakteryzuje si zbyt pesymistyczn ocen i dlatego wymaga korekty. Wze drzewa klasyfikacyjnego \(\mathbf{n}\) zastpujemy liciem \(\mathbf{l}\), jeli
\begin{equation}
    \hat{e}_T(\mathbf{l})\leq \hat{e}_T(\mathbf{n}),
\end{equation}
gdzie
\begin{equation}
    \hat{e}_T(\mathbf{n})=\sum_{\mathbf{n}'\in N(\mathbf{n})}\frac{|T_{\mathbf{n}'}|}{|T_\mathbf{n}|}\hat{e}_T(\mathbf{n}'),
\end{equation}
a \(N(\mathbf{n})\) jest zbiorem wszystkich mo偶liwych wz贸w potomnych wza \(\mathbf{n}\) i \begin{equation}
    \hat{e}_T(\mathbf{l})=1-\frac{|\{x\in T_\mathbf{l}|c(x)=d_{\mathbf{l}}\}|+mp}{|T_\mathbf{l}|+m},
\end{equation}
gdzie \(p\) jest prawdopodobiestwem przynale偶noci do klasy \(d_{\mathbf{l}}\) ustalona na podstawie zewntrznej wiedzy (gdy jej nie posiadamy przyjmujemy \(p=1/|C|\)).

W przypadku drzewa regresyjnego znajdujemy wiele analogii, poniewa偶 jeli dla pewnego zbioru rekord贸w \(T\) speniony jest warunek
\begin{equation}\label{kryterium1}
    \operatorname{mse}_T(\mathbf{l})\leq\operatorname{mse}_T(\mathbf{n}),
\end{equation}
gdzie \(\mathbf{l}\) i \(\mathbf{n}\) oznaczaj odpowiednio li i wze, to w贸wczas zastpujemy wze \(\mathbf{n}\) przez li \(\mathbf{l}\).

Estymatory wyznaczone na podstawie niewielkiej pr贸by, mog by obarczone znaczcym bdem. Wyliczanie bdu rednio-kwadratowego dla podzbioru nowych wartoci mo偶e si charakteryzowa takim obci偶eniem. Dlatego stosuje si statystyki opisowe z poprawk, kt贸rej pochodzenie mo偶e mie trzy 藕r贸da: wiedza merytoryczna na temat szukanej wartoci, zao偶e modelu lub na podstawie wylicze opartych o cay zbi贸r wartoci.

Skorygowany estymator bdu rednio-kwadratowego ma nastpujc posta
\begin{equation}\label{mse}
        \widehat{\operatorname{mse}}_T(\mathbf{l})=\frac{\sum_{x\in T}(f(x)-m_{\mathbf{l},m,m_0}(f))^2+mS_0^2}{|T_\mathbf{l}|+m},
\end{equation}
gdzie
\begin{equation}\label{poprawka}
        m_{\mathbf{l},m,m_0}(f)=\frac{\sum_{x\in T_\mathbf{l}}f(x)+mm_0}{|T_\mathbf{l}|+m},
\end{equation}
a \(m_0\) i \(S_0^2\) s redni i wariancj wyznaczonymi na caej pr贸bie uczcej.
Bd rednio-kwadratowy wza \(\mathbf{n}\) ma posta
\begin{equation}\label{propagacja}
        \widehat{\operatorname{mse}}_T(\mathbf{n})=\sum_{\mathbf{n}'\in N(\mathbf{n})}\frac{|T_{\mathbf{n}'}|}{|T_\mathbf{n}|}\widehat{\operatorname{mse}}_T(\mathbf{n}').
\end{equation}
W贸wczas kryterium podcicia mo偶na zapisa w nastpujcy spos贸b
\begin{equation}\label{kryterium2}
        \widehat{\operatorname{mse}}_T(\mathbf{l}) \leq \widehat{\operatorname{mse}}_T(\mathbf{n})
\end{equation}

\hypertarget{przycinanie-ze-wzgledu-na-wspoczynnik-zozonosci-drzewa}{%
\subsection{Przycinanie ze wzgldu na wsp贸czynnik zo偶onoci drzewa}\label{przycinanie-ze-wzgledu-na-wspoczynnik-zozonosci-drzewa}}

Przycinanie ze wzgldu na wsp贸czynnik zo偶onoci drzewa (ang. \emph{cost-complexity pruning}) polega na wprowadzeniu ``kary'' za zwikszon zo偶ono drzewa. Drzewa klasyfikacyjne przycinamy gdy speniony jest warunek
\begin{equation}
    e_T(\mathbf{l})\leq e_T(\mathbf{n})+\alpha C(\mathbf{n}),
\end{equation}
gdzie \(C(\mathbf{n})\) oznacza zo偶ono drzewa mierzon liczb lici, a \(\alpha\) parametrem wagi kary za zo偶ono drzewa.

Wspomniane kryterium przycicia dla drzew regresyjnych bazuje na wzgldnym bdzie rednio-kwadratowym (ang. \emph{relative square error}), czyli
\begin{equation}\label{rse}
        \widehat{\operatorname{rse}}_T(\mathbf{n})=\frac{|T|\widehat{\operatorname{mse}}_T(\mathbf{n})}{(|T|-1)S^2_T(f)},
\end{equation}
gdzie \(T\) oznacza podzbi贸r \(X\), \(S^2_T\) wariancj na zbiorze \(T\).
W贸wczas kryterium podcicia wyglda nastpujco
\begin{equation}\label{kryterium3}
    \widehat{\operatorname{rse}}_T(\mathbf{l})\leq \widehat{\operatorname{rse}}_T(\mathbf{n})+\alpha C(\mathbf{n}).
\end{equation}

\hypertarget{obsuga-brakow-danych}{%
\section{Obsuga brak贸w danych}\label{obsuga-brakow-danych}}

Drzewa decyzyjne wyjtkowo dobrze radz sobie z obsuga zbior贸w z brakami. Stosowane s g贸wnie dwie strategie:

\begin{itemize}
\tightlist
\item
  udzia贸w obserwacji (ang. \emph{fractional instances}) - rozwa偶ane s wszystkie mo偶liwe podziay dla brakujcej obserwacji i przypisywana jest im odpowiednia waga lub prawdopodobiestwo, w oparciu o zaobserwowany rozkad znanych obserwacji. Te same wagi s stosowane do predykcji wartoci na podstawie drzewa z brakami danych.
\item
  podzia贸w zastpczych (ang. \emph{surrogate splits}) - jeli wynik podziau nie mo偶e by ustalony dla obserwacji z brakami, to u偶ywany jest podzia zastpczy (pierwszy), jeli i ten nie mo偶e zosta ustalony, to stosuje si kolejny. Kolejne podziay zastpcze s generowane tak, aby wynik podziau mo偶liwie najbardziej przypomina podzia waciwy.
\end{itemize}

\hypertarget{zalety-i-wady}{%
\section{Zalety i wady}\label{zalety-i-wady}}

\hypertarget{zalety}{%
\subsection{Zalety}\label{zalety}}

\begin{itemize}
\tightlist
\item
  atwe w interpretacji;
\item
  nie wymagaj 偶mudnego przygotowania danych (brak standaryzacji, wprowadzania zmiennych binarnych, dopuszcza wystpowanie brak贸w danych);
\item
  dziaa na obu typach zmiennych - jakociowych i ilociowych;
\item
  dopuszcza nieliniowo zwizku midzy zmienn wynikow a predyktorami;
\item
  odporny na odstpstwa od zao偶e;
\item
  pozwala na obsug du偶ych zbior贸w danych.
\end{itemize}

\hypertarget{wady}{%
\subsection{Wady}\label{wady}}

\begin{itemize}
\tightlist
\item
  brak jawnej postaci zale偶noci;
\item
  zale偶no struktury drzewa od u偶ytego algorytmu;
\item
  przegrywa jakoci predykcji z innymi metodami nadzorowanego uczenia maszynowego.
\end{itemize}

\hypertarget{przyk41}{%
\section{Przykad}\label{przyk41}}

Przykadem zastosowania drzew decyzyjnych bdzie klasyfikacja irys贸w na podstawie dugoci i szerokoci kielicha i patka.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse) }
\KeywordTok{library}\NormalTok{(rpart) }\CommentTok{# pakiet do tworzenia drzew typu CART}
\KeywordTok{library}\NormalTok{(rpart.plot) }\CommentTok{# pakiet do rysowania drzew}
\end{Highlighting}
\end{Shaded}

Ka偶de zadanie ucznia maszynowego zaczynamy od czyszczenia danych i odpowiedniego ich przygotowania ale w tym przypadku skupimy si jedynie na budowie, optymalizacji i ewaluacji modelu.

\hypertarget{podzia-zbioru-na-probe-uczaca-i-testowa}{%
\subsection{Podzia zbioru na pr贸b uczc i testow}\label{podzia-zbioru-na-probe-uczaca-i-testowa}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{44}\NormalTok{)}
\NormalTok{dt.train <-}\StringTok{ }\NormalTok{iris }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{sample_frac}\NormalTok{(}\DataTypeTok{size =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{dt.test <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(iris, dt.train)}
\KeywordTok{str}\NormalTok{(dt.train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    105 obs. of  5 variables:
##  $ Sepal.Length: num  6.4 4.4 6.6 5.4 5 5.4 5.6 4.4 5.4 6.1 ...
##  $ Sepal.Width : num  2.7 3.2 3 3 3.6 3.4 2.9 2.9 3.9 2.9 ...
##  $ Petal.Length: num  5.3 1.3 4.4 4.5 1.4 1.7 3.6 1.4 1.3 4.7 ...
##  $ Petal.Width : num  1.9 0.2 1.4 1.5 0.2 0.2 1.3 0.2 0.4 1.4 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 3 1 2 2 1 1 2 1 1 2 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(dt.test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    45 obs. of  5 variables:
##  $ Sepal.Length: num  4.7 4.6 5.4 4.8 5.8 5.1 5.1 5.1 5 5.2 ...
##  $ Sepal.Width : num  3.2 3.1 3.9 3.4 4 3.8 3.7 3.3 3 3.5 ...
##  $ Petal.Length: num  1.3 1.5 1.7 1.6 1.2 1.5 1.5 1.7 1.6 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.4 0.2 0.2 0.3 0.4 0.5 0.2 0.2 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

\hypertarget{budowa-drzewa}{%
\subsection{Budowa drzewa}\label{budowa-drzewa}}

Budowy drzewa dokonujemy za pomoc funkcji \texttt{rpart} pakietu \textbf{rpart} \citep{R-rpart} stosujc zapis formuy zale偶noci. Drzewo zostanie zbudowane z uwzgldnieniem kilku kryteri贸w zatrzymania:

\begin{itemize}
\tightlist
\item
  minimalna liczebno wza, kt贸ry mo偶e zosta podzielony to 10 - ze wzgldu na ma liczebno zbioru uczcego;
\item
  minimalna liczebno licia to 5 - aby nie dopuci do przeuczenia modelu;
\item
  maksymalna gboko drzewa to 4 - aby nie dopuci do przeuczenia modelu.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.rpart <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.train, }
                   \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{10}\NormalTok{,}
                                           \DataTypeTok{minbucket =} \DecValTok{5}\NormalTok{,}
                                           \DataTypeTok{maxdepth =} \DecValTok{4}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(mod.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
##   n= 105 
## 
##           CP nsplit  rel error    xerror       xstd
## 1 0.51470588      0 1.00000000 1.1764706 0.06418173
## 2 0.41176471      1 0.48529412 0.6617647 0.07457243
## 3 0.02941176      2 0.07352941 0.1029412 0.03758880
## 4 0.01000000      3 0.04411765 0.1029412 0.03758880
## 
## Variable importance
##  Petal.Width Petal.Length Sepal.Length  Sepal.Width 
##           35           33           21           12 
## 
## Node number 1: 105 observations,    complexity param=0.5147059
##   predicted class=setosa      expected loss=0.647619  P(node) =1
##     class counts:    37    33    35
##    probabilities: 0.352 0.314 0.333 
##   left son=2 (37 obs) right son=3 (68 obs)
##   Primary splits:
##       Petal.Length < 2.45 to the left,  improve=35.95322, (0 missing)
##       Petal.Width  < 0.8  to the left,  improve=35.95322, (0 missing)
##       Sepal.Length < 5.45 to the left,  improve=25.39467, (0 missing)
##       Sepal.Width  < 3.35 to the right, improve=12.69596, (0 missing)
##   Surrogate splits:
##       Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.000, (0 split)
##       Sepal.Length < 5.45 to the left,  agree=0.924, adj=0.784, (0 split)
##       Sepal.Width  < 3.35 to the right, agree=0.819, adj=0.486, (0 split)
## 
## Node number 2: 37 observations
##   predicted class=setosa      expected loss=0  P(node) =0.352381
##     class counts:    37     0     0
##    probabilities: 1.000 0.000 0.000 
## 
## Node number 3: 68 observations,    complexity param=0.4117647
##   predicted class=virginica   expected loss=0.4852941  P(node) =0.647619
##     class counts:     0    33    35
##    probabilities: 0.000 0.485 0.515 
##   left son=6 (38 obs) right son=7 (30 obs)
##   Primary splits:
##       Petal.Width  < 1.75 to the left,  improve=25.286380, (0 missing)
##       Petal.Length < 4.75 to the left,  improve=24.879360, (0 missing)
##       Sepal.Length < 5.75 to the left,  improve= 6.713875, (0 missing)
##       Sepal.Width  < 3.25 to the left,  improve= 1.336180, (0 missing)
##   Surrogate splits:
##       Petal.Length < 4.75 to the left,  agree=0.882, adj=0.733, (0 split)
##       Sepal.Length < 6.15 to the left,  agree=0.721, adj=0.367, (0 split)
##       Sepal.Width  < 3.15 to the left,  agree=0.618, adj=0.133, (0 split)
## 
## Node number 6: 38 observations,    complexity param=0.02941176
##   predicted class=versicolor  expected loss=0.1315789  P(node) =0.3619048
##     class counts:     0    33     5
##    probabilities: 0.000 0.868 0.132 
##   left son=12 (32 obs) right son=13 (6 obs)
##   Primary splits:
##       Petal.Length < 4.95 to the left,  improve=4.0800440, (0 missing)
##       Petal.Width  < 1.45 to the left,  improve=1.2257490, (0 missing)
##       Sepal.Width  < 2.65 to the right, improve=0.6168705, (0 missing)
##       Sepal.Length < 5.95 to the left,  improve=0.4736842, (0 missing)
##   Surrogate splits:
##       Petal.Width < 1.55 to the left,  agree=0.868, adj=0.167, (0 split)
## 
## Node number 7: 30 observations
##   predicted class=virginica   expected loss=0  P(node) =0.2857143
##     class counts:     0     0    30
##    probabilities: 0.000 0.000 1.000 
## 
## Node number 12: 32 observations
##   predicted class=versicolor  expected loss=0.03125  P(node) =0.3047619
##     class counts:     0    31     1
##    probabilities: 0.000 0.969 0.031 
## 
## Node number 13: 6 observations
##   predicted class=virginica   expected loss=0.3333333  P(node) =0.05714286
##     class counts:     0     2     4
##    probabilities: 0.000 0.333 0.667
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(mod.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-15-1.pdf}
\caption{\label{fig:unnamed-chunk-15}Obraz drzewa klasyfikacyjnego.}
\end{figure}

Powy偶szy wykres przedstawia struktur drzewa klasyfikacyjnego. Kolorami s oznaczone klasy, kt贸re w danym w藕le dominuj. Nasycenie barwy decyduje o sile tej dominacji. W ka偶dym w藕le podana jest klasa, do kt贸rej najprawdopodobniej nale偶 jego obserwacje. Ponadto podane s proporcje przynale偶noci do klas zmiennej wynikowej oraz procent obserwacji zbioru uczcego nale偶cych do danego wza. Pod ka偶dym wzem podana jest regua podziau.

\hypertarget{przycinanie-drzewa}{%
\subsection{Przycinanie drzewa}\label{przycinanie-drzewa}}

Zanim przystpimy do przycinania drzewa nale偶y sprawdzi, jakie s zdolnoci generalizacyjne modelu. Oceny tej dokonujemy najczciej sprawdzajc macierz klasyfikacji.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.rpart, }
                     \DataTypeTok{newdata =}\NormalTok{ dt.test)}
\NormalTok{pred.prob[}\DecValTok{10}\OperatorTok{:}\DecValTok{20}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    setosa versicolor virginica
## 10      1    0.00000   0.00000
## 11      1    0.00000   0.00000
## 12      1    0.00000   0.00000
## 13      1    0.00000   0.00000
## 14      0    0.96875   0.03125
## 15      0    0.96875   0.03125
## 16      0    0.96875   0.03125
## 17      0    0.96875   0.03125
## 18      0    0.96875   0.03125
## 19      0    0.96875   0.03125
## 20      0    0.00000   1.00000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.class <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.rpart, }
                      \DataTypeTok{newdata =}\NormalTok{ dt.test,}
                      \DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{pred.class}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          1          2          3          4          5          6 
##     setosa     setosa     setosa     setosa     setosa     setosa 
##          7          8          9         10         11         12 
##     setosa     setosa     setosa     setosa     setosa     setosa 
##         13         14         15         16         17         18 
##     setosa versicolor versicolor versicolor versicolor versicolor 
##         19         20         21         22         23         24 
## versicolor  virginica versicolor versicolor versicolor versicolor 
##         25         26         27         28         29         30 
## versicolor versicolor versicolor versicolor versicolor versicolor 
##         31         32         33         34         35         36 
##  virginica  virginica  virginica  virginica  virginica  virginica 
##         37         38         39         40         41         42 
##  virginica  virginica  virginica  virginica  virginica  virginica 
##         43         44         45 
##  virginica  virginica  virginica 
## Levels: setosa versicolor virginica
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred.class, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15
\end{verbatim}

Jak wida z powy偶szej tabeli, model cakiem dobrze radzi sobie z poprawn klasyfikacj obserwacji do odpowiednich kategorii. Tylko jedna obserwacja zostaa bdnie zaklasyfikowana.

W dalszej kolejnoci sprawdzimy, czy nie jest konieczne przycicie drzewa. Jednym z kryteri贸w przycinania drzewa jest przycinanie ze wzgldu na zo偶ono drzewa. W tym przypadku jest wyra偶ony parametrem \texttt{cp}. Istnieje powszechnie stosowana regua jednego odchylenia standardowego, kt贸ra m贸wi, 偶e drzewo nale偶y przyci w贸wczas, gdy bd oszacowany na podstawie sprawdzianu krzy偶owego (\texttt{xerror}), pierwszy raz zejdzie poni偶ej poziomu wyznaczonego przez najni偶sz warto bdu powikszonego o odchylenie standardowe tego bdu (\texttt{xstd}). Na podstawie poni偶szej tabeli mo偶na ustali, 偶e poziomem odcicia jest warto \(0.10294+0.037589=0.140529\). Pierwszy raz bd przyjmuje warto mniejsz od \(0.140529\) po drugim podziale (\texttt{nsplit=2}). Temu poziomowi odpowiada \texttt{cp} o wartoci \(0.029412\) i to jest zo偶ono drzewa, kt贸r powinnimy przyj do przycicia drzewa.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{printcp}\NormalTok{(mod.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Classification tree:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
## 
## Variables actually used in tree construction:
## [1] Petal.Length Petal.Width 
## 
## Root node error: 68/105 = 0.64762
## 
## n= 105 
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.514706      0  1.000000 1.17647 0.064182
## 2 0.411765      1  0.485294 0.66176 0.074572
## 3 0.029412      2  0.073529 0.10294 0.037589
## 4 0.010000      3  0.044118 0.10294 0.037589
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotcp}\NormalTok{(mod.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-17-1.pdf}
\caption{\label{fig:unnamed-chunk-17}Na wykresie bd贸w punkt odcicia zaznaczony jest lini przerywan}
\end{figure}

Przycite drzewo wyglda nastpujco:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.rpart2 <-}\StringTok{ }\KeywordTok{prune}\NormalTok{(mod.rpart, }\DataTypeTok{cp =} \FloatTok{0.029412}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(mod.rpart2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
##   n= 105 
## 
##          CP nsplit  rel error    xerror       xstd
## 1 0.5147059      0 1.00000000 1.1764706 0.06418173
## 2 0.4117647      1 0.48529412 0.6617647 0.07457243
## 3 0.0294120      2 0.07352941 0.1029412 0.03758880
## 
## Variable importance
##  Petal.Width Petal.Length Sepal.Length  Sepal.Width 
##           35           31           22           12 
## 
## Node number 1: 105 observations,    complexity param=0.5147059
##   predicted class=setosa      expected loss=0.647619  P(node) =1
##     class counts:    37    33    35
##    probabilities: 0.352 0.314 0.333 
##   left son=2 (37 obs) right son=3 (68 obs)
##   Primary splits:
##       Petal.Length < 2.45 to the left,  improve=35.95322, (0 missing)
##       Petal.Width  < 0.8  to the left,  improve=35.95322, (0 missing)
##       Sepal.Length < 5.45 to the left,  improve=25.39467, (0 missing)
##       Sepal.Width  < 3.35 to the right, improve=12.69596, (0 missing)
##   Surrogate splits:
##       Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.000, (0 split)
##       Sepal.Length < 5.45 to the left,  agree=0.924, adj=0.784, (0 split)
##       Sepal.Width  < 3.35 to the right, agree=0.819, adj=0.486, (0 split)
## 
## Node number 2: 37 observations
##   predicted class=setosa      expected loss=0  P(node) =0.352381
##     class counts:    37     0     0
##    probabilities: 1.000 0.000 0.000 
## 
## Node number 3: 68 observations,    complexity param=0.4117647
##   predicted class=virginica   expected loss=0.4852941  P(node) =0.647619
##     class counts:     0    33    35
##    probabilities: 0.000 0.485 0.515 
##   left son=6 (38 obs) right son=7 (30 obs)
##   Primary splits:
##       Petal.Width  < 1.75 to the left,  improve=25.286380, (0 missing)
##       Petal.Length < 4.75 to the left,  improve=24.879360, (0 missing)
##       Sepal.Length < 5.75 to the left,  improve= 6.713875, (0 missing)
##       Sepal.Width  < 3.25 to the left,  improve= 1.336180, (0 missing)
##   Surrogate splits:
##       Petal.Length < 4.75 to the left,  agree=0.882, adj=0.733, (0 split)
##       Sepal.Length < 6.15 to the left,  agree=0.721, adj=0.367, (0 split)
##       Sepal.Width  < 3.15 to the left,  agree=0.618, adj=0.133, (0 split)
## 
## Node number 6: 38 observations
##   predicted class=versicolor  expected loss=0.1315789  P(node) =0.3619048
##     class counts:     0    33     5
##    probabilities: 0.000 0.868 0.132 
## 
## Node number 7: 30 observations
##   predicted class=virginica   expected loss=0  P(node) =0.2857143
##     class counts:     0     0    30
##    probabilities: 0.000 0.000 1.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(mod.rpart2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-18-1.pdf}
\caption{\label{fig:unnamed-chunk-18}Drzewo klasyfikacyjne po przyciciu}
\end{figure}

\hypertarget{ocena-dopasowania-modelu}{%
\subsection{Ocena dopasowania modelu}\label{ocena-dopasowania-modelu}}

Na koniec budowy modelu nale偶y sprawdzi jego jako na zbiorze testowym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.class2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.rpart2,}
                       \DataTypeTok{newdata =}\NormalTok{ dt.test,}
                       \DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{tab2 <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred.class2, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15
\end{verbatim}

Mimo przycicia drzewa, klasyfikacja pozostaje na niezmienionym poziomie. Odsetek poprawnych klasyfikacji mo偶emy oszacowa za pomoc

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(tab2))}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(tab2)}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 97.8
\end{verbatim}

\hypertarget{inne-algorytmy-budowy-drzew-decyzyjnych-implementowane-w-r}{%
\section{\texorpdfstring{Inne algorytmy budowy drzew decyzyjnych implementowane w \textbf{R}}{Inne algorytmy budowy drzew decyzyjnych implementowane w R}}\label{inne-algorytmy-budowy-drzew-decyzyjnych-implementowane-w-r}}

Opr贸cz najbardziej znanego algorytmu CART implementowanego w postaci funkcji pakietu \textbf{rpart}, istniej r贸wnie偶 inne algorytmy, kt贸re znalzay swoje implementacje w R. S to:

\begin{itemize}
\tightlist
\item
  \emph{CHAID}\footnote{Chi-square automatic interaction detection} - algorytm przeznaczony do budowy drzew klafyfikacyjnych, gdzie zar贸wno zmienna wynikowa, jak i zmienne niezale偶ne musz by ze skali jakociowej. G贸wn r贸偶nic w stosunku do drzew typu CART jest spos贸b budowy podzia贸w, oparty na tecie niezale偶noci \(\chi^2\) Pearsona. Wyboru reguy podziau dokonuje si poprzez testowanie niezale偶noci zmiennej niezale偶nej z predyktorami. Regua o najwikszej wartoci statystyki \(\chi^2\) jest stosowana w pierwszej kolejnoci. Implementacja tego algorytmu znajduje si w pakiecie \textbf{CHAID}\footnote{brak w oficjalnej dystrybucji CRAN} (funkcja do tworzenia drzewa o tej samej nazwie \texttt{chaid}) \citep{R-CHAID}.
\item
  \emph{Ctree}\footnote{Conditional Inference Trees} - algorytm zbli偶ony zasad dzialania do CHAID, poniewa偶 r贸wnie偶 wykorzystuje testowanie do wyboru reguy podziau. R贸偶ni si jednak tym, 偶e mo偶e by stosowany do zmiennych dowolnego typu. Implementacj R-ow mo偶na znale藕 w pakietach \textbf{party} \citep{R-party} lub \textbf{partykit} \citep{R-partykit} - funkcj do tworzenia modelu jest \texttt{ctree}.
\item
  \emph{C4.5} - algorytm stworzony przez \citet{quinlan1993} w oparciu, o r贸wnie偶 jego autorstwa, algorytm ID3. W du偶ym uproszczeniu, dob贸r regu podziau odbywa si na podstawie przyrostu informacji (patrz \protect\hyperlink{reguy-podziau}{Reguy podziau}). W przeciwiestwie do pierwotnego algorytmu ID3, C4.5 nie raczej nie przeucza drzew. Implementacja R-owa znajduje si w pakiecie \textbf{RWeka} \citep{R-Rweka} - funkcja do budowy drzewa to \texttt{J48}.
\item
  \emph{C5.0} - kolejny algorytm autorstwa \citet{R-C50} jest usprawnieniem algorytmu C4.5, generujcym mniejsze drzewa automatycznie przycinane na podstawie zo偶noci drzewa. Jest szybszy od poprzednika i pozwala na zastosowanie metody \emph{boosting}\footnote{budowa klasyfikatora w oparciu o proces iteracyjny, w kt贸rym kolejne w kolejnych iteracjach budowane s proste drzewa i przypisywane s im wagi - im gorszy klasyfikator, tym wiksza waga - po to aby nauczy drzewo klasyfikowa ``trudne'' przypadki}. Implementacja R-owa znajduje si w pakiecie \emph{C50}, a funkcja do budowy drzewa to \texttt{C5.0}.
\end{itemize}

\hypertarget{przyk42}{%
\section{Przykad}\label{przyk42}}

W celu por贸wnania wynik贸w klasyfikacji na podstawie drzew decyzyjnych o r贸偶nych algorytmach, zostan nauczone modele w oparciu o funkcje \texttt{ctree}, \texttt{J48} i \texttt{C5.0} dla tego samego zestawu danych co w \protect\hyperlink{przyk41}{przykadzie wczeniejszym}.

\hypertarget{ctree}{%
\subsection{\texorpdfstring{\texttt{ctree}}{ctree}}\label{ctree}}

Na pocztek ustalamy parametry ograniczajce rozrozt drzewa podobne jak w poprzednim przykadzie.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(partykit)}
\NormalTok{tree2 <-}\StringTok{ }\KeywordTok{ctree}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.train,}
               \DataTypeTok{control =} \KeywordTok{ctree_control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{10}\NormalTok{,}
                                       \DataTypeTok{minbucket =} \DecValTok{5}\NormalTok{,}
                                       \DataTypeTok{maxdepth =} \DecValTok{4}\NormalTok{))}
\NormalTok{tree2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Model formula:
## Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
## 
## Fitted party:
## [1] root
## |   [2] Petal.Length <= 1.9: setosa (n = 37, err = 0.0%)
## |   [3] Petal.Length > 1.9
## |   |   [4] Petal.Width <= 1.7
## |   |   |   [5] Petal.Length <= 4.9: versicolor (n = 32, err = 3.1%)
## |   |   |   [6] Petal.Length > 4.9: virginica (n = 6, err = 33.3%)
## |   |   [7] Petal.Width > 1.7: virginica (n = 30, err = 0.0%)
## 
## Number of inner nodes:    3
## Number of terminal nodes: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/ctree-1.pdf}
\caption{\label{fig:ctree}Wykres drzewa decyzyjnego zbudowanego metod ctree}
\end{figure}

Wydaje si, 偶e drzewo nie jest optymalne, poniewa偶 w w藕le 6 obserwacje z grup \texttt{versicolor} i \texttt{virginica} s nieco pomieszane. Ostateczne oceny dokonujemy na podstawie pr贸by testowej.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree2, }\DataTypeTok{newdata =}\NormalTok{ dt.test)}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred2, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15
\end{verbatim}

Dopiero ocena jakoci klasyfikacji na podstawe pr贸by testowej pokazuje, 偶e model zbudowany za pomoc \texttt{ctree} daje podobn precyzj jak \texttt{rpart} przycity.

\hypertarget{j48}{%
\subsection{\texorpdfstring{\texttt{J48}}{J48}}\label{j48}}

W tym przypadku model sam poszukuje optymalnego rozwiazania przycinajc si automatycznie.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(RWeka)}
\NormalTok{tree3 <-}\StringTok{ }\KeywordTok{J48}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.train)}
\NormalTok{tree3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## J48 pruned tree
## ------------------
## 
## Petal.Width <= 0.6: setosa (37.0)
## Petal.Width > 0.6
## |   Petal.Width <= 1.7
## |   |   Petal.Length <= 4.9: versicolor (32.0/1.0)
## |   |   Petal.Length > 4.9
## |   |   |   Petal.Width <= 1.5: virginica (3.0)
## |   |   |   Petal.Width > 1.5: versicolor (3.0/1.0)
## |   Petal.Width > 1.7: virginica (30.0)
## 
## Number of Leaves  :  5
## 
## Size of the tree :   9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree3)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/J48-1.pdf}
\caption{\label{fig:J48}Wykres drzewa decyzyjnego zbudowanego metod J48}
\end{figure}

Drzewo jest nieco bardziej rozbudowane ni偶 \texttt{tree2} i \texttt{mod.rpart2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(tree3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## === Summary ===
## 
## Correctly Classified Instances         103               98.0952 %
## Incorrectly Classified Instances         2                1.9048 %
## Kappa statistic                          0.9714
## Mean absolute error                      0.0208
## Root mean squared error                  0.1019
## Relative absolute error                  4.6776 %
## Root relative squared error             21.628  %
## Total Number of Instances              105     
## 
## === Confusion Matrix ===
## 
##   a  b  c   <-- classified as
##  37  0  0 |  a = setosa
##   0 33  0 |  b = versicolor
##   0  2 33 |  c = virginica
\end{verbatim}

Podsumowanie dopasowania drzewa na pr贸bie uczcej jest bardzo dobre, bo poprawnych klasyfikacji jest ponad 98\%. Oceny dopasowania i tak dokonujemy na zbiorze testowym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred3 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree3, }\DataTypeTok{newdata =}\NormalTok{ dt.test)}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred3, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15
\end{verbatim}

Otrzymujemy identyczn macierz klasyfikacji jak w poprzednich przypadkach.

\hypertarget{c50}{%
\subsection{\texorpdfstring{\texttt{C50}}{C50}}\label{c50}}

Tym razem r贸wnie偶 nie trzeba ustawia parametr贸w drzewa, poniewa偶 algorytm dziaa tak aby zapobiec rozrostowi drzewa przy jednoczesnej wysokiej poprawnoci klasyfikacji.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(C50)}
\NormalTok{tree4 <-}\StringTok{ }\KeywordTok{C5.0}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.train)}
\KeywordTok{summary}\NormalTok{(tree4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## C5.0.formula(formula = Species ~ ., data = dt.train)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Tue Mar 12 19:23:48 2019
## -------------------------------
## 
## Class specified by attribute `outcome'
## 
## Read 105 cases (5 attributes) from undefined.data
## 
## Decision tree:
## 
## Petal.Length <= 1.9: setosa (37)
## Petal.Length > 1.9:
## :...Petal.Width > 1.7: virginica (30)
##     Petal.Width <= 1.7:
##     :...Petal.Length <= 4.9: versicolor (32/1)
##         Petal.Length > 4.9: virginica (6/2)
## 
## 
## Evaluation on training data (105 cases):
## 
##      Decision Tree   
##    ----------------  
##    Size      Errors  
## 
##       4    3( 2.9%)   <<
## 
## 
##     (a)   (b)   (c)    <-classified as
##    ----  ----  ----
##      37                (a): class setosa
##            31     2    (b): class versicolor
##             1    34    (c): class virginica
## 
## 
##  Attribute usage:
## 
##  100.00% Petal.Length
##   64.76% Petal.Width
## 
## 
## Time: 0.0 secs
\end{verbatim}

Otrzymujemy identyczne drzewo jak w przypadku zastosowania algorytmu \texttt{ctree}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree4)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/C50-1.pdf}
\caption{\label{fig:C50}Wykres drzewa decyzyjnego zbudowanego metod C5.0}
\end{figure}

Dla pewnoci przeprowadzimy sprawdzenie na zbiorze testowym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred4 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree4, }\DataTypeTok{newdata =}\NormalTok{ dt.test)}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred4, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15
\end{verbatim}

\hypertarget{pochodne-drzew-decyzyjnych}{%
\chapter{Pochodne drzew decyzyjnych}\label{pochodne-drzew-decyzyjnych}}

Przykad zastosowania drzew decyzyjnych na zbiorze \texttt{iris} w poprzednich \protect\hyperlink{przyk41}{przykadach} mo偶e skania do przypuszczenia, 偶e drzewa decyzyjne zawsze dobrze radz sobie z predykcj wartoci wynikowej. Niestety w przykadach nieco bardziej skomplikowanych, gdzie chocia偶by klasy zmiennej wynikowej nie s tak wyra藕nie separowalne, drzewa decyzjne wypadaj gorzej w pr贸wnaniu z innymi modelami nadzorowanego uczenia maszynowego.

I tak u podstaw metod bazujcych na prostych drzewach decyzyjnych sta pomys, 偶e skoro jedno drzewo nie ma wystrczajcych wasnoci predykcyjnych, to mo偶e zastosowanie wielu drzew poczonych w pewien spos贸b poprawi je. Tak powstay metody \emph{bagging}, \emph{random forest} i \emph{boosting}\footnote{hyba tylko dla dr贸giej metody istniej dobre polskie tumaczenie nazwy - las losowy}.

\hypertarget{bagging}{%
\section{Bagging}\label{bagging}}

Technika ta zostala wprowadzona przez \citet{Breiman1996} i ma na celu zmniejszenie wariancji modelu pojedynczego drzewa. Podobnie jak technika \emph{bootstrap}, w kt贸rej statystyki s wyliczane na wielu pr贸bach pobranych z tego samego rozkadu (pr贸by), w metodzie bagging losuje si wiele pr贸b ze zbioru uczcego (najczciej poprzez wielokrotne losowanie pr贸by o rozmiarze zbioru uczcego ze zwracaniem), a nastpnie dla ka偶dej pr贸by bootstrapowej buduje si drzewo. W ten spos贸b otrzymujemy \(B\) drzew decyzyjnych \(\hat{f}^1(x), \hat{f}^2(x),\ldots, \hat{f}^B(x)\). Na koniec poprzez urednienie otrzymujemy model charakteryzujcy si wiksz precyzj
\begin{equation}
    \hat{f}_{bag}(x)=\frac1B\sum_{b=1}^B\hat{f}^b(x).
\end{equation}

Poniewa偶 podczas budowy drzew na podstawie pr贸b bootstrapowych nie kontrolujemy zo偶onoci, to w rezultacie ka偶de z drzew mo偶e charakteryzowa si du偶 wariancj. Poprzez urednianie wynik贸w pojedynczych drzew otrzymujemy mniejsze obci偶enie ale r贸wnie偶 przy dostatecznie du偶ej liczbie pr贸b (\(B\) czsto liczy si w setkach, czy tysicach) zmniejszamy wariancj ``redniej'' predykcji z drzew. Oczywicie metod t trzeba dostosowa do zada klasyfikacyjnych, poniewa偶 nie istnieje rednia klasyfikacji z wielu drzew. W miejsce reniej stosuje si mod, czyli warto dominujc.

Przyjrzyjmy si jak maszyna losuje obserwacje ze zwracaniem

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{m <-}\StringTok{ }\OtherTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{)\{}
\NormalTok{    x <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{, }\DataTypeTok{size =} \DecValTok{500}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T)}
\NormalTok{    y <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{, x)}
\NormalTok{    z <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(x)}
\NormalTok{    n[i] <-}\StringTok{ }\KeywordTok{length}\NormalTok{(z)}
\NormalTok{    m[i] <-}\StringTok{ }\KeywordTok{length}\NormalTok{(y)}
\NormalTok{\}}
\KeywordTok{mean}\NormalTok{(n)}\OperatorTok{/}\DecValTok{500}\OperatorTok{*}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 63.2574
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(m)}\OperatorTok{/}\DecValTok{500}\OperatorTok{*}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 36.7426
\end{verbatim}

Faktycznie uczenie modelu metod bagging odbywa si rednio na 2/3 obserwacji zbioru uczcego wylosowanych do pr贸b bootstrapowych, a pozostaa 1/3 (ang. \emph{out-of-bag}) jest wykorzystana do oceny jakoci predykcji.

Niewtpliw zalet drzew decyzyjnych bya ich atwa interpretacja. W przypadku metody bagging jest ona znacznie utrudniona, poniewa偶 jej wynik skada si z agregacji wielu drzew. Mo偶na natomiast oceni wa偶no predyktor贸w (ang. \emph{variable importance}). I tak, przez obserwacj spadku \(RSS\) dla baggingu regresyjnego przy zastosowaniu danego predyktora w podziaach drzewa i urednieniu wyniku otrzymamy wska藕nik wa偶noci predyktora du偶o lepszy ni偶 dla pojedynczego drzewa. W przypadku baggingu klasyfikacyjnego w miejsce \(RSS\) stosujemy indeks Gini'ego.

Implementacja R-owa metody bagging znajduje si w pakiecie \textbf{ipred}, a funkcja do budowy modelu nazywa si \texttt{bagging} \citep{R-ipred}. Mo偶na r贸wnie偶 stosowa funkcj \texttt{randomForest} pakietu \textbf{randomForest} \citep{R-las} - powody takiego dziaania wyjani si w rodziale \protect\hyperlink{lasy-losowe}{Lasy losowe}.

\hypertarget{przyk51}{%
\subsection{Przykad}\label{przyk51}}

Tym razem cel zadania jest regresyjny i polega na ustaleniu miary tendencji centralnej ceny mieszka w Bostonie na podstwie zmiennych umieszczonych w zbiorze \texttt{Boston} pakietu \textbf{MASS} \citep{R-MASS}. Zmienn zale偶n bdzie mediana cen mieszka na przedmieciach Bostonu (\texttt{medv}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{head}\NormalTok{(Boston)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black
## 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90
## 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90
## 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83
## 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63
## 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90
## 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12
##   lstat medv
## 1  4.98 24.0
## 2  9.14 21.6
## 3  4.03 34.7
## 4  2.94 33.4
## 5  5.33 36.2
## 6  5.21 28.7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2019}\NormalTok{)}
\NormalTok{boston.train <-}\StringTok{ }\NormalTok{Boston }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{sample_frac}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\NormalTok{boston.test <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(Boston, boston.train)}
\end{Highlighting}
\end{Shaded}

Aby m贸c por贸wna wyniki predykcji z metody bagging, najpierw zostanie zbudowane jedno drzewo decyzyjne w oparciu o algorytm CART.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rpart)}
\KeywordTok{library}\NormalTok{(rpart.plot)}
\NormalTok{boston.rpart <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ boston.train)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(boston.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## rpart(formula = medv ~ ., data = boston.train)
##   n= 337 
## 
##            CP nsplit rel error    xerror       xstd
## 1  0.43506104      0 1.0000000 1.0037495 0.10496568
## 2  0.21114710      1 0.5649390 0.6856438 0.07732133
## 3  0.05641774      2 0.3537919 0.4393220 0.05974589
## 4  0.04154842      3 0.2973741 0.3726563 0.05716622
## 5  0.02707678      4 0.2558257 0.3520312 0.05569786
## 6  0.01489117      5 0.2287489 0.3238915 0.05681943
## 7  0.01202564      6 0.2138578 0.2922610 0.05311293
## 8  0.01057622      7 0.2018321 0.2889364 0.05318206
## 9  0.01031677      8 0.1912559 0.2838433 0.05152251
## 10 0.01006729      9 0.1809391 0.2838187 0.05152098
## 11 0.01000000     10 0.1708718 0.2815210 0.05152993
## 
## Variable importance
##   lstat     nox   indus    crim     tax      rm     age     dis ptratio 
##      24      13      13      13      11      10      10       2       2 
##     rad   black 
##       1       1 
## 
## Node number 1: 337 observations,    complexity param=0.435061
##   mean=22.61157, MSE=79.33004 
##   left son=2 (186 obs) right son=3 (151 obs)
##   Primary splits:
##       lstat   < 10.02    to the right, improve=0.4350610, (0 missing)
##       rm      < 6.8375   to the left,  improve=0.4305766, (0 missing)
##       indus   < 6.66     to the right, improve=0.2914821, (0 missing)
##       ptratio < 19.15    to the right, improve=0.2608119, (0 missing)
##       nox     < 0.5125   to the right, improve=0.2169607, (0 missing)
##   Surrogate splits:
##       indus < 7.625    to the right, agree=0.846, adj=0.656, (0 split)
##       nox   < 0.519    to the right, agree=0.828, adj=0.616, (0 split)
##       crim  < 0.12995  to the right, agree=0.786, adj=0.523, (0 split)
##       age   < 63.9     to the right, agree=0.777, adj=0.503, (0 split)
##       tax   < 377      to the right, agree=0.769, adj=0.483, (0 split)
## 
## Node number 2: 186 observations,    complexity param=0.05641774
##   mean=17.31828, MSE=19.86042 
##   left son=4 (58 obs) right son=5 (128 obs)
##   Primary splits:
##       crim  < 5.84803  to the right, improve=0.4083024, (0 missing)
##       dis   < 2.0754   to the left,  improve=0.3684093, (0 missing)
##       lstat < 14.405   to the right, improve=0.3516672, (0 missing)
##       nox   < 0.657    to the right, improve=0.3255969, (0 missing)
##       age   < 84.9     to the right, improve=0.2247741, (0 missing)
##   Surrogate splits:
##       rad   < 16       to the right, agree=0.855, adj=0.534, (0 split)
##       tax   < 551.5    to the right, agree=0.839, adj=0.483, (0 split)
##       nox   < 0.657    to the right, agree=0.828, adj=0.448, (0 split)
##       dis   < 2.0754   to the left,  agree=0.801, adj=0.362, (0 split)
##       lstat < 19.055   to the right, agree=0.796, adj=0.345, (0 split)
## 
## Node number 3: 151 observations,    complexity param=0.2111471
##   mean=29.13179, MSE=75.5574 
##   left son=6 (120 obs) right son=7 (31 obs)
##   Primary splits:
##       rm      < 7.127    to the left,  improve=0.4947648, (0 missing)
##       lstat   < 4.495    to the right, improve=0.4054324, (0 missing)
##       nox     < 0.574    to the left,  improve=0.1389706, (0 missing)
##       ptratio < 14.75    to the right, improve=0.1349232, (0 missing)
##       age     < 89.45    to the left,  improve=0.1133301, (0 missing)
##   Surrogate splits:
##       lstat   < 3.21     to the right, agree=0.841, adj=0.226, (0 split)
##       ptratio < 14.15    to the right, agree=0.828, adj=0.161, (0 split)
##       tax     < 207      to the right, agree=0.808, adj=0.065, (0 split)
##       nox     < 0.639    to the left,  agree=0.801, adj=0.032, (0 split)
## 
## Node number 4: 58 observations
##   mean=13.08793, MSE=14.14485 
## 
## Node number 5: 128 observations,    complexity param=0.01489117
##   mean=19.23516, MSE=10.66681 
##   left son=10 (61 obs) right son=11 (67 obs)
##   Primary splits:
##       lstat   < 14.405   to the right, improve=0.2915760, (0 missing)
##       dis     < 1.99235  to the left,  improve=0.2280873, (0 missing)
##       age     < 84.15    to the right, improve=0.1950219, (0 missing)
##       ptratio < 20.95    to the right, improve=0.1349341, (0 missing)
##       rm      < 5.706    to the left,  improve=0.1194638, (0 missing)
##   Surrogate splits:
##       age   < 91.15    to the right, agree=0.758, adj=0.492, (0 split)
##       dis   < 2.0418   to the left,  agree=0.664, adj=0.295, (0 split)
##       nox   < 0.607    to the right, agree=0.633, adj=0.230, (0 split)
##       indus < 18.84    to the right, agree=0.625, adj=0.213, (0 split)
##       rm    < 5.703    to the left,  agree=0.617, adj=0.197, (0 split)
## 
## Node number 6: 120 observations,    complexity param=0.04154842
##   mean=26.02417, MSE=34.39883 
##   left son=12 (98 obs) right son=13 (22 obs)
##   Primary splits:
##       lstat < 5.145    to the right, improve=0.2690898, (0 missing)
##       dis   < 2.0891   to the right, improve=0.2163813, (0 missing)
##       rm    < 6.543    to the left,  improve=0.2036454, (0 missing)
##       age   < 89.45    to the left,  improve=0.1796977, (0 missing)
##       tax   < 548      to the left,  improve=0.1751322, (0 missing)
##   Surrogate splits:
##       zn    < 92.5     to the left,  agree=0.833, adj=0.091, (0 split)
##       nox   < 0.4035   to the right, agree=0.833, adj=0.091, (0 split)
##       indus < 1.495    to the right, agree=0.825, adj=0.045, (0 split)
##       dis   < 1.48495  to the right, agree=0.825, adj=0.045, (0 split)
## 
## Node number 7: 31 observations,    complexity param=0.02707678
##   mean=41.16129, MSE=52.78882 
##   left son=14 (11 obs) right son=15 (20 obs)
##   Primary splits:
##       rm      < 7.437    to the left,  improve=0.4423448, (0 missing)
##       lstat   < 5.185    to the right, improve=0.3125696, (0 missing)
##       ptratio < 15.05    to the right, improve=0.1896089, (0 missing)
##       black   < 392.715  to the right, improve=0.1133472, (0 missing)
##       age     < 37.6     to the right, improve=0.0737298, (0 missing)
##   Surrogate splits:
##       lstat < 4.635    to the right, agree=0.774, adj=0.364, (0 split)
##       indus < 2.32     to the left,  agree=0.742, adj=0.273, (0 split)
##       dis   < 5.9736   to the right, agree=0.710, adj=0.182, (0 split)
##       black < 390.095  to the right, agree=0.710, adj=0.182, (0 split)
##       crim  < 0.10593  to the left,  agree=0.677, adj=0.091, (0 split)
## 
## Node number 10: 61 observations
##   mean=17.38689, MSE=8.122779 
## 
## Node number 11: 67 observations
##   mean=20.91791, MSE=7.041172 
## 
## Node number 12: 98 observations,    complexity param=0.01202564
##   mean=24.58265, MSE=20.9745 
##   left son=24 (64 obs) right son=25 (34 obs)
##   Primary splits:
##       rm    < 6.543    to the left,  improve=0.1564077, (0 missing)
##       black < 364.385  to the right, improve=0.1331323, (0 missing)
##       age   < 89.45    to the left,  improve=0.1241124, (0 missing)
##       tax   < 223.5    to the right, improve=0.1204819, (0 missing)
##       dis   < 4.46815  to the right, improve=0.1048755, (0 missing)
##   Surrogate splits:
##       dis   < 3.6589   to the right, agree=0.704, adj=0.147, (0 split)
##       rad   < 6.5      to the left,  agree=0.704, adj=0.147, (0 split)
##       age   < 68.9     to the left,  agree=0.694, adj=0.118, (0 split)
##       indus < 1.605    to the right, agree=0.673, adj=0.059, (0 split)
##       nox   < 0.4045   to the right, agree=0.673, adj=0.059, (0 split)
## 
## Node number 13: 22 observations,    complexity param=0.01031677
##   mean=32.44545, MSE=43.70884 
##   left son=26 (15 obs) right son=27 (7 obs)
##   Primary splits:
##       tax   < 364      to the left,  improve=0.2868266, (0 missing)
##       lstat < 3.855    to the right, improve=0.2413545, (0 missing)
##       age   < 31.85    to the left,  improve=0.1598075, (0 missing)
##       dis   < 5.4085   to the right, improve=0.1258591, (0 missing)
##       black < 381.59   to the right, improve=0.1052855, (0 missing)
##   Surrogate splits:
##       crim  < 2.6956   to the left,  agree=0.773, adj=0.286, (0 split)
##       indus < 14       to the left,  agree=0.773, adj=0.286, (0 split)
##       nox   < 0.5875   to the left,  agree=0.773, adj=0.286, (0 split)
##       age   < 89.65    to the left,  agree=0.773, adj=0.286, (0 split)
##       dis   < 2.3371   to the right, agree=0.773, adj=0.286, (0 split)
## 
## Node number 14: 11 observations
##   mean=34.64545, MSE=3.304298 
## 
## Node number 15: 20 observations,    complexity param=0.01057622
##   mean=44.745, MSE=43.81147 
##   left son=30 (12 obs) right son=31 (8 obs)
##   Primary splits:
##       ptratio < 15.4     to the right, improve=0.3226860, (0 missing)
##       rad     < 6        to the right, improve=0.2170243, (0 missing)
##       tax     < 270      to the right, improve=0.1545997, (0 missing)
##       age     < 71.85    to the right, improve=0.1331209, (0 missing)
##       zn      < 10       to the left,  improve=0.1328727, (0 missing)
##   Surrogate splits:
##       zn   < 10       to the left,  agree=0.80, adj=0.500, (0 split)
##       nox  < 0.541    to the left,  agree=0.80, adj=0.500, (0 split)
##       age  < 86.7     to the left,  agree=0.80, adj=0.500, (0 split)
##       dis  < 2.5813   to the right, agree=0.80, adj=0.500, (0 split)
##       crim < 0.45114  to the left,  agree=0.75, adj=0.375, (0 split)
## 
## Node number 24: 64 observations,    complexity param=0.01006729
##   mean=23.2625, MSE=21.96891 
##   left son=48 (57 obs) right son=49 (7 obs)
##   Primary splits:
##       indus < 14.48    to the left,  improve=0.19142190, (0 missing)
##       crim  < 0.841845 to the left,  improve=0.17407590, (0 missing)
##       black < 374.635  to the right, improve=0.14590640, (0 missing)
##       dis   < 2.6499   to the right, improve=0.13374910, (0 missing)
##       age   < 79.85    to the left,  improve=0.08856433, (0 missing)
##   Surrogate splits:
##       crim  < 1.163695 to the left,  agree=0.984, adj=0.857, (0 split)
##       nox   < 0.589    to the left,  agree=0.984, adj=0.857, (0 split)
##       age   < 84.35    to the left,  agree=0.984, adj=0.857, (0 split)
##       dis   < 2.28545  to the right, agree=0.969, adj=0.714, (0 split)
##       black < 361.635  to the right, agree=0.969, adj=0.714, (0 split)
## 
## Node number 25: 34 observations
##   mean=27.06765, MSE=9.646894 
## 
## Node number 26: 15 observations
##   mean=30.02667, MSE=14.56062 
## 
## Node number 27: 7 observations
##   mean=37.62857, MSE=66.76776 
## 
## Node number 30: 12 observations
##   mean=41.675, MSE=48.28521 
## 
## Node number 31: 8 observations
##   mean=49.35, MSE=1.7575 
## 
## Node number 48: 57 observations
##   mean=22.54386, MSE=10.87053 
## 
## Node number 49: 7 observations
##   mean=29.11429, MSE=73.89265
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(boston.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-32-1.pdf}
\caption{\label{fig:unnamed-chunk-32}Drzewo regresyjne pene}
\end{figure}

Przycinamy drzewo\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{printcp}\NormalTok{(boston.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Regression tree:
## rpart(formula = medv ~ ., data = boston.train)
## 
## Variables actually used in tree construction:
## [1] crim    indus   lstat   ptratio rm      tax    
## 
## Root node error: 26734/337 = 79.33
## 
## n= 337 
## 
##          CP nsplit rel error  xerror     xstd
## 1  0.435061      0   1.00000 1.00375 0.104966
## 2  0.211147      1   0.56494 0.68564 0.077321
## 3  0.056418      2   0.35379 0.43932 0.059746
## 4  0.041548      3   0.29737 0.37266 0.057166
## 5  0.027077      4   0.25583 0.35203 0.055698
## 6  0.014891      5   0.22875 0.32389 0.056819
## 7  0.012026      6   0.21386 0.29226 0.053113
## 8  0.010576      7   0.20183 0.28894 0.053182
## 9  0.010317      8   0.19126 0.28384 0.051523
## 10 0.010067      9   0.18094 0.28382 0.051521
## 11 0.010000     10   0.17087 0.28152 0.051530
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotcp}\NormalTok{(boston.rpart)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-33-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.rpart2 <-}\StringTok{ }\KeywordTok{prune}\NormalTok{(boston.rpart, }\DataTypeTok{cp =} \FloatTok{0.012026}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(boston.rpart2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-34-1.pdf}
\caption{\label{fig:unnamed-chunk-34}Drzewo regresyjne przycite}
\end{figure}

Predykcja na podstawie drzewa na zbiorze testowym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(boston.rpart2, }\DataTypeTok{newdata =}\NormalTok{ boston.test)}
\NormalTok{rmse <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pred, obs) }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\KeywordTok{length}\NormalTok{(pred)}\OperatorTok{*}\KeywordTok{sum}\NormalTok{((pred}\OperatorTok{-}\NormalTok{obs)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\KeywordTok{rmse}\NormalTok{(boston.pred, boston.test}\OperatorTok{$}\NormalTok{medv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.825862
\end{verbatim}

Teraz zbudujemy model metod bagging.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{boston.bag <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ boston.train, }
                           \DataTypeTok{mtry =} \KeywordTok{ncol}\NormalTok{(boston.train)}\OperatorTok{-}\DecValTok{1}\NormalTok{)}
\NormalTok{boston.bag}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = medv ~ ., data = boston.train, mtry = ncol(boston.train) -      1) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 13
## 
##           Mean of squared residuals: 13.06701
##                     % Var explained: 83.53
\end{verbatim}

Predykcja na podstawie modelu

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(boston.bag, }\DataTypeTok{newdata =}\NormalTok{ boston.test)}
\KeywordTok{rmse}\NormalTok{(boston.pred2, boston.test}\OperatorTok{$}\NormalTok{medv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.039308
\end{verbatim}

Zatem predykcja na podstwie modelu bagging jest nico lepsza ni偶 z pojedynczego drzewa. Dodatkowo mo偶emy oceni wa偶no zmiennych u偶ytych w budowie drzew.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(boston.bag)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-38-1.pdf}
\caption{\label{fig:unnamed-chunk-38}Wykres wa偶noci predyktor贸w}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{importance}\NormalTok{(boston.bag)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         IncNodePurity
## crim       1200.11828
## zn           24.17836
## indus       262.33396
## chas         22.27133
## nox         417.32236
## rm         9102.58339
## age         416.48170
## dis        1494.79734
## rad         171.92103
## tax         403.66309
## ptratio     411.88528
## black       331.58495
## lstat     12137.38999
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{$}\NormalTok{variable.importance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      lstat        nox      indus       crim        tax         rm 
## 15197.8587  8683.8225  8325.2431  8074.7200  6991.0756  6768.5423 
##        age        dis    ptratio        rad      black         zn 
##  6538.5039  1305.3786  1193.2073   853.4309   323.8576   242.3521
\end{verbatim}

W por贸wnaniu do wa偶noci zmiennych dla pojedynczego drzewa wida pewne ro偶nice.

\hypertarget{lasy-losowe}{%
\section{Lasy losowe}\label{lasy-losowe}}

Lasy losowe s uog贸lnieniem metody bagging, polegajc na losowaniu dla ka偶dego drzewa wchodzcego w skad lasu \(m\) predyktor贸w spor贸d \(p\) dostpnych, a nastpnie budowaniu drzew z wykorzystaniem tylko tych predyktor贸w. Dziki temu za ka偶dy razem drzewo jest budowane w oparciu o nowy zestaw cech (najczciej przyjmujemy \(m=\sqrt{p}\)). W przypadku modeli bagging za ka偶dym razem najsilniejszy predyktor wchodzi w skad zbioru uczacego, a co za tym idzie r贸wnie偶 uczestniczy w tworzeniu regu podziau. W贸wczas wiele drzew zawierao reguy stosujce dany atrubut, a wtedy predykcje otrzymywane za pomoc drzew byy skorelowane. Dlatego nawet du偶a liczba pr贸b bootstrapowych nie zapewniaa poprawy precyzji.

\hypertarget{przyk52}{%
\subsection{Przykad}\label{przyk52}}

Kontynuujc poprzedni \protect\hyperlink{przyk51}{przykad} mo偶emy zbudowa las losowy aby przekona si czy nastpi poprawa predykcji zmiennej wynikowej.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.rf <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ boston.train)}
\NormalTok{boston.rf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = medv ~ ., data = boston.train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 4
## 
##           Mean of squared residuals: 13.09902
##                     % Var explained: 83.49
\end{verbatim}

Porownanie MSE na pr贸bach uczcych pomidzy lasem losowym i modelem bagging wypada nieco na korzy bagging.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred3 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(boston.rf, }\DataTypeTok{newdata =}\NormalTok{ boston.test)}
\KeywordTok{rmse}\NormalTok{(boston.pred3, boston.test}\OperatorTok{$}\NormalTok{medv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.418302
\end{verbatim}

Wa偶no zmiennych r贸wnie偶 si nieco r贸偶ni.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(boston.rf)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-41-1.pdf}

\hypertarget{boosting}{%
\section{Boosting}\label{boosting}}

\bibliography{book.bib,packages.bib}


\end{document}
