<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>4 Drzewa decyzyjne | Eksploracja danych</title>
  <meta name="description" content="Książka stanowi materiał źródłowy do przeprowadzenia przedmiotu Eksploracja Danych.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="4 Drzewa decyzyjne | Eksploracja danych" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Książka stanowi materiał źródłowy do przeprowadzenia przedmiotu Eksploracja Danych." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Drzewa decyzyjne | Eksploracja danych" />
  
  <meta name="twitter:description" content="Książka stanowi materiał źródłowy do przeprowadzenia przedmiotu Eksploracja Danych." />
  



<meta name="date" content="2019-03-22">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="podzia-metod-data-mining.html">
<link rel="next" href="pochodne-drzew-decyzyjnych.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
        P: '{\\mathrm{P}}',
        E: '{\\mathrm{E}}',
        Var: '{\\mathrm{Var}}',
        Cor: '{\\mathrm{Cor}}',
        Cov: '{\\mathrm{Cov}}',
        Tr: '{\\mathrm{Tr}}'
    },
}
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Eksploracja Danych</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Wstęp</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#o-ksiazce"><i class="fa fa-check"></i>O książce</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#zakres-przedmiotu"><i class="fa fa-check"></i>Zakres przedmiotu</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#zakres-technik-stosowanych-w-data-mining"><i class="fa fa-check"></i>Zakres technik stosowanych w data mining</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#etapy-eksploracji-danych"><i class="fa fa-check"></i>Etapy eksploracji danych</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="roz1.html"><a href="roz1.html"><i class="fa fa-check"></i><b>1</b> Import danych</a></li>
<li class="chapter" data-level="2" data-path="przygotowanie-danych.html"><a href="przygotowanie-danych.html"><i class="fa fa-check"></i><b>2</b> Przygotowanie danych</a><ul>
<li class="chapter" data-level="2.1" data-path="przygotowanie-danych.html"><a href="przygotowanie-danych.html#identyfikacja-brakow-danych"><i class="fa fa-check"></i><b>2.1</b> Identyfikacja braków danych</a></li>
<li class="chapter" data-level="2.2" data-path="przygotowanie-danych.html"><a href="przygotowanie-danych.html#zastepowanie-brakow-danych"><i class="fa fa-check"></i><b>2.2</b> Zastępowanie braków danych</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html"><i class="fa fa-check"></i><b>3</b> Podział metod data mining</a><ul>
<li class="chapter" data-level="3.1" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#rodzaje-wnioskowania"><i class="fa fa-check"></i><b>3.1</b> Rodzaje wnioskowania</a><ul>
<li class="chapter" data-level="3.1.1" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#dziedzina"><i class="fa fa-check"></i><b>3.1.1</b> Dziedzina</a></li>
<li class="chapter" data-level="3.1.2" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#obserwacja"><i class="fa fa-check"></i><b>3.1.2</b> Obserwacja</a></li>
<li class="chapter" data-level="3.1.3" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#atrybuty-obserwacji"><i class="fa fa-check"></i><b>3.1.3</b> Atrybuty obserwacji</a></li>
<li class="chapter" data-level="3.1.4" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#zbior-uczacy"><i class="fa fa-check"></i><b>3.1.4</b> Zbiór uczący</a></li>
<li class="chapter" data-level="3.1.5" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#zbior-testowy"><i class="fa fa-check"></i><b>3.1.5</b> Zbiór testowy</a></li>
<li class="chapter" data-level="3.1.6" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#model"><i class="fa fa-check"></i><b>3.1.6</b> Model</a></li>
<li class="chapter" data-level="3.1.7" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#jakosc-dopasowania-modelu"><i class="fa fa-check"></i><b>3.1.7</b> Jakość dopasowania modelu</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#modele-regresyjne"><i class="fa fa-check"></i><b>3.2</b> Modele regresyjne</a></li>
<li class="chapter" data-level="3.3" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#modele-klasyfikacyjne"><i class="fa fa-check"></i><b>3.3</b> Modele klasyfikacyjne</a></li>
<li class="chapter" data-level="3.4" data-path="podzia-metod-data-mining.html"><a href="podzia-metod-data-mining.html#modele-grupujace"><i class="fa fa-check"></i><b>3.4</b> Modele grupujące</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html"><i class="fa fa-check"></i><b>4</b> Drzewa decyzyjne</a><ul>
<li class="chapter" data-level="4.1" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#wezy-i-gaezie"><i class="fa fa-check"></i><b>4.1</b> Węzły i gałęzie</a></li>
<li class="chapter" data-level="4.2" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#rodzaje-regu-podziau"><i class="fa fa-check"></i><b>4.2</b> Rodzaje reguł podziału</a><ul>
<li class="chapter" data-level="4.2.1" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#podziay-dla-atrybutow-ze-skali-nominalnej"><i class="fa fa-check"></i><b>4.2.1</b> Podziały dla atrybutów ze skali nominalnej</a></li>
<li class="chapter" data-level="4.2.2" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#podziay-dla-atrybutow-ze-skali-ciagej"><i class="fa fa-check"></i><b>4.2.2</b> Podziały dla atrybutów ze skali ciągłej</a></li>
<li class="chapter" data-level="4.2.3" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#podziay-dla-atrybutow-ze-skali-porzadkowej"><i class="fa fa-check"></i><b>4.2.3</b> Podziały dla atrybutów ze skali porządkowej</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#algorytm-budowy-drzewa"><i class="fa fa-check"></i><b>4.3</b> Algorytm budowy drzewa</a></li>
<li class="chapter" data-level="4.4" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#kryteria-zatrzymania"><i class="fa fa-check"></i><b>4.4</b> Kryteria zatrzymania</a></li>
<li class="chapter" data-level="4.5" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#reguy-podziau"><i class="fa fa-check"></i><b>4.5</b> Reguły podziału</a></li>
<li class="chapter" data-level="4.6" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#przycinanie-drzewa-decyzyjnego"><i class="fa fa-check"></i><b>4.6</b> Przycinanie drzewa decyzyjnego</a><ul>
<li class="chapter" data-level="4.6.1" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#przycinanie-redukujace-bad"><i class="fa fa-check"></i><b>4.6.1</b> Przycinanie redukujące błąd</a></li>
<li class="chapter" data-level="4.6.2" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#przycinanie-minimalizujace-bad"><i class="fa fa-check"></i><b>4.6.2</b> Przycinanie minimalizujące błąd</a></li>
<li class="chapter" data-level="4.6.3" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#przycinanie-ze-wzgledu-na-wspoczynnik-zozonosci-drzewa"><i class="fa fa-check"></i><b>4.6.3</b> Przycinanie ze względu na współczynnik złożoności drzewa</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#obsuga-brakow-danych"><i class="fa fa-check"></i><b>4.7</b> Obsługa braków danych</a></li>
<li class="chapter" data-level="4.8" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#zalety-i-wady"><i class="fa fa-check"></i><b>4.8</b> Zalety i wady</a><ul>
<li class="chapter" data-level="4.8.1" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#zalety"><i class="fa fa-check"></i><b>4.8.1</b> Zalety</a></li>
<li class="chapter" data-level="4.8.2" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#wady"><i class="fa fa-check"></i><b>4.8.2</b> Wady</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="drzewa-decyzyjne.html"><a href="drzewa-decyzyjne.html#inne-algorytmy-budowy-drzew-decyzyjnych-implementowane-w-r"><i class="fa fa-check"></i><b>4.9</b> Inne algorytmy budowy drzew decyzyjnych implementowane w <strong>R</strong></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pochodne-drzew-decyzyjnych.html"><a href="pochodne-drzew-decyzyjnych.html"><i class="fa fa-check"></i><b>5</b> Pochodne drzew decyzyjnych</a><ul>
<li class="chapter" data-level="5.1" data-path="pochodne-drzew-decyzyjnych.html"><a href="pochodne-drzew-decyzyjnych.html#bagging"><i class="fa fa-check"></i><b>5.1</b> Bagging</a></li>
<li class="chapter" data-level="5.2" data-path="pochodne-drzew-decyzyjnych.html"><a href="pochodne-drzew-decyzyjnych.html#lasy-losowe"><i class="fa fa-check"></i><b>5.2</b> Lasy losowe</a></li>
<li class="chapter" data-level="5.3" data-path="pochodne-drzew-decyzyjnych.html"><a href="pochodne-drzew-decyzyjnych.html#boosting"><i class="fa fa-check"></i><b>5.3</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="klasyfikatory-liniowe.html"><a href="klasyfikatory-liniowe.html"><i class="fa fa-check"></i><b>6</b> Klasyfikatory liniowe</a><ul>
<li class="chapter" data-level="6.1" data-path="klasyfikatory-liniowe.html"><a href="klasyfikatory-liniowe.html#reprezentacja-progowa"><i class="fa fa-check"></i><b>6.1</b> Reprezentacja progowa</a></li>
<li class="chapter" data-level="6.2" data-path="klasyfikatory-liniowe.html"><a href="klasyfikatory-liniowe.html#reprezentacja-logitowa"><i class="fa fa-check"></i><b>6.2</b> Reprezentacja logitowa</a></li>
<li class="chapter" data-level="6.3" data-path="klasyfikatory-liniowe.html"><a href="klasyfikatory-liniowe.html#wady-klasyfikatorow-liniowych"><i class="fa fa-check"></i><b>6.3</b> Wady klasyfikatorów liniowych</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html"><i class="fa fa-check"></i><b>7</b> Klasyfikatory bayesowskie</a><ul>
<li class="chapter" data-level="7.1" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html#klasyfikator-maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.1</b> Klasyfikator maximum a posteriori (MAP)</a></li>
<li class="chapter" data-level="7.2" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html#klasyfikator-najwiekszej-warogodnosci-ml"><i class="fa fa-check"></i><b>7.2</b> Klasyfikator największej warogodności (ML)</a></li>
<li class="chapter" data-level="7.3" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html#naiwny-klasyfikator-bayesa-nb"><i class="fa fa-check"></i><b>7.3</b> Naiwny klasyfikator Bayesa (NB)</a></li>
<li class="chapter" data-level="7.4" data-path="klasyfikatory-bayesowskie.html"><a href="klasyfikatory-bayesowskie.html#zalety-i-wady-1"><i class="fa fa-check"></i><b>7.4</b> Zalety i wady</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Eksploracja danych</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="drzewa-decyzyjne" class="section level1">
<h1><span class="header-section-number">4</span> Drzewa decyzyjne</h1>
<p><em>Drzewo decyzyjne</em><a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> jest strukturą hierarchiczną przedstawiającą model klasyfikacyjny lub regresyjny. Stosowane są szczególnie często wówczas, gdy funkcyjna postać związku pomiędzy predyktorami a zmienną wynikową jest nieznana lub ciężka do ustalenia.
Każde drzewo decyzyjne składa się z korzenia (ang. <em>root</em>), węzłów (ang. <em>nodes</em>) i liści (ang. <em>leaves</em>). Korzeniem nazywamy początkowy węzeł drzewa, z którego poprzez podziały (ang. <em>splits</em>) powstają kolejne węzły potomne. Końcowe węzły, które nie podlegają podziałom nazywamy liśćmi, a linie łączące węzły nazywamy gałęziami (ang. <em>branches</em>).</p>
<p>Jeśli drzewo służy do zadań klasyfikacyjnych, to liście zawierają informację o tym, która klasa w danym ciągu podziałów jest najbardziej prawdopodobna. Natomiast, jeśli drzewo jest regresyjne, to liście zawierają warunkowe miary tendencji centralnej (najczęściej średnią) wartości zmiennej wynikowej. Warunek stanowi szereg podziałów doprowadzający do danego węzła terminalnego (liścia). W obu przypadkach (klasyfikacji i regresji) drzewo “dąży” do takiego podziału by kolejne węzły, a co za tym idzie również liście, były ja najbardziej jednorodne ze względu na zmienną wynikową.</p>
<div class="figure"><span id="fig:unnamed-chunk-11"></span>
<img src="EksploracjaDanych_files/figure-html/unnamed-chunk-11-1.png" alt="Przykład działania drzewa regresyjnego. Wykes w lewym górnym rogu pokazuje prawdziwą zależność, wyres po prawej stronie jest ilustracją drzewa decyzyjnego, a wykres w lewym dolnym rogu pokazuje dyskretyzację przestrzeni dokonaną przez drzewo, czyli sposób jego działania." width="1056" />
<p class="caption">
Rysunek 4.1: Przykład działania drzewa regresyjnego. Wykes w lewym górnym rogu pokazuje prawdziwą zależność, wyres po prawej stronie jest ilustracją drzewa decyzyjnego, a wykres w lewym dolnym rogu pokazuje dyskretyzację przestrzeni dokonaną przez drzewo, czyli sposób jego działania.
</p>
</div>
<div id="wezy-i-gaezie" class="section level2">
<h2><span class="header-section-number">4.1</span> Węzły i gałęzie</h2>
<p>Każdy podział rozdziela dziedzinę <span class="math inline">\(X\)</span> na dwa lub więcej podobszarów dziedziny i wówczas każda obserwacja węzła nadrzędnego jest przyporządkowana węzłom potomnym. Każdy odchodzący węzeł potomny jest połączony gałęzią, która to wiąże się ściśle z możliwymi wynikami podziału. Każdy <span class="math inline">\(\mathbf{n}\)</span>-ty węzeł można opisać jako podzbiór dziedziny w następujący sposób
<span class="math display">\[\begin{equation}
    X_{\mathbf{n}}=\{x\in X|t_1(x)=r_1,t_2(x)=r_2,\ldots,t_k(x)=r_k\},
\end{equation}\]</span>
gdzie <span class="math inline">\(t_1,t_2,\ldots,t_k\)</span> są podziałami, które przeprowadzają <span class="math inline">\(x\)</span> w obszary <span class="math inline">\(r_1, r_2,\ldots, r_k\)</span>. Przez
<span class="math display">\[\begin{equation}
    S_{\mathbf{n}, t=r}=\{x\in S|t(x)=r\}
\end{equation}\]</span>
rozumiemy, że dokonano takiego ciągu podziałów zbioru <span class="math inline">\(S\)</span>, że jego wartości znalazły się w <span class="math inline">\(\mathbf{n}\)</span>-tym węźle.</p>
</div>
<div id="rodzaje-regu-podziau" class="section level2">
<h2><span class="header-section-number">4.2</span> Rodzaje reguł podziału</h2>
<p>Najczęściej występujące reguły podziału w drzewach decyzyjnych są jednowymiarowe, czyli warunek podziału jest generowany na podstawie jednego atrybutu. Istnieją podziały wielowymiarowe ale ze względu na złożoność obliczeniową są rzadziej stosowane.</p>
<div id="podziay-dla-atrybutow-ze-skali-nominalnej" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Podziały dla atrybutów ze skali nominalnej</h3>
<p>Istnieją dwa typy reguł podziału dla skali nominalnej:</p>
<ul>
<li>oparte na wartości atrybutu (ang. <em>value based</em>) - wówczas funkcja testowa przyjmuje postać <span class="math inline">\(t(x)=a(x)\)</span>, czyli podział generują wartości atrybutu;</li>
<li>oparte na równości (ang. <em>equality based</em>) - gdzie funkcja testowa jest zdefiniowana jako
<span class="math display">\[\begin{equation}
  t(x)= \begin{cases}
      1, &amp;\text{ gdy } a(x)=\nu\\
      0, &amp; \text{ w przeciwnym przypadku},
  \end{cases}
\end{equation}\]</span>
gdzie <span class="math inline">\(\nu\in A\)</span> i <span class="math inline">\(A\)</span> jest zbiorem możliwych wartości <span class="math inline">\(a\)</span>. W tym przypadku podział jest dychotomiczny, albo obiekt ma wartość atrybutu równą <span class="math inline">\(\nu\)</span>, albo go nie ma.</li>
</ul>
</div>
<div id="podziay-dla-atrybutow-ze-skali-ciagej" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Podziały dla atrybutów ze skali ciągłej</h3>
<p>Reguły podziału stosowane do skali ciągłej, to:</p>
<ul>
<li>oparta na nierównościach (ang. <em>inequality based</em>) - zdefiniowana jako
<span class="math display">\[\begin{equation}
t(x) = \begin{cases}
  1, &amp;\text{ gdy }a(x)\leq \nu\\
  0, &amp; \text{w przeciwnym przypadku},
  \end{cases}
\end{equation}\]</span>
gdzie <span class="math inline">\(\nu\in A\)</span>;</li>
<li>przedziałowa (ang. <em>interval based</em>) - zdefiniowana jako
<span class="math display">\[\begin{equation}
  t(x) = \begin{cases}
      1, &amp;\text{ gdy }a(x) \in I_1\\
      2, &amp;\text{ gdy }a(x) \in I_2\\
      \vdots &amp; \\
      k, &amp;\text{ gdy }a(x) \in I_k\\
  \end{cases}
\end{equation}\]</span>
gdzie <span class="math inline">\(I_1,I_2,\ldots,I_k\subset A\)</span> stanowią rozłączny podział (przedziałami) przeciwdziedziny <span class="math inline">\(A\)</span>.</li>
</ul>
</div>
<div id="podziay-dla-atrybutow-ze-skali-porzadkowej" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Podziały dla atrybutów ze skali porządkowej</h3>
<p>Podziały te mogą wykorzystywać oba wcześniej wspomniane typy, w zależności od potrzeb.</p>
</div>
</div>
<div id="algorytm-budowy-drzewa" class="section level2">
<h2><span class="header-section-number">4.3</span> Algorytm budowy drzewa</h2>
<ol style="list-style-type: decimal">
<li>stwórz początkowy węzeł (korzeń) i oznacz go jako <em>otwarty</em>;</li>
<li>przypisz wszystkie możliwe rekordy do węzła początkowego;</li>
<li><strong>dopóki</strong> istnieją otwarte węzły <strong>wykonuj</strong>:
<ul>
<li>wybierz węzeł <span class="math inline">\(\mathbf{n}\)</span>, wyznacz potrzebne statystyki opisowe zmiennej zależnej dla tego węzła i przypisz wartość docelową;</li>
<li><strong>jeśli</strong> kryterium zatrzymania podziału jest spełnione dla węzła <span class="math inline">\(n\)</span>, <strong>to</strong> oznacz go za <strong>zamknięty</strong>;</li>
<li><strong>w przeciwnym przypadku</strong> wybierz podział <span class="math inline">\(r\)</span> elementów węzła <span class="math inline">\(\mathbf{n}\)</span>, i dla każdego podzbioru podziału stwórz węzeł niższego rzędu (potomka) <span class="math inline">\(\mathbf{n}_r\)</span> oraz oznacz go jako <em>otwarty</em>;</li>
<li>następnie przypisz wszystkie przypadki generowane podziałem <span class="math inline">\(r\)</span> do odpowiednich węzłów potomków <span class="math inline">\(\mathbf{n}_r\)</span>;</li>
<li>oznacza węzeł <span class="math inline">\(\mathbf{n}\)</span> jako <em>zamknięty</em>.</li>
</ul></li>
</ol>
<p>Sposób przypisywania wartości docelowej wiąże się ściśle z rodzajem drzewa. W drzewach regresyjnych chodzi o wyliczenie średniej lub mediany dla obserwacji ujętych w danym węźle. Natomiast w przypadku drzewa klasyfikacyjnego, wyznacza się wartości prawdopodobieństw przynależności obserwacji znajdującej się w danym węźle do poszczególnych klas
<span class="math display">\[\begin{equation}
    \P(d|\mathbf{n})=\P_{T_\mathbf{n}}(d)=\frac{|T_\mathbf{n}^d|}{|T_\mathbf{n}|},
\end{equation}\]</span>
gdzie <span class="math inline">\(T_\mathbf{n}\)</span> oznaczają obserwacje zbioru uczącego znajdujące się w węźle <span class="math inline">\(\mathbf{n}\)</span>, a <span class="math inline">\(T_\mathbf{n}^d\)</span> oznacza dodatkowo podzbiór zbioru uczącego w <span class="math inline">\(\mathbf{n}\)</span> węźle, które należą do klasy <span class="math inline">\(d\)</span>. Oczywiście klasyfikacja na podstawie otrzymanych prawdopodobieństw w danym węźle jest dokonana przez wybór klasy charakteryzującej się najwyższym prawdopodobieństwem.</p>
</div>
<div id="kryteria-zatrzymania" class="section level2">
<h2><span class="header-section-number">4.4</span> Kryteria zatrzymania</h2>
<p>Kryterium zatrzymania jest warunkiem, który decyduje o tym, że dany węzeł uznajemy za zamknięty i nie dokonujemy dalszego jego podziału. Wyróżniamy następujące kryteria zatrzymania:</p>
<ul>
<li>jednorodność węzła - w przypadku drzewa klasyfikacyjnego może zdarzyć się sytuacja, że wszystkie obserwacje węzła będą pochodziły z jednej klasy. Wówczas nie ma sensu dokonywać dalszego podziału węzła;</li>
<li>węzeł jest pusty - zbiór przypisanych obserwacji zbioru uczącego do <span class="math inline">\(\mathbf{n}\)</span>-tego węzła jest pusty;</li>
<li>brak reguł podziału - wszystkie reguły podziału zostały wykorzystane, zatem nie da się stworzyć potomnych węzłów, które charakteryzowałyby się większą homogenicznością;</li>
</ul>
<p>Warunki ujęte w pierwszych dwóch kryteriach mogą być nieco złagodzone, poprzez zatrzymanie podziałów wówczas, gdy prawdopodobieństwo przynależenia do pewnej klasy przekroczy ustalony próg lub gdy liczebność węzła spadnie poniżej ustalonej wartości.</p>
<p>W literaturze tematu istnieje jeszcze jedno często stosowane kryterium zatrzymania oparte na wielkości drzewa. Węzeł potomny ustala się jako zamknięty, gdy długość ścieżki dojścia do nie go przekroczy ustaloną wartość.</p>
</div>
<div id="reguy-podziau" class="section level2">
<h2><span class="header-section-number">4.5</span> Reguły podziału</h2>
<p>Ważnym elementem algorytmu tworzenia drzewa regresyjnego jest <em>reguła podziału</em>. Dobierana jest w taki sposób aby zmaksymalizować zdolności generalizacyjne drzewa. Złożoność drzewa mierzona jest najczęściej przeciętną liczbą podziałów potrzebnych do dotarcia do liścia zaczynając od korzenia. Liście są najczęściej tworzone wówczas gdy dyspersja wartości wynikowej jest stosunkowo mała lub węzeł zawiera w miarę homogeniczne obserwacje ze względu na przynależność do klasy zmiennej wynikowej. W przypadku drzew regresyjnych zmienność na poziomie węzłów jest dobrą miarą służącą do definiowania podziału w węźle. I tak, jeśli pewien podział generuje nam stosunkowo małe dyspersje wartości docelowych w węzłach potomnych, to można ten podział uznać za właściwy. Jeśli <span class="math inline">\(T_n\)</span> oznacza zbiór rekordów należących do węzła <span class="math inline">\(n\)</span>, a <span class="math inline">\(T_{n,t=r}\)</span> są podzbiorami generowanymi przez podział <span class="math inline">\(r\)</span> w węzłach potomnych dla <span class="math inline">\(n\)</span>, to dyspersję wartości docelowej <span class="math inline">\(f\)</span> będziemy oznaczali następująco
<span class="math display">\[\begin{equation}\label{dyspersja}
     \operatorname{disp}_{T_{n,t=r}}(f).
\end{equation}\]</span></p>
<p>Regułę podziału możemy określać poprzez minimalizację średniej ważonej dyspersji wartości docelowej następującej postaci
<span class="math display">\[\begin{equation}\label{reg_podz}
        \operatorname{disp}_n(f|t)=\sum_{r\in R_t}\frac{|T_{n,t=r}|}{|T_n|}\operatorname{disp}_{T_{n,t=r}}(f),
\end{equation}\]</span>
gdzie <span class="math inline">\(|\  |\)</span> oznacza moc zbioru, a <span class="math inline">\(R_t\)</span> zbiór wszystkich możliwych wartości reguły podziału. Czasami wygodniej będzie maksymalizować przyrost dyspersji (lub spadek)
<span class="math display">\[\begin{equation}\label{przyrost}
        \bigtriangleup \operatorname{disp}_n(f|t)=\operatorname{disp}_n(f)-\sum_{r\in R_t}\frac{|T_{n,t=r}|}{|T_n|}\operatorname{disp}_{T_{n,t=r}}(f).
\end{equation}\]</span></p>
<p>Miarą heterogeniczności węzłów ze względu na zmienną wynikową (ang. <em>impurity</em>) w drzewach klasyfikacyjnych, która pozwala na tworzenie kolejnych podziałów węzła, są najczęściej wskaźnik Gini’ego i entropia <span class="citation">(Breiman et al. <a href="#ref-Breiman1984">2017</a>)</span>.</p>
<p>Entropią podzbioru uczącego w węźle <span class="math inline">\(\mathbf{n}\)</span>, wyznaczamy wg wzoru
<span class="math display">\[\begin{equation}
E_{T_{\mathbf{n}}}(c|t) = \sum_{x\in R_t} \frac{|T_{\mathbf{n}, t=r}|}{|T_{\mathbf{n}}|}E_{T_{\mathbf{n}, t=r}}(c),
\end{equation}\]</span>
gdzie <span class="math inline">\(t\)</span> jest podziałem (kandydatem), <span class="math inline">\(r\)</span> potencjalnym wynikiem podziału <span class="math inline">\(t\)</span>, <span class="math inline">\(c\)</span> jest oznaczeniem klasy zmiennej wynikowej, a
<span class="math display">\[\begin{equation}
    E_{T_{\mathbf{n}, t=r}}(c) = \sum_{d\in C}-\P_{T_{\mathbf{n}, t=r}}(c=d)\log\P_{T_{\mathbf{n}, t=r}}(c=d),
\end{equation}\]</span>
przy czym
<span class="math display">\[\begin{equation}
    \P_{T_{\mathbf{n}, t=r}}(c=d)= \P_{T_{\mathbf{n}}}(c=d|t=r).
\end{equation}\]</span></p>
<p>Podobnie definiuje się indeks Gini’ego
<span class="math display">\[\begin{equation}
Gi_{T_{\mathbf{n}}}(c|t) = \sum_{x\in R_t} \frac{|T_{\mathbf{n}, t=r}|}{|T_{\mathbf{n}}|}Gi_{T_{\mathbf{n}, t=r}}(c),
\end{equation}\]</span>
gdzie
<span class="math display">\[\begin{equation}
    Gi_{T_{\mathbf{n}, t=r}}(c) = \sum_{d\in C}\P_{T_{\mathbf{n}, t=r}}(c=d)\cdot(1-\P_{T_{\mathbf{n}, t=r}}(c=d))= 1-\sum_{d\in C}\P^2_{T_{\mathbf{n}, t=r}}(c=d).
\end{equation}\]</span>
Dla tak zdefiniowanych miar “nieczystości” węzłów, podziału dokonujemy w taki sposób, aby zminimalizować współczynnik Gini’ego lub entropię. Im niższe miary nieczystości, tym bardziej obserwacje znajdujące się w węźle są monokulturą<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>. Nierzadko korzysta się również z współczynnika przyrostu informacji (ang. <em>information gain</em>)
<span class="math display">\[\begin{equation}
    \Delta E_{T_{\mathbf{n}}}(c|t)=E_{T_{\mathbf{n}}}(c)-E_{T_{\mathbf{n}}}(c|t).
\end{equation}\]</span>
Istnieje również jego odpowiednik dla indeksu Gini’ego. W obu przypadkach optymalnego podziału szukamy poprzez maksymalizację przyrostu informacji.</p>
</div>
<div id="przycinanie-drzewa-decyzyjnego" class="section level2">
<h2><span class="header-section-number">4.6</span> Przycinanie drzewa decyzyjnego</h2>
<p>Uczenie drzewa decyzyjnego wiąże się z ryzykiem przeuczenia modelu (podobnie jak to się ma w przypadku innych modeli predykcyjnych). Wcześniej przytoczone reguły zatrzymania (np. głębokość drzewa czy zatrzymanie przy osiągnięciu jednorodności na zadanym poziomie) pomagają kontrolować poziom generalizacji drzewa ale czasami będzie dodatkowo potrzebne przycięcie drzewa, czyli usunięcie pewnych podziałów, a co za tym idzie, również liści (węzłów).</p>
<div id="przycinanie-redukujace-bad" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Przycinanie redukujące błąd</h3>
<p>Jedną ze strategii przycinania drzewa jest przycinanie redukujące błąd (ang. <em>reduced error pruning</em>). Polega ono na porównaniu błędów (najczęściej używana jest miara odsetka błędnych klasyfikacji lub MSE) liścia <span class="math inline">\(\mathbf{l}\)</span> i węzła do którego drzewo przycinamy <span class="math inline">\(\mathbf{n}\)</span> na całkiem nowym zbiorze uczącym <span class="math inline">\(R\)</span>. Niech <span class="math inline">\(e_R(\mathbf{l})\)</span> i <span class="math inline">\(e_R(\mathbf{n})\)</span> oznaczają odpowiednio błędy na zbiorze <span class="math inline">\(R\)</span> liścia i węzła. Przez błąd węzła rozumiemy błąd pod-drzewa o korzeniu <span class="math inline">\(\mathbf{n}\)</span>. Wówczas jeśli zachodzi warunek
<span class="math display">\[\begin{equation}
    e_R(\mathbf{l})\leq e_R(\mathbf{n}), 
\end{equation}\]</span>
to zaleca się zastąpić węzeł <span class="math inline">\(\mathbf{n}\)</span> liściem <span class="math inline">\(\mathbf{l}\)</span>.</p>
</div>
<div id="przycinanie-minimalizujace-bad" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Przycinanie minimalizujące błąd</h3>
<p>Przycinanie minimalizujące błąd opiera się na spostrzeżeniu, że błąd drzewa przyciętego charakteryzuje się zbyt pesymistyczną oceną i dlatego wymaga korekty. Węzeł drzewa klasyfikacyjnego <span class="math inline">\(\mathbf{n}\)</span> zastępujemy liściem <span class="math inline">\(\mathbf{l}\)</span>, jeśli
<span class="math display">\[\begin{equation}
    \hat{e}_T(\mathbf{l})\leq \hat{e}_T(\mathbf{n}),
\end{equation}\]</span>
gdzie
<span class="math display">\[\begin{equation}
    \hat{e}_T(\mathbf{n})=\sum_{\mathbf{n}&#39;\in N(\mathbf{n})}\frac{|T_{\mathbf{n}&#39;}|}{|T_\mathbf{n}|}\hat{e}_T(\mathbf{n}&#39;),
\end{equation}\]</span>
a <span class="math inline">\(N(\mathbf{n})\)</span> jest zbiorem wszystkich możliwych węzłów potomnych węzła <span class="math inline">\(\mathbf{n}\)</span> i <span class="math display">\[\begin{equation}
    \hat{e}_T(\mathbf{l})=1-\frac{|\{x\in T_\mathbf{l}|c(x)=d_{\mathbf{l}}\}|+mp}{|T_\mathbf{l}|+m},
\end{equation}\]</span>
gdzie <span class="math inline">\(p\)</span> jest prawdopodobieństwem przynależności do klasy <span class="math inline">\(d_{\mathbf{l}}\)</span> ustalona na podstawie zewnętrznej wiedzy (gdy jej nie posiadamy przyjmujemy <span class="math inline">\(p=1/|C|\)</span>).</p>
<p>W przypadku drzewa regresyjnego znajdujemy wiele analogii, ponieważ jeśli dla pewnego zbioru rekordów <span class="math inline">\(T\)</span> spełniony jest warunek
<span class="math display">\[\begin{equation}\label{kryterium1}
    \operatorname{mse}_T(\mathbf{l})\leq\operatorname{mse}_T(\mathbf{n}),
\end{equation}\]</span>
gdzie <span class="math inline">\(\mathbf{l}\)</span> i <span class="math inline">\(\mathbf{n}\)</span> oznaczają odpowiednio liść i węzeł, to wówczas zastępujemy węzeł <span class="math inline">\(\mathbf{n}\)</span> przez liść <span class="math inline">\(\mathbf{l}\)</span>.</p>
<p>Estymatory wyznaczone na podstawie niewielkiej próby, mogą być obarczone znaczącym błędem. Wyliczanie błędu średnio-kwadratowego dla podzbioru nowych wartości może się charakteryzować takim obciążeniem. Dlatego stosuje się statystyki opisowe z poprawką, której pochodzenie może mieć trzy źródła: wiedza merytoryczna na temat szukanej wartości, założeń modelu lub na podstawie wyliczeń opartych o cały zbiór wartości.</p>
<p>Skorygowany estymator błędu średnio-kwadratowego ma następującą postać
<span class="math display">\[\begin{equation}\label{mse}
        \widehat{\operatorname{mse}}_T(\mathbf{l})=\frac{\sum_{x\in T}(f(x)-m_{\mathbf{l},m,m_0}(f))^2+mS_0^2}{|T_\mathbf{l}|+m},
\end{equation}\]</span>
gdzie
<span class="math display">\[\begin{equation}\label{poprawka}
        m_{\mathbf{l},m,m_0}(f)=\frac{\sum_{x\in T_\mathbf{l}}f(x)+mm_0}{|T_\mathbf{l}|+m},
\end{equation}\]</span>
a <span class="math inline">\(m_0\)</span> i <span class="math inline">\(S_0^2\)</span> są średnią i wariancją wyznaczonymi na całej próbie uczącej.
Błąd średnio-kwadratowy węzła <span class="math inline">\(\mathbf{n}\)</span> ma postać
<span class="math display">\[\begin{equation}\label{propagacja}
        \widehat{\operatorname{mse}}_T(\mathbf{n})=\sum_{\mathbf{n}&#39;\in N(\mathbf{n})}\frac{|T_{\mathbf{n}&#39;}|}{|T_\mathbf{n}|}\widehat{\operatorname{mse}}_T(\mathbf{n}&#39;).
\end{equation}\]</span>
Wówczas kryterium podcięcia można zapisać w następujący sposób
<span class="math display">\[\begin{equation}\label{kryterium2}
        \widehat{\operatorname{mse}}_T(\mathbf{l}) \leq \widehat{\operatorname{mse}}_T(\mathbf{n})
\end{equation}\]</span></p>
</div>
<div id="przycinanie-ze-wzgledu-na-wspoczynnik-zozonosci-drzewa" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Przycinanie ze względu na współczynnik złożoności drzewa</h3>
<p>Przycinanie ze względu na współczynnik złożoności drzewa (ang. <em>cost-complexity pruning</em>) polega na wprowadzeniu “kary” za zwiększoną złożoność drzewa. Drzewa klasyfikacyjne przycinamy gdy spełniony jest warunek
<span class="math display">\[\begin{equation}
    e_T(\mathbf{l})\leq e_T(\mathbf{n})+\alpha C(\mathbf{n}),
\end{equation}\]</span>
gdzie <span class="math inline">\(C(\mathbf{n})\)</span> oznacza złożoność drzewa mierzoną liczbą liści, a <span class="math inline">\(\alpha\)</span> parametrem wagi kary za złożoność drzewa.</p>
<p>Wspomniane kryterium przycięcia dla drzew regresyjnych bazuje na względnym błędzie średnio-kwadratowym (ang. <em>relative square error</em>), czyli
<span class="math display">\[\begin{equation}\label{rse}
        \widehat{\operatorname{rse}}_T(\mathbf{n})=\frac{|T|\widehat{\operatorname{mse}}_T(\mathbf{n})}{(|T|-1)S^2_T(f)},
\end{equation}\]</span>
gdzie <span class="math inline">\(T\)</span> oznacza podzbiór <span class="math inline">\(X\)</span>, <span class="math inline">\(S^2_T\)</span> wariancję na zbiorze <span class="math inline">\(T\)</span>.
Wówczas kryterium podcięcia wygląda następująco
<span class="math display">\[\begin{equation}\label{kryterium3}
    \widehat{\operatorname{rse}}_T(\mathbf{l})\leq \widehat{\operatorname{rse}}_T(\mathbf{n})+\alpha C(\mathbf{n}).
\end{equation}\]</span></p>
</div>
</div>
<div id="obsuga-brakow-danych" class="section level2">
<h2><span class="header-section-number">4.7</span> Obsługa braków danych</h2>
<p>Drzewa decyzyjne wyjątkowo dobrze radzą sobie z obsługa zbiorów z brakami. Stosowane są głównie dwie strategie:</p>
<ul>
<li>udziałów obserwacji (ang. <em>fractional instances</em>) - rozważane są wszystkie możliwe podziały dla brakującej obserwacji i przypisywana jest im odpowiednia waga lub prawdopodobieństwo, w oparciu o zaobserwowany rozkład znanych obserwacji. Te same wagi są stosowane do predykcji wartości na podstawie drzewa z brakami danych.</li>
<li>podziałów zastępczych (ang. <em>surrogate splits</em>) - jeśli wynik podziału nie może być ustalony dla obserwacji z brakami, to używany jest podział zastępczy (pierwszy), jeśli i ten nie może zostać ustalony, to stosuje się kolejny. Kolejne podziały zastępcze są generowane tak, aby wynik podziału możliwie najbardziej przypominał podział właściwy.</li>
</ul>
</div>
<div id="zalety-i-wady" class="section level2">
<h2><span class="header-section-number">4.8</span> Zalety i wady</h2>
<div id="zalety" class="section level3">
<h3><span class="header-section-number">4.8.1</span> Zalety</h3>
<ul>
<li>łatwe w interpretacji;</li>
<li>nie wymagają żmudnego przygotowania danych (brak standaryzacji, wprowadzania zmiennych binarnych, dopuszcza występowanie braków danych);</li>
<li>działa na obu typach zmiennych - jakościowych i ilościowych;</li>
<li>dopuszcza nieliniowość związku między zmienną wynikową a predyktorami;</li>
<li>odporny na odstępstwa od założeń;</li>
<li>pozwala na obsługę dużych zbiorów danych.</li>
</ul>
</div>
<div id="wady" class="section level3">
<h3><span class="header-section-number">4.8.2</span> Wady</h3>
<ul>
<li>brak jawnej postaci zależności;</li>
<li>zależność struktury drzewa od użytego algorytmu;</li>
<li>przegrywa jakością predykcji z innymi metodami nadzorowanego uczenia maszynowego.</li>
</ul>

<div class="example">
<span id="exm:przyk41" class="example"><strong>Przykład 4.1  </strong></span>Przykładem zastosowania drzew decyzyjnych będzie klasyfikacja irysów na podstawie długości i szerokości kielicha i płatka.
</div>

<p>Przykładem zastosowania drzew decyzyjnych będzie klasyfikacja irysów na podstawie długości i szerokości kielicha i płatka.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse) 
<span class="kw">library</span>(rpart) <span class="co"># pakiet do tworzenia drzew typu CART</span>
<span class="kw">library</span>(rpart.plot) <span class="co"># pakiet do rysowania drzew</span></code></pre>
<p>Każde zadanie ucznia maszynowego zaczynamy od czyszczenia danych i odpowiedniego ich przygotowania ale w tym przypadku skupimy się jedynie na budowie, optymalizacji i ewaluacji modelu.</p>
<p><strong>Podział zbioru na próbę uczącą i testową</strong></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">44</span>)
dt.train &lt;-<span class="st"> </span>iris <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">sample_frac</span>(<span class="dt">size =</span> <span class="fl">0.7</span>)
dt.test &lt;-<span class="st"> </span><span class="kw">setdiff</span>(iris, dt.train)
<span class="kw">str</span>(dt.train)</code></pre>
<pre><code>## &#39;data.frame&#39;:    105 obs. of  5 variables:
##  $ Sepal.Length: num  6.4 4.4 6.6 5.4 5 5.4 5.6 4.4 5.4 6.1 ...
##  $ Sepal.Width : num  2.7 3.2 3 3 3.6 3.4 2.9 2.9 3.9 2.9 ...
##  $ Petal.Length: num  5.3 1.3 4.4 4.5 1.4 1.7 3.6 1.4 1.3 4.7 ...
##  $ Petal.Width : num  1.9 0.2 1.4 1.5 0.2 0.2 1.3 0.2 0.4 1.4 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 3 1 2 2 1 1 2 1 1 2 ...</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(dt.test)</code></pre>
<pre><code>## &#39;data.frame&#39;:    45 obs. of  5 variables:
##  $ Sepal.Length: num  4.7 4.6 5.4 4.8 5.8 5.1 5.1 5.1 5 5.2 ...
##  $ Sepal.Width : num  3.2 3.1 3.9 3.4 4 3.8 3.7 3.3 3 3.5 ...
##  $ Petal.Length: num  1.3 1.5 1.7 1.6 1.2 1.5 1.5 1.7 1.6 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.4 0.2 0.2 0.3 0.4 0.5 0.2 0.2 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p><strong>Budowa drzewa</strong></p>
<p>Budowy drzewa dokonujemy za pomocą funkcji <code>rpart</code> pakietu <strong>rpart</strong> <span class="citation">(Therneau and Atkinson <a href="#ref-R-rpart">2018</a>)</span> stosując zapis formuły zależności. Drzewo zostanie zbudowane z uwzględnieniem kilku kryteriów zatrzymania:</p>
<ul>
<li>minimalna liczebność węzła, który może zostać podzielony to 10 - ze względu na małą liczebność zbioru uczącego;</li>
<li>minimalna liczebność liścia to 5 - aby nie dopuścić do przeuczenia modelu;</li>
<li>maksymalna głębokość drzewa to 4 - aby nie dopuścić do przeuczenia modelu.</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">mod.rpart &lt;-<span class="st"> </span><span class="kw">rpart</span>(Species<span class="op">~</span>., <span class="dt">data =</span> dt.train, 
                   <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">minsplit =</span> <span class="dv">10</span>,
                                           <span class="dt">minbucket =</span> <span class="dv">5</span>,
                                           <span class="dt">maxdepth =</span> <span class="dv">4</span>))
<span class="kw">summary</span>(mod.rpart)</code></pre>
<pre><code>## Call:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
##   n= 105 
## 
##           CP nsplit  rel error    xerror       xstd
## 1 0.51470588      0 1.00000000 1.1764706 0.06418173
## 2 0.41176471      1 0.48529412 0.6617647 0.07457243
## 3 0.02941176      2 0.07352941 0.1029412 0.03758880
## 4 0.01000000      3 0.04411765 0.1029412 0.03758880
## 
## Variable importance
##  Petal.Width Petal.Length Sepal.Length  Sepal.Width 
##           35           33           21           12 
## 
## Node number 1: 105 observations,    complexity param=0.5147059
##   predicted class=setosa      expected loss=0.647619  P(node) =1
##     class counts:    37    33    35
##    probabilities: 0.352 0.314 0.333 
##   left son=2 (37 obs) right son=3 (68 obs)
##   Primary splits:
##       Petal.Length &lt; 2.45 to the left,  improve=35.95322, (0 missing)
##       Petal.Width  &lt; 0.8  to the left,  improve=35.95322, (0 missing)
##       Sepal.Length &lt; 5.45 to the left,  improve=25.39467, (0 missing)
##       Sepal.Width  &lt; 3.35 to the right, improve=12.69596, (0 missing)
##   Surrogate splits:
##       Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.000, (0 split)
##       Sepal.Length &lt; 5.45 to the left,  agree=0.924, adj=0.784, (0 split)
##       Sepal.Width  &lt; 3.35 to the right, agree=0.819, adj=0.486, (0 split)
## 
## Node number 2: 37 observations
##   predicted class=setosa      expected loss=0  P(node) =0.352381
##     class counts:    37     0     0
##    probabilities: 1.000 0.000 0.000 
## 
## Node number 3: 68 observations,    complexity param=0.4117647
##   predicted class=virginica   expected loss=0.4852941  P(node) =0.647619
##     class counts:     0    33    35
##    probabilities: 0.000 0.485 0.515 
##   left son=6 (38 obs) right son=7 (30 obs)
##   Primary splits:
##       Petal.Width  &lt; 1.75 to the left,  improve=25.286380, (0 missing)
##       Petal.Length &lt; 4.75 to the left,  improve=24.879360, (0 missing)
##       Sepal.Length &lt; 5.75 to the left,  improve= 6.713875, (0 missing)
##       Sepal.Width  &lt; 3.25 to the left,  improve= 1.336180, (0 missing)
##   Surrogate splits:
##       Petal.Length &lt; 4.75 to the left,  agree=0.882, adj=0.733, (0 split)
##       Sepal.Length &lt; 6.15 to the left,  agree=0.721, adj=0.367, (0 split)
##       Sepal.Width  &lt; 3.15 to the left,  agree=0.618, adj=0.133, (0 split)
## 
## Node number 6: 38 observations,    complexity param=0.02941176
##   predicted class=versicolor  expected loss=0.1315789  P(node) =0.3619048
##     class counts:     0    33     5
##    probabilities: 0.000 0.868 0.132 
##   left son=12 (32 obs) right son=13 (6 obs)
##   Primary splits:
##       Petal.Length &lt; 4.95 to the left,  improve=4.0800440, (0 missing)
##       Petal.Width  &lt; 1.45 to the left,  improve=1.2257490, (0 missing)
##       Sepal.Width  &lt; 2.65 to the right, improve=0.6168705, (0 missing)
##       Sepal.Length &lt; 5.95 to the left,  improve=0.4736842, (0 missing)
##   Surrogate splits:
##       Petal.Width &lt; 1.55 to the left,  agree=0.868, adj=0.167, (0 split)
## 
## Node number 7: 30 observations
##   predicted class=virginica   expected loss=0  P(node) =0.2857143
##     class counts:     0     0    30
##    probabilities: 0.000 0.000 1.000 
## 
## Node number 12: 32 observations
##   predicted class=versicolor  expected loss=0.03125  P(node) =0.3047619
##     class counts:     0    31     1
##    probabilities: 0.000 0.969 0.031 
## 
## Node number 13: 6 observations
##   predicted class=virginica   expected loss=0.3333333  P(node) =0.05714286
##     class counts:     0     2     4
##    probabilities: 0.000 0.333 0.667</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rpart.plot</span>(mod.rpart)</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-15"></span>
<img src="EksploracjaDanych_files/figure-html/unnamed-chunk-15-1.png" alt="Obraz drzewa klasyfikacyjnego." width="1056" />
<p class="caption">
Rysunek 4.2: Obraz drzewa klasyfikacyjnego.
</p>
</div>
<p>Powyższy wykres przedstawia strukturę drzewa klasyfikacyjnego. Kolorami są oznaczone klasy, które w danym węźle dominują. Nasycenie barwy decyduje o sile tej dominacji. W każdym węźle podana jest klasa, do której najprawdopodobniej należą jego obserwacje. Ponadto podane są proporcje przynależności do klas zmiennej wynikowej oraz procent obserwacji zbioru uczącego należących do danego węzła. Pod każdym węzłem podana jest reguła podziału.</p>
<p><strong>Przycinanie drzewa</strong></p>
<p>Zanim przystąpimy do przycinania drzewa należy sprawdzić, jakie są zdolności generalizacyjne modelu. Oceny tej dokonujemy najczęściej sprawdzając macierz klasyfikacji.</p>
<pre class="sourceCode r"><code class="sourceCode r">pred.prob &lt;-<span class="st"> </span><span class="kw">predict</span>(mod.rpart, 
                     <span class="dt">newdata =</span> dt.test)
pred.prob[<span class="dv">10</span><span class="op">:</span><span class="dv">20</span>,]</code></pre>
<pre><code>##    setosa versicolor virginica
## 10      1    0.00000   0.00000
## 11      1    0.00000   0.00000
## 12      1    0.00000   0.00000
## 13      1    0.00000   0.00000
## 14      0    0.96875   0.03125
## 15      0    0.96875   0.03125
## 16      0    0.96875   0.03125
## 17      0    0.96875   0.03125
## 18      0    0.96875   0.03125
## 19      0    0.96875   0.03125
## 20      0    0.00000   1.00000</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">pred.class &lt;-<span class="st"> </span><span class="kw">predict</span>(mod.rpart, 
                      <span class="dt">newdata =</span> dt.test,
                      <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
pred.class</code></pre>
<pre><code>##          1          2          3          4          5          6 
##     setosa     setosa     setosa     setosa     setosa     setosa 
##          7          8          9         10         11         12 
##     setosa     setosa     setosa     setosa     setosa     setosa 
##         13         14         15         16         17         18 
##     setosa versicolor versicolor versicolor versicolor versicolor 
##         19         20         21         22         23         24 
## versicolor  virginica versicolor versicolor versicolor versicolor 
##         25         26         27         28         29         30 
## versicolor versicolor versicolor versicolor versicolor versicolor 
##         31         32         33         34         35         36 
##  virginica  virginica  virginica  virginica  virginica  virginica 
##         37         38         39         40         41         42 
##  virginica  virginica  virginica  virginica  virginica  virginica 
##         43         44         45 
##  virginica  virginica  virginica 
## Levels: setosa versicolor virginica</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tab &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">predykcja =</span> pred.class, <span class="dt">obserwacja =</span> dt.test<span class="op">$</span>Species)
tab</code></pre>
<pre><code>##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15</code></pre>
<p>Jak widać z powyższej tabeli, model całkiem dobrze radzi sobie z poprawną klasyfikacją obserwacji do odpowiednich kategorii. Tylko jedna obserwacja została błędnie zaklasyfikowana.</p>
<p>W dalszej kolejności sprawdzimy, czy nie jest konieczne przycięcie drzewa. Jednym z kryteriów przycinania drzewa jest przycinanie ze względu na złożoność drzewa. W tym przypadku jest wyrażony parametrem <code>cp</code>. Istnieje powszechnie stosowana reguła jednego odchylenia standardowego, która mówi, że drzewo należy przyciąć wówczas, gdy błąd oszacowany na podstawie sprawdzianu krzyżowego (<code>xerror</code>), pierwszy raz zejdzie poniżej poziomu wyznaczonego przez najniższą wartość błędu powiększonego o odchylenie standardowe tego błędu (<code>xstd</code>). Na podstawie poniższej tabeli można ustalić, że poziomem odcięcia jest wartość <span class="math inline">\(0.10294+0.037589=0.140529\)</span>. Pierwszy raz błąd przyjmuje wartość mniejszą od <span class="math inline">\(0.140529\)</span> po drugim podziale (<code>nsplit=2</code>). Temu poziomowi odpowiada <code>cp</code> o wartości <span class="math inline">\(0.029412\)</span> i to jest złożoność drzewa, którą powinniśmy przyjąć do przycięcia drzewa.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">printcp</span>(mod.rpart)</code></pre>
<pre><code>## 
## Classification tree:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
## 
## Variables actually used in tree construction:
## [1] Petal.Length Petal.Width 
## 
## Root node error: 68/105 = 0.64762
## 
## n= 105 
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.514706      0  1.000000 1.17647 0.064182
## 2 0.411765      1  0.485294 0.66176 0.074572
## 3 0.029412      2  0.073529 0.10294 0.037589
## 4 0.010000      3  0.044118 0.10294 0.037589</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotcp</span>(mod.rpart)</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-17"></span>
<img src="EksploracjaDanych_files/figure-html/unnamed-chunk-17-1.png" alt="Na wykresie błędów punkt odcięcia zaznaczony jest linią przerywaną" width="1056" />
<p class="caption">
Rysunek 4.3: Na wykresie błędów punkt odcięcia zaznaczony jest linią przerywaną
</p>
</div>
<p>Przycięte drzewo wygląda następująco:</p>
<pre class="sourceCode r"><code class="sourceCode r">mod.rpart2 &lt;-<span class="st"> </span><span class="kw">prune</span>(mod.rpart, <span class="dt">cp =</span> <span class="fl">0.029412</span>)
<span class="kw">summary</span>(mod.rpart2)</code></pre>
<pre><code>## Call:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
##   n= 105 
## 
##          CP nsplit  rel error    xerror       xstd
## 1 0.5147059      0 1.00000000 1.1764706 0.06418173
## 2 0.4117647      1 0.48529412 0.6617647 0.07457243
## 3 0.0294120      2 0.07352941 0.1029412 0.03758880
## 
## Variable importance
##  Petal.Width Petal.Length Sepal.Length  Sepal.Width 
##           35           31           22           12 
## 
## Node number 1: 105 observations,    complexity param=0.5147059
##   predicted class=setosa      expected loss=0.647619  P(node) =1
##     class counts:    37    33    35
##    probabilities: 0.352 0.314 0.333 
##   left son=2 (37 obs) right son=3 (68 obs)
##   Primary splits:
##       Petal.Length &lt; 2.45 to the left,  improve=35.95322, (0 missing)
##       Petal.Width  &lt; 0.8  to the left,  improve=35.95322, (0 missing)
##       Sepal.Length &lt; 5.45 to the left,  improve=25.39467, (0 missing)
##       Sepal.Width  &lt; 3.35 to the right, improve=12.69596, (0 missing)
##   Surrogate splits:
##       Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.000, (0 split)
##       Sepal.Length &lt; 5.45 to the left,  agree=0.924, adj=0.784, (0 split)
##       Sepal.Width  &lt; 3.35 to the right, agree=0.819, adj=0.486, (0 split)
## 
## Node number 2: 37 observations
##   predicted class=setosa      expected loss=0  P(node) =0.352381
##     class counts:    37     0     0
##    probabilities: 1.000 0.000 0.000 
## 
## Node number 3: 68 observations,    complexity param=0.4117647
##   predicted class=virginica   expected loss=0.4852941  P(node) =0.647619
##     class counts:     0    33    35
##    probabilities: 0.000 0.485 0.515 
##   left son=6 (38 obs) right son=7 (30 obs)
##   Primary splits:
##       Petal.Width  &lt; 1.75 to the left,  improve=25.286380, (0 missing)
##       Petal.Length &lt; 4.75 to the left,  improve=24.879360, (0 missing)
##       Sepal.Length &lt; 5.75 to the left,  improve= 6.713875, (0 missing)
##       Sepal.Width  &lt; 3.25 to the left,  improve= 1.336180, (0 missing)
##   Surrogate splits:
##       Petal.Length &lt; 4.75 to the left,  agree=0.882, adj=0.733, (0 split)
##       Sepal.Length &lt; 6.15 to the left,  agree=0.721, adj=0.367, (0 split)
##       Sepal.Width  &lt; 3.15 to the left,  agree=0.618, adj=0.133, (0 split)
## 
## Node number 6: 38 observations
##   predicted class=versicolor  expected loss=0.1315789  P(node) =0.3619048
##     class counts:     0    33     5
##    probabilities: 0.000 0.868 0.132 
## 
## Node number 7: 30 observations
##   predicted class=virginica   expected loss=0  P(node) =0.2857143
##     class counts:     0     0    30
##    probabilities: 0.000 0.000 1.000</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rpart.plot</span>(mod.rpart2)</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-18"></span>
<img src="EksploracjaDanych_files/figure-html/unnamed-chunk-18-1.png" alt="Drzewo klasyfikacyjne po przycięciu" width="1056" />
<p class="caption">
Rysunek 4.4: Drzewo klasyfikacyjne po przycięciu
</p>
</div>
<p><strong>Ocena dopasowania modelu</strong></p>
<p>Na koniec budowy modelu należy sprawdzić jego jakość na zbiorze testowym.</p>
<pre class="sourceCode r"><code class="sourceCode r">pred.class2 &lt;-<span class="st"> </span><span class="kw">predict</span>(mod.rpart2,
                       <span class="dt">newdata =</span> dt.test,
                       <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
tab2 &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">predykcja =</span> pred.class2, <span class="dt">obserwacja =</span> dt.test<span class="op">$</span>Species)
tab2</code></pre>
<pre><code>##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15</code></pre>
<p>Mimo przycięcia drzewa, klasyfikacja pozostaje na niezmienionym poziomie. Odsetek poprawnych klasyfikacji możemy oszacować za pomocą</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">sum</span>(<span class="kw">diag</span>(tab2))<span class="op">/</span><span class="kw">sum</span>(tab2)<span class="op">*</span><span class="dv">100</span>,<span class="dv">1</span>)</code></pre>
<pre><code>## [1] 97.8</code></pre>
</div>
</div>
<div id="inne-algorytmy-budowy-drzew-decyzyjnych-implementowane-w-r" class="section level2">
<h2><span class="header-section-number">4.9</span> Inne algorytmy budowy drzew decyzyjnych implementowane w <strong>R</strong></h2>
<p>Oprócz najbardziej znanego algorytmu CART implementowanego w postaci funkcji pakietu <strong>rpart</strong>, istnieją również inne algorytmy, które znalzały swoje implementacje w R. Są to:</p>
<ul>
<li><em>CHAID</em><a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> - algorytm przeznaczony do budowy drzew klafyfikacyjnych, gdzie zarówno zmienna wynikowa, jak i zmienne niezależne muszą być ze skali jakościowej. Główną różnicą w stosunku do drzew typu CART jest sposób budowy podziałów, oparty na teście niezależności <span class="math inline">\(\chi^2\)</span> Pearsona. Wyboru reguły podziału dokonuje się poprzez testowanie niezależności zmiennej niezależnej z predyktorami. Reguła o największej wartości statystyki <span class="math inline">\(\chi^2\)</span> jest stosowana w pierwszej kolejności. Implementacja tego algorytmu znajduje się w pakiecie <strong>CHAID</strong><a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> (funkcja do tworzenia drzewa o tej samej nazwie <code>chaid</code>) <span class="citation">(Team <a href="#ref-R-CHAID">2015</a>)</span>.</li>
<li><em>Ctree</em><a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> - algorytm zbliżony zasadą dzialania do CHAID, ponieważ również wykorzystuje testowanie do wyboru reguły podziału. Różni się jednak tym, że może być stosowany do zmiennych dowolnego typu oraz tym, że może być zarowno drzewem klasyfikacyjnym jak i regresyjnym. Implementację R-ową można znaleźć w pakietach <strong>party</strong> <span class="citation">(Hothorn, Hornik, and Zeileis <a href="#ref-R-party">2006</a>)</span> lub <strong>partykit</strong> <span class="citation">(Hothorn and Zeileis <a href="#ref-R-partykit">2015</a>)</span> - funkcją do tworzenia modelu jest <code>ctree</code>.</li>
<li><em>C4.5</em> - algorytm stworzony przez <span class="citation">Quinlan (<a href="#ref-quinlan1993">1993</a>)</span> w oparciu, o również jego autorstwa, algorytm ID3. Służy jedynie do zadań klasyfikacyjnych. W dużym uproszczeniu, dobór reguł podziału odbywa się na podstawie przyrostu informacji (patrz <a href="drzewa-decyzyjne.html#reguy-podziau">Reguły podziału</a>). W przeciwieństwie do pierwotnego algorytmu ID3, C4.5 nie raczej nie przeucza drzew. Implementacja R-owa znajduje się w pakiecie <strong>RWeka</strong> <span class="citation">(Hornik, Buchta, and Zeileis <a href="#ref-R-Rweka">2009</a>)</span> - funkcja do budowy drzewa to <code>J48</code>.</li>
<li><em>C5.0</em> - kolejny algorytm autorstwa <span class="citation">Kuhn and Quinlan (<a href="#ref-R-C50">2018</a>)</span> jest usprawnieniem algorytmu C4.5, generującym mniejsze drzewa automatycznie przycinane na podstawie złożności drzewa. Służy jedynie do zadań klasyfikacyjnych. Jest szybszy od poprzednika i pozwala na zastosowanie metody <em>boosting</em><a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>. Implementacja R-owa znajduje się w pakiecie <em>C50</em>, a funkcja do budowy drzewa to <code>C5.0</code>.</li>
</ul>

<div class="example">
<span id="exm:przyk42" class="example"><strong>Przykład 4.2  </strong></span>W celu porównania wyników klasyfikacji na podstawie drzew decyzyjnych o różnych algorytmach, zostaną nauczone modele w oparciu o funkcje <code>ctree</code>, <code>J48</code> i <code>C5.0</code> dla tego samego zestawu danych co w przykładzie wcześniejszym <a href="drzewa-decyzyjne.html#exm:przyk41">4.1</a>.
</div>

<ul>
<li><strong>Drzewo <code>ctree</code></strong></li>
</ul>
<p>Na początek ustalamy parametry ograniczające rozrozt drzewa podobne jak w poprzednim przykładzie.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(partykit)
tree2 &lt;-<span class="st"> </span><span class="kw">ctree</span>(Species<span class="op">~</span>., <span class="dt">data =</span> dt.train,
               <span class="dt">control =</span> <span class="kw">ctree_control</span>(<span class="dt">minsplit =</span> <span class="dv">10</span>,
                                       <span class="dt">minbucket =</span> <span class="dv">5</span>,
                                       <span class="dt">maxdepth =</span> <span class="dv">4</span>))
tree2</code></pre>
<pre><code>## 
## Model formula:
## Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
## 
## Fitted party:
## [1] root
## |   [2] Petal.Length &lt;= 1.9: setosa (n = 37, err = 0.0%)
## |   [3] Petal.Length &gt; 1.9
## |   |   [4] Petal.Width &lt;= 1.7
## |   |   |   [5] Petal.Length &lt;= 4.9: versicolor (n = 32, err = 3.1%)
## |   |   |   [6] Petal.Length &gt; 4.9: virginica (n = 6, err = 33.3%)
## |   |   [7] Petal.Width &gt; 1.7: virginica (n = 30, err = 0.0%)
## 
## Number of inner nodes:    3
## Number of terminal nodes: 4</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(tree2)</code></pre>
<div class="figure"><span id="fig:ctree"></span>
<img src="EksploracjaDanych_files/figure-html/ctree-1.png" alt="Wykres drzewa decyzyjnego zbudowanego metodą ctree" width="1152" />
<p class="caption">
Rysunek 4.5: Wykres drzewa decyzyjnego zbudowanego metodą ctree
</p>
</div>
<p>Wydaje się, że drzewo nie jest optymalne, ponieważ w węźle 6 obserwacje z grup <code>versicolor</code> i <code>virginica</code> są nieco pomieszane. Ostateczne oceny dokonujemy na podstawie próby testowej.</p>
<pre class="sourceCode r"><code class="sourceCode r">pred2 &lt;-<span class="st"> </span><span class="kw">predict</span>(tree2, <span class="dt">newdata =</span> dt.test)
tab &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">predykcja =</span> pred2, <span class="dt">obserwacja =</span> dt.test<span class="op">$</span>Species)
tab</code></pre>
<pre><code>##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15</code></pre>
<p>Dopiero ocena jakości klasyfikacji na podstawe próby testowej pokazuje, że model zbudowany za pomocą <code>ctree</code> daje podobną precyzję jak <code>rpart</code> przycięty.</p>
<ul>
<li><strong>Drzewo <code>J48</code></strong></li>
</ul>
<p>W tym przypadku model sam poszukuje optymalnego rozwiazania przycinając się automatycznie.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(RWeka)
tree3 &lt;-<span class="st"> </span><span class="kw">J48</span>(Species<span class="op">~</span>., <span class="dt">data =</span> dt.train)
tree3</code></pre>
<pre><code>## J48 pruned tree
## ------------------
## 
## Petal.Width &lt;= 0.6: setosa (37.0)
## Petal.Width &gt; 0.6
## |   Petal.Width &lt;= 1.7
## |   |   Petal.Length &lt;= 4.9: versicolor (32.0/1.0)
## |   |   Petal.Length &gt; 4.9
## |   |   |   Petal.Width &lt;= 1.5: virginica (3.0)
## |   |   |   Petal.Width &gt; 1.5: versicolor (3.0/1.0)
## |   Petal.Width &gt; 1.7: virginica (30.0)
## 
## Number of Leaves  :  5
## 
## Size of the tree :   9</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(tree3)</code></pre>
<div class="figure"><span id="fig:J48"></span>
<img src="EksploracjaDanych_files/figure-html/J48-1.png" alt="Wykres drzewa decyzyjnego zbudowanego metodą J48" width="1152" />
<p class="caption">
Rysunek 4.6: Wykres drzewa decyzyjnego zbudowanego metodą J48
</p>
</div>
<p>Drzewo jest nieco bardziej rozbudowane niż <code>tree2</code> i <code>mod.rpart2</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(tree3)</code></pre>
<pre><code>## 
## === Summary ===
## 
## Correctly Classified Instances         103               98.0952 %
## Incorrectly Classified Instances         2                1.9048 %
## Kappa statistic                          0.9714
## Mean absolute error                      0.0208
## Root mean squared error                  0.1019
## Relative absolute error                  4.6776 %
## Root relative squared error             21.628  %
## Total Number of Instances              105     
## 
## === Confusion Matrix ===
## 
##   a  b  c   &lt;-- classified as
##  37  0  0 |  a = setosa
##   0 33  0 |  b = versicolor
##   0  2 33 |  c = virginica</code></pre>
<p>Podsumowanie dopasowania drzewa na próbie uczącej jest bardzo dobre, bo poprawnych klasyfikacji jest ponad 98%. Oceny dopasowania i tak dokonujemy na zbiorze testowym.</p>
<pre class="sourceCode r"><code class="sourceCode r">pred3 &lt;-<span class="st"> </span><span class="kw">predict</span>(tree3, <span class="dt">newdata =</span> dt.test)
tab &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">predykcja =</span> pred3, <span class="dt">obserwacja =</span> dt.test<span class="op">$</span>Species)
tab</code></pre>
<pre><code>##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15</code></pre>
<p>Otrzymujemy identyczną macierz klasyfikacji jak w poprzednich przypadkach.</p>
<ul>
<li><strong>Drzewo <code>C50</code></strong></li>
</ul>
<p>Tym razem również nie trzeba ustawiać parametrów drzewa, ponieważ algorytm działa tak aby zapobiec rozrostowi drzewa przy jednoczesnej wysokiej poprawności klasyfikacji.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(C50)
tree4 &lt;-<span class="st"> </span><span class="kw">C5.0</span>(Species<span class="op">~</span>., <span class="dt">data =</span> dt.train)
<span class="kw">summary</span>(tree4)</code></pre>
<pre><code>## 
## Call:
## C5.0.formula(formula = Species ~ ., data = dt.train)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Fri Mar 22 11:38:21 2019
## -------------------------------
## 
## Class specified by attribute `outcome&#39;
## 
## Read 105 cases (5 attributes) from undefined.data
## 
## Decision tree:
## 
## Petal.Length &lt;= 1.9: setosa (37)
## Petal.Length &gt; 1.9:
## :...Petal.Width &gt; 1.7: virginica (30)
##     Petal.Width &lt;= 1.7:
##     :...Petal.Length &lt;= 4.9: versicolor (32/1)
##         Petal.Length &gt; 4.9: virginica (6/2)
## 
## 
## Evaluation on training data (105 cases):
## 
##      Decision Tree   
##    ----------------  
##    Size      Errors  
## 
##       4    3( 2.9%)   &lt;&lt;
## 
## 
##     (a)   (b)   (c)    &lt;-classified as
##    ----  ----  ----
##      37                (a): class setosa
##            31     2    (b): class versicolor
##             1    34    (c): class virginica
## 
## 
##  Attribute usage:
## 
##  100.00% Petal.Length
##   64.76% Petal.Width
## 
## 
## Time: 0.0 secs</code></pre>
<p>Otrzymujemy identyczne drzewo jak w przypadku zastosowania algorytmu <code>ctree</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(tree4)</code></pre>
<div class="figure"><span id="fig:C50"></span>
<img src="EksploracjaDanych_files/figure-html/C50-1.png" alt="Wykres drzewa decyzyjnego zbudowanego metodą C5.0" width="1152" />
<p class="caption">
Rysunek 4.7: Wykres drzewa decyzyjnego zbudowanego metodą C5.0
</p>
</div>
<p>Dla pewności przeprowadzimy sprawdzenie na zbiorze testowym.</p>
<pre class="sourceCode r"><code class="sourceCode r">pred4 &lt;-<span class="st"> </span><span class="kw">predict</span>(tree4, <span class="dt">newdata =</span> dt.test)
tab &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">predykcja =</span> pred4, <span class="dt">obserwacja =</span> dt.test<span class="op">$</span>Species)
tab</code></pre>
<pre><code>##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         16         0
##   virginica       0          1        15</code></pre>

</div>
</div>
<h3>Bibliografia</h3>
<div id="refs" class="references">
<div id="ref-Breiman1984">
<p>Breiman, Leo, J. H. Friedman, Richard A. Olshen, and Charles J. Stone. 2017. <em>Classification and Regression Trees.</em> Routledge. <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=edsebk&amp;AN=1619230&amp;lang=pl&amp;site=eds-live&amp;scope=site">http://search.ebscohost.com/login.aspx?direct=true&amp;db=edsebk&amp;AN=1619230&amp;lang=pl&amp;site=eds-live&amp;scope=site</a>.</p>
</div>
<div id="ref-R-rpart">
<p>Therneau, Terry, and Beth Atkinson. 2018. <em>Rpart: Recursive Partitioning and Regression Trees</em>. <a href="https://CRAN.R-project.org/package=rpart">https://CRAN.R-project.org/package=rpart</a>.</p>
</div>
<div id="ref-R-CHAID">
<p>Team, The FoRt Student Project. 2015. <em>CHAID: CHi-Squared Automated Interaction Detection</em>.</p>
</div>
<div id="ref-R-party">
<p>Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” <em>Journal of Computational and Graphical Statistics</em> 15 (3): 651–74. <a href="https://doi.org/10.1198/106186006X133933">https://doi.org/10.1198/106186006X133933</a>.</p>
</div>
<div id="ref-R-partykit">
<p>Hothorn, Torsten, and Achim Zeileis. 2015. “Partykit: A Modular Toolkit for Recursive Partytioning in R.” <em>Journal of Machine Learning Research</em> 16: 3905–9. <a href="http://jmlr.org/papers/v16/hothorn15a.html">http://jmlr.org/papers/v16/hothorn15a.html</a>.</p>
</div>
<div id="ref-quinlan1993">
<p>Quinlan, J Ross. 1993. <em>C4. 5: Programs for Machine Learning</em>. Morgan Kaufmann.</p>
</div>
<div id="ref-R-Rweka">
<p>Hornik, Kurt, Christian Buchta, and Achim Zeileis. 2009. “Open-Source Machine Learning: R Meets Weka.” <em>Computational Statistics</em> 24 (2): 225–32. <a href="https://doi.org/10.1007/s00180-008-0119-7">https://doi.org/10.1007/s00180-008-0119-7</a>.</p>
</div>
<div id="ref-R-C50">
<p>Kuhn, Max, and Ross Quinlan. 2018. <em>C50: C5.0 Decision Trees and Rule-Based Models</em>. <a href="https://CRAN.R-project.org/package=C50">https://CRAN.R-project.org/package=C50</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>wyglądem przypomina odwrócone drzewo, stąd nazwa<a href="drzewa-decyzyjne.html#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p>prawie wszystkie są w jednej klasie<a href="drzewa-decyzyjne.html#fnref12" class="footnote-back">↩</a></p></li>
<li id="fn13"><p>Chi-square automatic interaction detection<a href="drzewa-decyzyjne.html#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>brak w oficjalnej dystrybucji CRAN<a href="drzewa-decyzyjne.html#fnref14" class="footnote-back">↩</a></p></li>
<li id="fn15"><p>Conditional Inference Trees<a href="drzewa-decyzyjne.html#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p>budowa klasyfikatora w oparciu o proces iteracyjny, w którym kolejne w kolejnych iteracjach budowane są proste drzewa i przypisywane są im wagi - im gorszy klasyfikator, tym większa waga - po to aby nauczyć drzewo klasyfikować “trudne” przypadki<a href="drzewa-decyzyjne.html#fnref16" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="podzia-metod-data-mining.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pochodne-drzew-decyzyjnych.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["EksploracjaDanych.pdf", "EksploracjaDanych.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
